
  Agenda
  -----------   
   Scala - Quick Refresher 

   Spark - basics & Architecture
   
   Spark Core API
	-> RDD Transformations & Actions
	-> Caching and Persistence
	-> Shared variables
	-> Spark-submit

   Spark SQL
	-> DataFrame Operations
	-> Ingrations with MySQL and Hive
        -> SQL Optimizations and Tuning

   Spark Streaming
	-> DStreams API (introduction)
	-> Structured Streaming
  
   Materials        
   ----------
	-> PDF Presentations
	-> Code Modules - Scala & Spark
	-> Class Notes
	-> Github: https://github.com/ykanakaraju/sparkscala

  
   Gettind Started with Spark & Scala
   -----------------------------------

     1. vLab - Lab allocated to you. 
	
	 -> Follow the instructions given in the attached document.
	 -> You will be loging into a Window Server
	     -> Here you find a document on the desktop with useris and password. 
	 -> Click on the "Oracle VM Virtualbox" and connect to Ubuntu lab. 

	 => here you can open a terminal and connect to Spark shell (type "spark-shell")
	 => You can also launch "Eclipse" 

     2. Setting up your own environemnt on your personal machine. 

	   Pre-requisite: Java 8
	   => Open a terminal and type "java -version" (it has to be 1.8.xxx or up)
		
	   1. Scala IDE (version of Eclipse)

		URL: http://scala-ide.org/download/sdk.html

		Download Scala IDE for your OS and unzip it to a suitable location
		Navigate into the unzipped folder and click on "Eclipse" application icon to launch Scala IDE.

	   2. IntelliJ

		Follow the instructions @ https://docs.scala-lang.org/getting-started/index.html
		
	       Two build tools for Scala:
		-> Maven (Scala IDE + Maven)  
		-> SBT  (IntelliJ + SBT)
		
     3. Signup to "Databricks Community" Edition Free account.

		URL to Signup : https://databricks.com/try-databricks
		URL to Login: https://community.cloud.databricks.com/login.html


    Creating a Scala Project from Scratch
    -------------------------------------

		
    Scala
    -----   

      -> SCAable LAnguage -> SCALA
      -> Scala is a JVM based Language
	
      -> Scala is multi-paradigm programming language.
	
	-> Objected Oriented Programming
	-> Functional Programming

	-> Scala is BOTH object-oriented & functional programming.

      -> Scala is a statically (strongly) typed language
           -> The data type of every variable is fixed and known at compile time

      => Scala "Pure Object Oriented" programming language.
	   -> Every data point is a "object" in scala
	   -> There are no premitives in Scala

		val i = 10
		val j = 20
		val k = i + j
                    => In the above statement i & J are Int objects
		    =>  '+' is a method			

      Scala Variables and Values
		val -> immutable values
	       	once a value is assigned, you can not change it.

		var -> mutable variable
	       	the value can be changed after assignment

      Type Declaration:   
		-> val i : Int = 10


      Scala Type Inference
         	-> Scala can implicitly infer the types based on the assigned value.
	 	-> We do not have to explicitly declare data types.

      Scala is PURE object oriented language
      --------------------------------------
	
      -> Scala does not have primitives or operators.	
      -> In scala, all data is objects and operations are method invocations.

	  val i = 10.*(40)  
   
             -> 10 is an Int object
	     -> * is a method invoked on 10 (Int object)
	     -> 40 is an Int object passed as a parameter
	     -> i is an Int object returned by the * method.
		
      -> <obj>.method<param1> => <obj> method<param1> => <obj> method param1


    
      Scala Class Hierarchy 
      ---------------------
	   Any 	=> AnyVal  => Int, Long, Boolean, Unit, Byte, Char, ...
		=> AnyRef  => String, List, Map, ....


      Scala Blocks
      ------------
	 => A block any code enclosed in  { }
	 => A block is scala has a return value.
	 => The block returns the value of the last expression that is executed.

         Scala Unit => In Scala "Unit" is an object that represents "no value"
		    prined as "()"

   
      String Interpolations
      ---------------------
	s => Allows you to invoke variables using $ sybmol
		ex: val str = f"i = $i\nj = $j\nk = $k"

	f => s interpolator + can use formatting symbols
		ex: val str = f"i = $i%.2f\nj = $j%.2f\nk = $k"

        raw => s interpolator + escapes the escape chars
		val str = raw"i = $i%.2f\nj = $j%.2f\nk = $k"


      Flow Control Constructs
      -----------------------
	1. if .. else if .. else

		In scala, if condition returns a value
		The type of the value returned is the common-denomitor class of various branches if if condition.

        2. match .. case

		   val k = j match {     
     				case 10 => { "Ten" }
     				case 20 => { "Twenty" }
     				case _ if (j % 2 == 0) => { "Even Number" }
     				case _ if ( j > 100 ) => { "Greater than 100" }
     				case _ => { "Nothing matches" }
   			  }
      Loop
      ----
	=> while
	=> do .. while
	=> foreach : take a function as a parameter and applies that on all elements of a collection object
		List(1,2,3,4,7,5,6,7).foreach( x => println(s"x = $x") )

	=> for loop
	
		for(<generator>, <generator>, ...) { .. }

		for( i <-  1 to 10 by 2){
        	    println(s"i = $i")
     		}

		for( i <-  1 to 10 by 2; j <- 1 to 20 by 4 ){
        	    println(s"i = $i, j = $j")
     		}

		for( i <-  1 to 10 by 2 if (i != 3); j <- 1 to 20 by 4 if (j > i) ){
       		    println(s"i = $i, j = $j")
     		}

		for comprehension
		-----------------
		val l1 = for(i <- 1 to 100 by 2) yield(i)

    
      Range Object
      -------------
		Range(0, 100, 10)  => 0: start, 100: end (excluded), 10: step
		Range(0, 10)  => step is one if not specified
		Range(10, 1, -1) =>  10, 9, 8, 7, 6, 5, 4, 3, 2
		Range(100, 0, -10) => 100, 90, 80, 70, 60, 50, 40, 30, 20, 10
		Range(0, 100, -2) => Empty
		
		0 until 10 by 2 => Range(0, 10, 2)
		0 to 10 by 2    => Range(0, 11, 2)


     Exception Handling 
     ------------------
	
         try {
		some code that might through an exception
         }
         catch {
	     case e: ArrayIndexOutofBoundsException => {
	     }
             case f: FileNotFoundException => {
             }
	     case _ => {
             }
         }
         finally {
	     // code that executes after try/catch blocks
         }

      Example
      --------------

      try {
         println("Reading from a file ...")   
         val f = new FileReader("file1456.txt")            
      } 
      catch {
         case ex: FileNotFoundException => {
            println("Missing file exception")
            println( ex.getMessage() )
         }         
         case _: IOException => {
            println("IO Exception")
         }
         case _: Exception => {
            println("generic exception")
         }
      } 
      finally {
         println("Exiting finally...") 
      }  


     Methods
     --------
	-> Reusable code block that return some value
	-> Method will have a name and optionally some parameters/arguments

	-> NOTE: note that we are using an "=" symbol

	def sum(a: Int, b: Int) : Int = {
        	a + b
     	}
	
	-> methods can take 0 or more arguments                
	-> Methods can be called by positional parameters
	-> Methods can be called by named parameters

	val x = sum(10, 20)		// positional params
	val x = sum(b=10, a=20)   	// named params


	-> Method arguments can have default values	

	def sum(a: Int, b: Int, c: Int = 0) = {       
        	println( s"a = $a, b = $b, c = $c" )    
        	a + b + c
     	}

        -> Methods can have variable length arguments
	  
		 def sum(a: Int, i: Int*) = {       
       			var s = 0 
       
       			for (x <- i){
         			println(s"x = $x")
         			s += x
       			}
       
       			a + s
     		}
    
     		val x = sum(10, 20, 30, 40, 50)
	
		-> In this example, i represents [20, 30, 40, 50]

	-> methods can call themselves within the code. (recursive methods)

		def factorial(n: Int) : Int = {
        		if ( n == 1 ) n
        		else n * factorial(n - 1)
      		}

         -> methods can take multiple parameter lists

		def sum(a: Int, b: Int)(c: Int)(d: Int) = (a + b)*c*d    
    		val x = sum(10, 20)(5)(2)


    Procedures
    ----------
	-> Are like methods but does not return any value.
	-> A procedure always return "Unit" irrespective of the return type of the block
	-> Procedure does not have the = symbol in the defintion

	def box(name: String) {       
        	val line = "-" * name.length() + "----"
        	println( line + "\n| " + name.toUpperCase  + " |\n" + line)        
     	}
     
     	box("scala is a programming language")
      

   Functions
   ---------

       -> In Functional programming, a 'function' is like a literal.
       -> A function has a value and type of its own.
       -> Functions are anonymous by default
       -> A function can be assigned to a variable (named functions)
       -> A function can be passed as a parameter to a method/function
       -> A block can return a function as final value
       -> A method can return a function as return value
  
         function literal		 	type
	----------------------------------------------------------
	(a: Int, b: Int) => a + b		(Int, Int) => Int
	(a: Int) => a * a			Int => Int
	(a : String) => a.length		String => Int
	() => "Windows 10"			() => String
	(name" String) => print(name)		String => Unit	
	(a: Int, b: Int) => List(a, b)		(Int, Int) => List[Int]
	(a: Int, b: Int) => (a, b)		(Int, Int) => (Int, Int)
	(a: (Int, Int), b: (Int, Int)) => (a._1 + b._1, a._2 + b._2)
					((Int, Int), (Int, Int)) => (Int, Int)

        //method returning a function
	def compute(op: String) = {
          (i: Int, j: Int) => {
             op match {
               case "+" => i + j
               case "-" => i - j
               case "*" => i * j
               case _ => i % j
             }
          }
        }      

        // method taking function as a parameter
       def calculate(a: Int, b: Int, f: (Int, Int) => Int) = {
         f(a, b)
       }
      
       val sum = compute("+")
       val diff = compute("-")
       val prod = compute("*")
       val mod = compute("blah")
      
      
       println ( calculate(10, 20, compute("+")) )
       println ( calculate(10, 20, sum) )

       println ( calculate(10, 20, (a: Int, b: Int) => a + b) )
       println ( calculate(10, 20, (a, b) => a + b) )
       println ( calculate(10, 20, _+_) )


    Collections
    -----------

     => Array: mutable & fixed length
     => ArrayBuffer: mutable with variable length

     Immutable Collections
    
        -> Seq  (Sequences)
	     -> Ordered collections and elements can be accessed using an index.

	     -> Indexed Sequences
		 -> Vector
		 -> Range

		 => Optimized for fast random-acccess		

	     -> Linear Sequences
		-> List
		-> Queue
		-> Stream 

		=> Optimized for visiting the elements linearly (i.e in a loop)
		=> They are organized as linked lists
		
		List(1,2,3,4,5,6) => List(1, List(2,3,4,5,6))   
			// here 1 is head, List(2,3,4,5,6) is tail
		        => List(1, List(2, List(3, List(4, List(5, List(6, List())))))) 

		list => List(head, tail)

	-> Set
	     -> Is a unordered collection of unique values.
	     -> We can NOT access the elements using an index.
	     -> SortedSet, BitSet

	-> Map
	     -> A collection of (K, V) pairs

		val m1 = Map( (1, 'A'), (2, 'B'), (3, 'C') )
		val m1 = Map( 1 -> "A", 2 -> "B", 3 -> "C" )

                m1(1) -> "A"
		m1(10) -> raise java.util.NoSuchElementException	
	
     Range Object
     -------------
	=> exclusive range (the final value is excluded)

	Range(1, 10)        => 1,2,3,4,5,6,7,8,9
	Range(1, 10, 2)     => 1,3,5,7,9
	Range(100, 0, -20)  => 100, 80, 60, 40, 20
	Range(1, 20, -1)    => Empty Range() object

	1.to(10) or 1 to 10 => Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
				-> Inclusive Range

	0 to 10 by 2	  => Range(0, 2, 4, 6, 8, 10)    // inclusive
	0 until 10 by 2	  => Range(0, 2, 4, 6, 8)	 // exclusive	
	
	100 until 0 by -25  -> Range(100, 75, 50, 25)

	
     Tuple
     -----
	-> Is an object which can hold elements of multiple data types
	-> A Tuple woth only two elements is generally called a "pair"
  
  	 val t1 = (10, 10.5, 10>5, "Hello", (10, 20))

	 val t2 = (1,2.5,5,false)

	 t1._2     // 10.5
	 t1._5     // (10, 20)
	 t1._5._2  // 20

	
    Option[U]
    ---------

      -> Represents an data type that represents a value that may or may not exist.
      -> Some[U] & None  --> are the values of Option Type

	Example
	--------
	val m1 = Map( 1 -> 100, 2 -> 200, 3 -> 300 )

	m1.get(<key>) returns an Option[Int] object which have a value of 'Some[Int]' if
	the key is present in the map, else it return 'None'

	m1.get(1)  => Some(100)
	m1.get(10) => None


   Reading from a file
   -------------------

      import scala.io.Source

      val lines = Source.fromFile("E:\\Spark\\wordcount.txt")


   Scala Higher Order Functions (HOF)
   ----------------------------------

	=> A HOF is a method/function that takes a function as a paramer or return a function
	   as a return value. 
	=> They usually operate on collections and apply the function to the elements of the colelction.

     1. map		P: U => V
			Element to element transformation
			Returns a Collection that transformed by applying the function on all the elements of the
			collection. 

		 lines.map( a => a.split(" ") )
		 l1.map(x => x > 5)

		 List[String].map( String => Array[String] ) => List[Array[String]]
		 List[U].map(U -> V) => List[V]

    2. filter		  P: U -> Boolean
			  Only those elements for which the function returns True will be there in the output
			  collection.

			  List[U].filter => List[U]

    3. reduceLeft (reduce), reduceRight  P: (U, U) => U
			  The function is iterativly applied on all elements of the collections and reduces the
			  entire collection to one value of the same type. 

			  List[U].reduce( (U, U) => U ) => onr value of type "U"

		List(1, 2, 1, 3, 2, 5, 7, 4, 6, 7, 8, 9, 0, 9).reduce( (x, y) => x + y ) = 64

    4. flatMap		  P: U -> scala.collection.GenTraversableOnce[V]
			  flatMap flattens all the elements of the collection

			   List[String].flatMap( String => Array[String] ) => List[String]  
			   Collection[U].flatMap( U -> Iterable[V]) => List[V]

    5. sortWith		  P: (U, U) => Binary Sorting function
			  Based on the sorting function the elements of the output list are sorted. 
		
			  words.sortWith( (x, y) => x < y )
			  words.sortWith( (x, y) => x.length < y.length )

    6. groupBy		  P: U => V
			  Returns a Map objects, where
				key: Each unique value of the function output
				value: List object containing all the objects that produced the key.

				List[U].groupBy( U => V ) => Map[ V, List[U] ]

    7. mapValues	  P: U -> V
			  Applies only on Map objects
			  Transforms the value part of the map by applying the function.

    8. foldLeft, foldRight    P:  Two parameters:
			    	1. zero-value : an initial value of the type of output you want to produce
			    	2. fol function: merges all the values of the collection with the zero-value.

		l1.foldLeft((0,0))( (z, v) => (z._1 + v, z._2 + 1) )


    Scala Wordcount Program
    -----------------------

	val wordcount = Source.fromFile("file1.txt")
                    .getLines()
                    .toList
                    .flatMap(x => x.split(" "))
                    .groupBy(x => x)
                    .mapValues( x => x.length )
                    .toSeq
                    .sortWith( (a,b) => (a._2 > b._2))
                    
    	wordcount.foreach(println)


   Spark
   ======

     => Spark is a unified in-memory distributed computing framework

     => Spark is used for performing Big Data Analytics

     => Spark framework is written using "Scala" programming. 
	
     => Spark is a polyglot
	 => Support Scala, Java, Python, R 

     => Spark Unified Framework

	-> Spark provides consistent set of APIs for performing different analytics workloads
	   using same execution engine. 

		=> Batch Processing of Unstructured data  :  Spark Core
		=> Batch Processing of Structured data	  :  Spark SQL
		=> Stream Processing (Real time)	  :  DStreams API, Structured Streaming
		=> Predictive Analytics (Mach. Learn.)	  :  Spark MLLib
		=> Graph Parallel Computations		  :  Spark GraphX

     
    Getting started with Spark using Scala
    --------------------------------------

        => Two package managers (& build tools) are popular with Spark (Scala or Java)
		=> Maven
		=> SBT (Scala Build Tool / Simple Build Tool)



    Spark Architecture & Building blocks
    ------------------------------------
    
    1. Cluster Manager

    2. Driver

    3. Executors

    4. SparkContext   


   Spark Core API
   ---------------
	=> Is the low-level API
	=> This is responsible for performing all low level operations such as task-schedules,
           memory-management, fault-recovery etc.


   RDD (Resilient Distributed Dataset)
   -----------------------------------
	
     => Is the fundamental in-memory data abstraction of Spark core API

     => RDD is a collection os distributed in-memory partitions
	   -> Partition is a collection of objects (of any type)

     => RDD partitions are immutable
	  -> Can apply transformation to create new RDDs

     => RDDs are lazily evaluated
	-> Transformations does not execution
	-> Only Action commands trigger execution.

     => RDDs are resilient to missin in-memory partitions
	  -> RDDs can recreate missing partitions at run time.


   Creating RDDs
   -------------
	Three ways:

	1. Creating RDD from external data file
	
		val rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

		val rddFile = sc.textFile("E:\\Spark\\wordcount.txt")
			-> The number of partitions of rddFile is determined by "sc.defaultMinPartitions"
	
	2. Creating RDD from programmatic data
	
		val rdd1 = sc.parallelize( 1 to 100 by 5, 3 )	

		val rdd1 = sc.parallelize( 1 to 100 by 5 )
			-> The number of partitions of rdd1 is determined by "sc.defaultParallelism"	

        3. Create an RDD by applying transformations on existing RDD

		val rddWords = rddFile.flatMap(x => x.split(" "))

   RDD Operations
   --------------
	Two operations:

	1. Transformations

        2. Actions


   RDD Lineage DAG
   ---------------	

	val rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

(4) E:\Spark\wordcount.txt MapPartitionsRDD[1] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[0] at textFile at <console>:24 []

	val rddWords = rddFile.flatMap(x => x.split(" "))

(4) MapPartitionsRDD[2] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[1] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[0] at textFile at <console>:24 []

	val rddPairs = rddWords.map(x => (x, 1))

(4) MapPartitionsRDD[3] at map at <console>:25 []
 |  MapPartitionsRDD[2] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[1] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[0] at textFile at <console>:24 []

	val rddWc = rddPairs.reduceByKey( (x,y) => x + y )

(4) ShuffledRDD[4] at reduceByKey at <console>:25 []
 +-(4) MapPartitionsRDD[3] at map at <console>:25 []
    |  MapPartitionsRDD[2] at flatMap at <console>:25 []
    |  E:\Spark\wordcount.txt MapPartitionsRDD[1] at textFile at <console>:24 []
    |  E:\Spark\wordcount.txt HadoopRDD[0] at textFile at <console>:24 []


   RDD Persistence
   ===============
	
      	val rdd1 = sc.textFile(...)
	val rdd2 = rdd1.t2(...)	
	val rdd3 = rdd1.t3(...)	
	val rdd4 = rdd3.t4(...)	
	val rdd5 = rdd3.t5(...)	
	val rdd6 = rdd5.t6(...)	
	rdd6.persist( StorageLevel.MEMORY_AND_DISK )   ===> instruction to Spark to save rdd6 partitions. 
	val rdd7 = rdd6.t7(...)	

	rdd6.collect()
		=> sc.textFile (rdd1) -> t3 (rdd3) -> t5 (rdd5) -> t6 (rdd6)  ===> collect

        rdd7.collect()
		=> t7 (rdd7)  ===> collect

	rdd6.unpersist()


	Storage Levels
        ==============

	  Persistence Formats
	  -------------------
		=> In-Memory Serialized Format
		=> In-Memory Deserialized Format
		=> On-Disk

	1. MEMORY_ONLY	    	=> default, Memory Deserialized 1x Replicated
	2. MEMORY_AND_DISK  	=> Disk Memory Deserialized 1x Replicated
	3. MEMORY_ONLY_SER  	=> Memory Serialized 1x Replicated
	4. MEMORY_AND_DISK_SER  => Disk Memory Serialized 1x Replicated
	5. DISK_ONLY		=> Disk Serialized 1x Replicated
	6. MEMORY_ONLY_2	=> 2 copies of the partitions on two different executors. 	
	7. MEMORY_AND_DISK_2    => 2 copies of the partitions on two different executors. 

	Commands
	--------
		=> rdd1.cache()      => in-memory deserialized persistence
		=> rdd1.persist(StorageLevel.MEMORY_AND_DISK)
		=> rdd1.persist()

		=> rdd1.unpersist()


   Executor memory structure
   ==========================
   	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)

   Types of Transformations
   =========================

	-> Narrow Transformations
		-> is a transformation that does not cause data shuffling
		-> partition to partition transformations
		-> number of partitions of the output rdd is equal to that of input rdd

	-> Wide Transformations
		-> cause shuffling of the data across various input partitions
		-> number of partitions of the output rdd can be different than that of input rdd



   RDD Transformations 
   ====================

   1. map		P: U => V
			Object to object transformation
			Transforms each object of the input RDD into corresponding output object by applying the function
			input RDD: N Objects; output RDD: N objects

		rddWords.map(x => (x, x.toUpperCase)).collect()

   2. filter		P: U => Boolean
			Only those objects of the input RDD for which the function returns True will be in the output
			RDD. 
			input RDD: N Objects; output RDD: <= N objects

		rddFile.filter(x => x.split(" ").length > 8).collect()

   3. glom		P: None
			Returns one array object per partition in the output RDD with all the objects of the RDD. 

		rdd1			rdd2 = rdd1.glom()
		P0: 2,1,2,3,5 -> glom -> P0: Array(2,1,2,3,5)
		P1: 5,3,6,7,9 -> glom -> P1: Array(5,3,6,7,9)
		P2: 6,8,9,0,4 -> glom -> P2: Array(6,8,9,0,4)

		rdd1.count = 15		rdd2.count = 3 

   4. flatMap		P: U -> TraversableOnce[V]
			flatMap flattens the iterables produced by the function.
			input RDD: N Objects; output RDD: >= N objects

		rddFile.flatMap(x => x.split(" ")).collect()

   5. mapPartitions		P: Iterator[U] => Iterator[V]
				Partion to partition transformation. Transforms an entire partition by 
				applying the function.

		rdd1	   rdd2 = rdd1.mapPartitions(p => List(p.toList.sum).iterator)
		P0: 2,1,2,3,5 -> mapPartitions -> P0: 13
		P1: 5,3,6,7,9 -> mapPartitions -> P1: 30
		P2: 6,8,9,0,4 -> mapPartitions -> P2: 27

		rdd1.mapPartitions(p => List(p.toList.sum).iterator ).collect
		rdd1.mapPartitions(p => p.map( _*10 ) ).collect


   6. mapPartitionsWithIndex	P: (Int, Iterator[U]) => Iterator[V]
				Similar to mapPartition, but we partition index as an additional function parameter. 

		rdd1.mapPartitionsWithIndex( (i, x) => x.map( a => (i, a*10) ) ).collect()
		rdd1.mapPartitionsWithIndex( (i, x) => List((i, x.toList.sum)).iterator ).collect()

   7. distinct			P: None, Optional: numPartition
				Returns an RDD with distinct objects of the input RDD.
						
		rdd1.distinct.glom.collect
		rdd1.distinct(5).glom.collect

   8. sortBy			P: U => V; Optional: ascending (true/false), numPartitions
				The objects of the output RDD are sorted based on the value of the function
				output. 

		rdd1.sortBy(x => x%2 == 0).glom.collect
		rdd1.sortBy(x => x%2 == 0, false).glom.collect
		rdd1.sortBy(x => x%2 == 0, false, 2).glom.collect

		rddWords.sortBy( x => x(x.length - 1), false ).collect()

   Types of RDDs
   -------------

	1. Generic RDD 	: RDD[U]
	2. Pair RDD	: RDD[(K, V)]

    9. groupBy			P: U => V
				Returns a Pair RDD where:
				  key: Each unique value of the function output
				  value: CompactBuffer containing all the values of the RDD that produced the key.
				
		rddWords.groupBy(x => x).map(x => (x._1, x._2.toList.length)).collect()

   10. mapValues			P: U => V
				Applied only to Pair RDDs
				Transforms only the value part the (k, v) pairs
				The function input (U) is the 'value' part of the (k,v) pairs. 

		rddWc.mapValues(x => (x,x)).collect()
		rddWords.groupBy(x => x).mapValues(x => x.toList.length).collect






















   



   

