
  Agenda (Spark using Scala)
  --------------------------
   -> Scala Refresher 
   -> Spark - Basics & Architecture
   -> Spark Core API
	-> RDD - Transformations and Actions
	-> Spark Shared Variables
   -> Spark-Submit
   -> Spark SQL
	-> DataFrame Operations
	-> Integration with MySQL & Hive
	-> SQL Optimizations & Tuning
   -> Spark Streaming
	-> DStreams API (introduction)
	-> Structured Streaming

  Materials
  ---------
	=> PDF Presentations
	=> Code Modules
	=> Class Notes
	=> Github: https://github.com/ykanakaraju/sparkscala


   Getting Started with Spark & Scala
   ----------------------------------

     1. vLab - Lab allocated to you. 
	
	 -> Follow the instructions given in the attached document.

	 -> You will be loging into a Window Server
	     -> Here you find a document on the desktop with useris and password. 

	 -> Click on the "Oracle VM Virtualbox" and connect to Ubuntu lab. 
	 => here you can open a terminal and connect to Spark shell (type "spark-shell")
	 => You can also launch "Eclipse" 

     2. Setting up your own environemnt on your personal machine. 

	   Pre-requisite: Java 8
	   => Open a terminal and type "java -version" (it has to be 1.8.xxx or up)
		
	   1. Scala IDE (version of Eclipse)

		URL: http://scala-ide.org/download/sdk.html

		Download Scala IDE for your OS and unzip it to a suitable location
		Navigate into the unzipped folder and click on "Eclipse" application icon to launch Scala IDE.

	   2. IntelliJ

	       Follow the instructions @ https://docs.scala-lang.org/getting-started/index.html					 
		
	       Two build tools for Scala:
		-> Maven (Scala IDE + Maven)  
		-> SBT  (IntelliJ + SBT)
		
     3. Signup to "Databricks Community" Edition Free account.

		URL to Signup : https://databricks.com/try-databricks
		URL to Login: https://community.cloud.databricks.com/login.html


  Scala Refresher
  ---------------
  
   => Scala : SCAlable LAnguage
	      Compiler based language (based on Java)
	      Interoperable with Java

   => Scala is a multi-paradigm programming language
	 -> Scala is a "Pure" object oriented prog. lang.
	 -> Scala is a functional programming language

  Pure OOP => Scala does not have primitives and operators

  -> Scala Variables :  immutables -> val
		      mutables  -> var
	-> Scala prefers immutables. 

  -> Scala has implicit type inference

  -> Scala uses infix notation
	obj.method(param) => obj method param

  -> Scala is a statically typed language
	-> Need to know the type of every object at compile type

  Scala is PURE object oriented language
  --------------------------------------
	
      -> Scala does not have primitives or operators.	
      -> In scala, all data is objects and operations are method invocations.

	  val i = 10.*(40)  
   
             -> 10 is an Int object
	     -> * is a method invoked on 10 (Int object)
	     -> 40 is an Int object passed as a parameter
	     -> i is an Int object returned by the * method.
		
      -> <obj>.method<param1> => <obj> method<param1> => <obj> method param1


  -> Blocks
	 => A block any code enclosed in  { }
	 => A block is scala has a return value.
	 => The block returns the value of the last expression that is executed.

         Scala Unit => In Scala "Unit" is an object that represents "no value"
		       prined as "()"

   -> Unit : Is a value class in Scala which represents 'no value'
             Printed as "()"

   -> Scala imports
	
        import scala.io.StdIn		  => single class
        import scala.io.{StdIn, Source}   => multiple classes
	import scala.io._                 => all classes

   -> User Input

		val name = StdIn.readLine("What is your name ?")
    		println("Hello " + name)     
    		println("What is your age ?")
    		val age = StdIn.readInt()
    		println("Age : " + age) 

    -> Output

		printf("Name: %s, age: %2.2f", "Kanakaraju", 47d)
		println(name, age)

    -> String interpolations

		s interpolator: s"Name: $name, Age: ${age + 10}"
		f interpolator: s interpolator + formting chars
				 f"Name: $name, Age: ${age + 10}%2.2f"
		raw interpolator : will escape the escape chars

		 	val filePath = raw"E:\newdir\total\red.txt"
    		 	println(filePath)


   Flow control constructs
   -----------------------

   if..else if.. else

	if statement can return a value
	val x = if (i > 100) "> 100" else if(i < 100)  "< 100" else "== 100"

   match ..case

	   val output = i match {
     		case 10 => "ten"
     		case 20 => "twenty"
     		case 30 => "thirty"
     		case 40 => "fourty"
     		case x if (x % 2 == 0) => s"even number"
     		case y if (y % 2 != 0) => s"odd number"
     		case _  => "no match"
   	   }

  Scala Class Hierarchy
  ---------------------
	Any =>  AnyVal => Int, Double, Long, Char, Byte, Unit, Boolean, ...
            =>  AnyRef => String, List[U], all other classes

  Tuple
  ------

    -> Is a class which can hold multiple elements of different type.
	
	val t1 = (10, 20, 10.5, true, "Hello")
	t1: (Int, Int, Double, Boolean, String) = (10,20,10.5,true,Hello)


	val t2 = (10, List(10, 20, 30), Seq(10.5, 12.2), "hello")
	t2: (Int, List[Int], Seq[Double], String) = (10,List(10, 20, 30),List(10.5, 12.2),hello)

      ** A tuple with two elements is called a "Pair"


  Collections
  -----------

     Array           -> mutable collection, fixed-length
     ArrayBuffer     -> mutable collection, variable-length

     => Array: mutable & fixed length
     => ArrayBuffer: mutable with variable length

     Immutable Collections
    
        -> Seq  (Sequences)
	     -> Ordered collections and elements can be accessed using an index.

	     -> Indexed Sequences
		 -> Vector
		 -> Range

		 => Optimized for fast random-acccess		

	     -> Linear Sequences
		-> List
		-> Queue
		-> Stream 

		=> Optimized for visiting the elements linearly (i.e in a loop)
		=> They are organized as linked lists
		
		List(1,2,3,4,5,6) => List(1, List(2,3,4,5,6))   
			// here 1 is head, List(2,3,4,5,6) is tail
		        => List(1, List(2, List(3, List(4, List(5, List(6, List())))))) 

		list => List(head, tail)

	-> Set
	     -> Is a unordered collection of unique values.
	     -> We can NOT access the elements using an index.
	     -> SortedSet, BitSet

	-> Map
	     -> A collection of (K, V) pairs

		val m1 = Map( (1, 'A'), (2, 'B'), (3, 'C') )
		val m1 = Map( 1 -> "A", 2 -> "B", 3 -> "C" )

                m1(1) -> "A"
		m1(10) -> raise java.util.NoSuchElementException


  Range
  ------
		Range(0, 100, 10)  => 0: start, 100: end (excluded), 10: step
		Range(0, 10)  => step is one if not specified
		Range(10, 1, -1) =>  10, 9, 8, 7, 6, 5, 4, 3, 2
		Range(100, 0, -10) => 100, 90, 80, 70, 60, 50, 40, 30, 20, 10
		Range(0, 100, -2) => Empty
		
		0 until 10 by 2 => Range(0, 10, 2)
		0 to 10 by 2    => Range(0, 11, 2)

  
  Option
  -------
        -> reprsents an option (i.e value may be or may not be there)
	-> returns instances of 'Some' class or 'None' class


  Loops
  ------

     while

		while( i > 0 ){
            		print(i + " ")         
            		i -= 1
        	}

     do..while

		do {
         		print(i + " ")         
         		i -= 1
      		} while( i > 0 )

     foreach

	        <collection>.foreach( function )
		(1 to 10).foreach( println )

     for

		for(<generator>, <generator>, ...) { .. }

		for( i <-  1 to 10 by 2){
        	    println(s"i = $i")
     		}

		for( i <-  1 to 10 by 2; j <- 1 to 20 by 4 ){
        	    println(s"i = $i, j = $j")
     		}

		for( i <-  1 to 10 by 2 if (i != 3); j <- 1 to 20 by 4 if (j > i) ){
       		    println(s"i = $i, j = $j")
     		}

		for comprehension
		-----------------
		val l1 = for(i <- 1 to 100 by 2) yield(i)


    Exception Handling
    ------------------

        try {
		<some code that could throw an exception>
        }
        catch {
	    case e: ArrayIndexOutOfBoundException  => { .... }
	    case e: FileNotFoundException => { .... }
	    case _: Exception => { .... }
			
        }
        finally {
		<some code that is always executed
        }

     Example
     =======

      try {
         println("Reading from a file ...")   
         val f = new FileReader("file1rtyryr.txt")            
      } 
      catch {
         case ex: FileNotFoundException => {
            println("FileNotFoundException exception")
            println( ex.getMessage )
            println( ex.getStackTrace )
         }         
         case _: IOException => {
            println("IO Exception")
         }
         case _: Exception => {
            println("generic exception")
         }
      } 
      finally {
         println("Exiting finally...") 
      }  

       
   Methods
   --------
	
     => method is a reusable code blocks with a name declared using def keyword.

	-> methods can be called by positional arguments
	-> methods can be called by named arguments
	-> method arguments can have default values
	-> methods can take parameter lists

		def f1(a: Int, b: Int)(c: Int) = ( a + b ) * c     
     		val i = f1(10, 20)(30)     
     		println(i)

	-> methods can have one variable length argument (last argument)
		-> You can have default values for args if you are using veriable-length argument

		def sum(a: Int, b: Int*) : Int = {
        		var s = 0
        		for (i <- b) s += i
        		a + s
     		}       
     		val s1 = sum(10)

	-> methods can be called recursivly

		def factorial(n: Int) : Int = {
        		if (n < 2) 1
        		else n * factorial(n - 1)
     		}
     
     		println( factorial(6) )

	-> methods can be nested 

  Procedure
  ---------
     -> Procedure is similar method, but always returns "Unit"
     -> Syntactically there is no "=" symbol in the definition  

	def box(name: String) {
        	val line = "-" * name.length + "----"        
        	println(line + "\n| " + name + " |\n" + line)        
     	}     
     
     	box("Scala is a programming language")


  Function
  --------

	=> A function is treated as a literal value (such as 10, "hello" etc)
	=> A function has a value (of its own)
	=> A function has a type
	=> A function is anonymous (by nature)

	=> A function can be assigned to a variable
		val add = (x: Int, y: Int) => x + y

	function literal			function type
        -----------------------------------------------------
	(x: Int, y: Int) => x + y		(Int, Int) => Int
	(x: String, y: Int) => x * y		(String, Int) => String
	() => "Windows 10"			() => String
	(name: String) => println(name)		String => Unit
	(l: List[Int]) => l.sum			List[Int] => Int
	(t: (Int, Int)) => t._1 + t._2		 ((Int, Int)) => Int


        => A function can be passed as argument to another method/function

		def calculate(a: Int, b: Int, f: (Int, Int) => Int) = {
       			f(a, b)
     		}
		val result = calculate(345, 20, (x: Int, y: Int) => x % y)     
     		println( result )

       => A method/function can return a function as its return value
		-> A block can return a function as final value
	
		def compute(op: String) = {       
       			op match {
          			case "+" => (x : Int, y: Int) => x + y
          			case "-" => (x : Int, y: Int) => x - y
          			case "*" => (x : Int, y: Int) => x * y
          			case "/" => (x : Int, y: Int) => x / y
          			case _ => (x : Int, y: Int) => x % y
       			}       
     		}        
     		println( compute("+")(321, 10) )


   Higher Order Functions
   ----------------------
	=> Take a function as an argument
	=> Applied on some Iterable object. 

   1. map		P: U => V
			Transforms each object of the input to another obejct in the output
			input: N elements, output: N element

		l1.map( s => s.split(" ").length )

   2. filter		P: U => Boolean
			The output collection will have only those objects for which the funtion
			returns true. 
			input: N elements, output: <= N element

		l4.filter( t => t._1 + t._2 >= 10 )

   3. flatMap		P: U => GenTraversableOnce[V]    (U => Collection)
			flattens the output of the function
			input: N elements, output: >= N element

		words.flatMap(x => x.toUpperCase)

    4. reduce        => reduceLeft (reduce) & reduceRight
			P: (U, U) => U
			Reduces the entire input collection to one value of the 'same type' by 'iterativly
			applying' the function

		List(2,1,2,4,5,4,3,2).reduce( (x, y) => x - y )  => 23
		
    5. sortWith		P: binary sorting function
			Elements of the collections are sorted based on the sorting function.

		l4.sortWith( (m,n) => m._1 > n._1 )  
    
     6. groupBy		P: U => V
			Returns a Map objects, where
			   key: Each unique value of the function output
			   value: List object containing all the objects that produced the key.

			List[U].groupBy( U => V ) => Map[ V, List[U] ]

     7. mapValues	P: U => V
			Applied only on (key, value) pairs i.e Map objects
			It will transform only the value of the (K, V) pairs

			val m2 = m1.mapValues( x => (x,x) )

     8. fold		=> foldLeft (fold) & foldRight
		
  
  WordCount Program
  -----------------

     val filePath = "E:\\Spark\\wordcount.txt"
     
     val wc = Source.fromFile(filePath)
              .getLines()
              .toList
              .flatMap( x => x.split(" ") )
              .groupBy( x => x )
              .toList
              .map( x => (x._1, x._2.length ) )
              .sortWith( (a, b) => (a._2 > b._2) )

    val wc = Source.fromFile(filePath)
              .getLines()
              .toList
              .flatMap( x => x.split(" ") )
              .groupBy( x => x )
              .mapValues( x => x.length )
              .toList
              .sortWith( (a, b) => (a._2 > b._2) )
              
     wc.foreach(println)

  ========================================================  
    Spark
  ========================================================

   => Spark is an in-memory distributed computing framework.

   	 Cluster => A unified entity containing many nodes whose cumulative resources can be used to
               	    distribute store and/or computations of big data. 

     	in-memory computing -> The intermediate results of distributed computing can be saved in-memory
			       and subsequent tasks can run on these saved partitions. 

        => Spark is 100x faster than MapReduce if you use 100% in-memory processing
	   Spark is 6 t 7x faster than MapReduce even if you use disk based processing

   => Spark is written in Scala. 

   => Spark is a polyglot
	-> Support supports Scala, Java, Python and R

   => Spark provides a "Unified framework" for analytical workloads

	Spark provides a set of consistent APIs that can used to process different analytical workloads
	based on the same execution engine. 

	 -> Batch Analytics on unstructured data	: Spark Core API
	 -> Batch Analytics on structured data		: Spark SQL
	 -> Stream Analytics (real-time processing)	: Spark Streaming, Strcutured Streaming
	 -> Predistive analytics (using ML)		: Spark MLlib
	 -> Graph parallel computations			: Spark GraphX

   => Spark applications can be submitted to multiple cluster managers
	=> local, spark standalone, YARN, Mesos, Kubernetes 
	

    Spark Architecture
    ------------------

    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.


   Getting started with Spark
   --------------------------	
	Popular Options : 
	
	=> Scale-IDE for Eclipse + Maven
		=> Add required dependencies from Central maven repo
		=> The required packges are download and added to your project.
	=> IntelliJ + SBT

        Central maven repository: https://mvnrepository.com/ (search for Spark)

	Downloading "winutils.exe" to your local directory
		=> Create the following folder: C:\hadoop\bin
		=> Download an appropriate version of winutils.exe from the following url;
			https://github.com/steveloughran/winutils
    

   RDD (Resilient distributed dataset)
   -----------------------------------
    -> RDD is the fundamental data abstraction for in-memory data in Spark core API
    -> RDD is a collection of distributed in-memory partitions
	-> A partition is a collection of objects (of any type)

    -> RDDs are immutable

    -> RDDs are lazily evaluated
	-> Transformations does not cause execution. 
           They only create lineage LAGs
        -> Action commands trigger execution.

    -> RDD are resilient
	-> RDDs can create any missing partitions on the fly. 

  Creating RDD
  ------------
	Three ways:

	1. Create an RDD from external data file such as text files

		val rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)


	2. Create an RDD from programming data

		val rdd1 = sc.parallelize( List(1,2,1,3,2,4,5,3,5,6,7,8,9,8,9,6,7,1,2,4,2,3,4,5,6,2,1), 3 )


        3. By applying transformations on existing RDDs

		val rddWords = rddFile.flatMap(x => x.split(" "))


  RDD Operations
  --------------

    Two things:

	1. Transformations	
		-> Returns an RDD
		-> Does not cause execution
		-> Transformation only create RDD Lineage DAGs

        2. Actions
		-> Trigger execution of the RDD
		-> Produces output
		-> Converts thr logical Plan into a physical execution plan.


  RDD Lineage DAG
  ----------------

   => RDD Lineage is a logical plan maintained by the driver
   => RDD lineage DAG contains the hierarchy of dependencies all the way from the very first RDD.
	

	 val rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

(4) E:\Spark\wordcount.txt MapPartitionsRDD[7] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[6] at textFile at <console>:24 []


	val rddWords = rddFile.flatMap(x => x.split(" "))

(4) MapPartitionsRDD[9] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[7] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[6] at textFile at <console>:24 []

 
	val rddPairs = rddWords.map(x => (x,1))

(4) MapPartitionsRDD[10] at map at <console>:25 []
 |  MapPartitionsRDD[9] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[7] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[6] at textFile at <console>:24 []

    
	val rddWc = rddPairs.reduceByKey( (a, b) => a + b )
   
(4) ShuffledRDD[11] at reduceByKey at <console>:25 []
 +-(4) MapPartitionsRDD[10] at map at <console>:25 []
    |  MapPartitionsRDD[9] at flatMap at <console>:25 []
    |  E:\Spark\wordcount.txt MapPartitionsRDD[7] at textFile at <console>:24 []
    |  E:\Spark\wordcount.txt HadoopRDD[6] at textFile at <console>:24 []

   

  RDD Persistence
  ---------------

	val rdd1 = sc.textFile( <file>, 4 )
	val rdd2 = rdd1.t2( ... )
	val rdd3 = rdd1.t3( ... )
	val rdd4 = rdd3.t4( ... )
	val rdd5 = rdd3.t5( ... )
	val rdd6 = rdd5.t6( ... )
	rdd6.persist( StorageLevel.DISK_ONLY )  --> instruction to spark to save rdd6 partitions.
	val rdd7 = rdd6.t7( ... )

	rdd6.collect()

	Lineage of rdd6 => rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[sc.textFile, t3, t5, t6] -> collected

	rdd7.collect()
	
	Lineage of rdd7 => rdd7 -> rdd6.t7
		[t7] -> collected

	rdd6.unpersist()


	StorageLevels
        -------------
	1. MEMORY_ONLY	       -> default, Memory deserialized 1x replicated
	2. MEMORY_AND_DISK     -> Disk Memory deserialized 1x replicated
	3. DISK_ONLY	       -> Disk serialized 1x replicated
	4. MEMORY_ONLY_SER     -> Memory serialized 1x replicated
	5. MEMORY_AND_DISK_SER -> Disk Memory serialized 1x replicated
	6. MEMORY_ONLY_2       -> Memory deserialized 2x replicated
	7. MEMORY_AND_DISK_2   -> Disk Memory deserialized 2x replicated

	Commands
	--------
		rdd1.cache()    => memory-only
		rdd1.persist()	=> memory-only

		import org.apache.spark.storage.StorageLevel
		rdd1.persist( StorageLevel.MEMORY_AND_DISK)

		rdd1.unpersist()

   Executor memory structure
   ==========================

   	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   Types of Transformations
   ------------------------

	-> Narrow Transformations
		-> is a transformation that does not cause data shuffling
		-> partition to partition transformations
		-> number of partitions of the output rdd is equal to that of input rdd

	-> Wide Transformations
		-> cause shuffling of the data across various input partitions
		-> number of partitions of the output rdd can be different than that of input rdd

  RDD Transformations
  -------------------

  1. map		P: U => V
			Object to object transformation
			input RDD: N objects; output RDD: N objects
	
   	rddFile.map(x => x.split(" ").length ).collect

  2. filter		P: U => Boolean
			Filter only those objects for which the function returns true
			input RDD: N objects; output RDD: <= N objects
	
	rddFile.filter(x => x.split(" ").length > 8).collect

  3. glom		P: None
			Returns one Array per partition with all the objects of the partition.

		rdd1		      rdd2 = rdd1.glom()
		P0: 3,2,1,3,2,4,5,6 -> glom -> P0: Array(3,2,1,3,2,4,5,6)
		P1: 4,3,1,3,6,7,8,1 -> glom -> P1: Array(4,3,1,3,6,7,8,1)
		P2: 3,2,5,7,5,8,9,1 -> glom -> P2: Array(3,2,5,7,5,8,9,1)
	
		rdd1.count = 24 (Int)		rdd2.count = 3 (Array[Int])

		rdd1.glom.map(x => x.sum).collect

  4. flatMap		P: U -> TraversableOnce[V]
			flattens the iterables produced by the function
			input RDD: N objects; output RDD: >= N objects

		rddWords.flatMap(x => x.toUpperCase).collect

  5. mapPartitions	P: Iterator[U] => Iterator[V]
			Transforms each input partition to corresponding output partition

		rdd1		   rdd2 = rdd1.mapPartitions( p => p.map(x => x*10) )
		P0: 3,2,1,3,2,4,5,6 -> mapPartitions -> P0: 30,20,10,30,20,40,50,60
		P1: 4,3,1,3,6,7,8,1 -> mapPartitions -> P1: 40,30,10,30,60,70,80,10
		P2: 3,2,5,7,5,8,9,1 -> mapPartitions -> P2: 30,20,50,70,50,80,90,10
   

   6. mapPartitionsWithIndex	P: (Int, Iterator[U]) => Iterator[V]
				Same as mapPartitions but you get partition index as an additional parameter.

		rdd1.mapPartitionsWithIndex( (i, p) => List((i, p.sum)).iterator ).collect

		rdd1.mapPartitionsWithIndex((i, p) => p.map(x => (i, x))).filter(x => x._1 == 0).map(x => x._2).collect


    7. distinct			P: None, Optional: numPartitions
				Returns an RDD with unique objects of the input RDD.

   		rddWords.distinct.collect
		rddWords.distinct(5).collect

	Types of RDDs
	-------------
	1. Generic RDD  => RDD[U]
	2. Pair RDD	=> RDD[(K, V)]


    8. mapValues		P: U => V
				Applied only on PairRDDs
				Transforms only the value part of the (k,v) pairs

		 RDD[(K, V)].mapValues( V => W ) => RDD[(K, W)]

		 val rdd3 = rdd2.mapValues(x => (x,x))

    9. sortBy			P: U => V, Optional: ascending (true or false), numPartitions
				Sorts the elements of the RDD based on the function output

		rddWords.sortBy(x => x(x.length - 1), true).collect()
		rdd1.sortBy(x => x%3, false, 1).glom.collect

    10. groupBy			P: U => V
				Returns a PairRDD where
					key: unique value of the function output
					value: compactBuffer of all the value of the RDD that produced the key.																																																																

		val wordcount = sc.textFile("data/wordcount.txt", 4)
                        .flatMap(s => s.split(" "))
                        .groupBy(x => x)
                        .mapValues(x => x.toList.length)
                        .sortBy(x => x._2, false, 1)      
           
      		wordcount.saveAsTextFile("output/wordcount")

   11. randomSplit		P: Array of weights (ex: Array(0.4, 0.3, 0.3) ), Optional: seed
				Splits the RDD randomly into multiple RDDs in the given weights

		val rddArr = rdd1.randomSplit( Array(0.6, 0.4))
		val rddArr = rdd1.randomSplit( Array(0.6, 0.4), 3454 )   //3454 is a seed


   12. repartition		P: numPartitions
				Used to increase or decrease the number of partitions of the output RDD
				Cause global shuffle

		val rddWords2 = rddWords.coalesce(8)


   13. coalesce			P: numPartitions
				Used to only decrease the number of partitions of the output RDD
				Causes partition-merging

		val rddWords2 = rddWords.coalesce(2)
			

    Recommendations for better performance
    ---------------------------------------
	=> The size of each partition should be ideally approx. 128 MB (100 MB to 1 GB is fine) 
	=> The number of partitions should be a multiple of number of cores allocated
	=> Number of CPU cores per executor should be 5 

  
    14. partitionBy		P: partitioner
				Applied ONLY to pair RDDs. 
				Partitioning happens based on the 'key'
				Is used to control which data goes to which partition.

		Built-in partitioners:

		1. Range Partitioner

			val rangePartitioner = new RangePartitioner(5, rdd)    
    			val partitionedRdd = rdd.partitionBy(rangePartitioner)

		2. Hash Partitioner

			val hashPartitioner = new HashPartitioner(3)    
    			val partitionedRdd = rdd.partitionBy(hashPartitioner)

		3. Custom partitioner
			=> User can define custom partitioning logic.


    15. union, intersection, subtract, cartesian

	Let us rdd1 has M partitions and rdd2 has N partition
	
	 command			output partitions etc.
	 -------------------------------------------------------
	 rdd1.union(rdd2)		M + N, narrow
	 rdd1.intersection(rdd2, [n])    Bigger of M & N, wide 
	 rdd1.subtract(rdd2, [n])	M, wide
	 rdd1.cartesian(rdd2)		M*N, wide



    ..ByKey Transformations
    -----------------------
        => Are wide transformations
	=> Are applied only to PairRDD

     16. keys, values	  P: None
		
		RDD[(K, V)].keys => RDD[K]
		RDD[(K, V)].values => RDD[V]		

		RDD[(Int, String)].keys => RDD[Int]
		RDD[(Int, String)].values => RDD[String]

     17. sortByKey	P: None, Optional: ascending (true/false), numPartitions
			The objects of the PairRDD are sorted based on the value of the key

		rddPairs.sortByKey().collect()
		rddPairs.sortByKey(false).collect()
		rddPairs.sortByKey(true, 1).collect()
		
     18. groupByKey	P: None, Optional: numPartitions
			Returns a PairRDD where
				key: each unique key of the input RDD
				value: CompactBuffer with all the values for that key

			*** WARNING: AVOID groupByKey if possible ***

		  val wordcount = sc.textFile("data/wordcount.txt", 4)
                        .flatMap(s => s.split(" "))
                        .map(x => (x, 1))
                        .groupByKey()
                        .mapValues(x => x.toList.length)
                        .sortBy(x => x._2, false, 1)

     19. reduceByKey		P: (U, U) -> U, Optional: numPartitions
				Reduces all the values of each unique key by iterativly applying the function
				on all partitions (narrow op) and then across the output generated at each 
				partition (shuffle op).

		val output = sc.textFile("E:\\Spark\\wordcount.txt", 4)
                     .flatMap(x => x.split(" "))
                     .map(x => (x, 1))
                     .reduceByKey( (x, y) => x + y )
                     .sortBy(x =>x._2, false, 1)

    20. aggregateByKey
		=> Reduces the values of each unique key to a value of type zero-value. 

					Three parameters:

					1. zero-value:  the final value of each unique-key is if type zero-value
					2. sequence function
					3. combine function
		
		val rdd_students = sc.parallelize(students_list, 3)
                            .map(x => (x._1, x._3))
                            .aggregateByKey( (0,0) )(seq_fun, comb_fun)
                            .mapValues(x => x._1.toDouble/x._2)



	val students_list = List(
  	("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  	("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  	("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  	("Keerti", "Maths", 87), ("Keerti", "Physics", 93), ("Keerti", "Chemistry", 91), ("Keerti", "English", 74), 
  	("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  	("Anitha", "Maths", 86), ("Anitha", "Physics", 62), ("Anitha", "Chemistry", 75), ("Anitha", "English", 83), 
  	("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60))
  
  
      	val seq_op = (z: (Int, Int), v: Int ) => (z._1 + v, z._2 + 1)      
      	val comb_op = (a: (Int, Int), b: (Int, Int)) => (a._1 + b._1, a._2 + b._2)      
      
      	val students_rdd = sc.parallelize(students_list, 3)
                           .map(x => (x._1, x._3) )
                           .aggregateByKey((0,0))(seq_op, comb_op)
                           .mapValues( x => x._1.toDouble/x._2 )
      
      	students_rdd.collect.foreach( println )


    21. joins => join, leftOuterJoin, rightOuterJoin, fullOuterJoin


    22. cogroup		=> Used when you want to Join RDDs with duplicate keys.
			=> Apply groupByKey on each RDD and then apply fullOuterJoin
      


  RDD Actions
  -----------

  1. collect

  2. count

  3. saveAsTextFile

  4. reduce		  	(U, U) => U
				reduces the entire RDD to one value of the same type by iterativly appyling the 
				function on all the partitions and then on the results of each partition.
             	rdd1		
		P0 : 5, 4, 3, 7, 8, 9, 0	=> reduce => 36 => 99
		P1 : 9, 0, 6, 7, 5, 4, 6	=> reduce => 37
		P2 : 1, 2, 3, 2, 4, 3, 5, 6	=> reduce => 26
			
			val s  = rdd1.reduce( (x, y) => x + y )	

   5. aggregate		   -> Reduces the entire RDD to a type different than the type of elements using
			      a zero-value. The final output is of the type of zero-value (not of the type 
			      of elements)

			      Three parameters:  RDD[U]

			      1. zero-value : Z (type of zero-value)
			      2. seq-operation:  Operates on each partition and folds the elements with the 
						 zero-value.
					         (Z, U) => Z     (similar to scala 'fold' HOF)
			      3. combine operation: Reduces all the values of each partition produced by 
						    seq-operation using a reduce function.
						  (Z, Z) => Z						
			rdd1: 
			P0: 8, 3, 8, 9, 8, 3   => (39, 6)  => (100, 18)
			P1: 4, 2, 1, 4, 6, 7   => (24, 6)
			P2: 8, 9, 8, 5, 6, 1   => (37, 6)

			
			rdd1[U].aggregate(zv: Z)(seq-fn: (Z, U) => Z, comb-fn: (Z, Z) => Z)

			rdd1[U].aggregate( (0,0) )( (z, v) => (z._1 + v, z._2 + 1) , 
						    (a, b) => (a._1 + b._1, a._2 + b._2) )

   6. first

   7. take
		rdd1.take(15)

   8. takeOrdered
		rdd1.takeOrdered(15)
		rdd1.takeOrdered(15)(Ordering[Int].reverse)

   9. takeSample
		 rdd1.takeSample(true, 10)
		 rdd1.takeSample(true, 10, 464)
		 rdd1.takeSample(true, 100, 464)

		 rdd1.takeSample(false, 10)
		 rdd1.takeSample(false, 10, 464)

   10. countByValue

		rdd1.countByValue()
		res106: scala.collection.Map[Int,Long] 
		    = Map(0 -> 2, 5 -> 3, 1 -> 1, 6 -> 3, 9 -> 2, 2 -> 2, 7 -> 2, 3 -> 3, 8 -> 1, 4 -> 3)
 
   11. countByKey      => Applied on pair RDD
			  Returns a Map with how many times each key is repeated. 
		rdd2.countByKey
		res110: scala.collection.Map[Int,Long] = Map(5 -> 7, 1 -> 7, 6 -> 7, 2 -> 7, 7 -> 7, 3 -> 7, 8 -> 7, 4 -> 7)


   12. foreach

   13. foreachPartition
		rdd1.foreachPartition(p => println(p.toList.sum))

   14. saveAsSequenceFile
		rddWc.saveAsSequenceFile("E:\\Spark\\output\\seq")

   15. saveAsObjectFile
		rddWc.saveAsObjectFile("E:\\Spark\\output\\obj")


   
   Jobs, Stages and Tasks
   =======================
	
	1. A single spark application can have multiple jobs.
	2. Each "Action command" launches a new Job
	3. Each job can have one or more stages that are executed sequentially (one AFTER another)
	4. Each stage will have one or more tasks that run in parallel
		(Each task may have several transformations)
	5. each "Wide transformation" will cause a new stage to be launched.


   Use-case
   --------
	
     Dataset: https://github.com/ykanakaraju/sparkscala/blob/master/data/cars.tsv

     => From cars.tsv dataset, find the average-weight of each make of American cars.
	 -> Arrange the data in the DESC order of average weight
	 -> Save the output as a single text file.

        => Try it yourself..


  Closure
  --------
	In Spark, a Closure is all the variables and methods that must be visible inside an executor for it to
	perform its computation on RDD partitions. 

	=> The Driver serliazes the closure and a separate copy is sent to ever executor.

   
		var c = 0

		def isPrime(n) = {		    
		    var flag = true
                    for(i <- 2 to n-1) {
			if (n % i == 0) flag = false
		    }
		    flag
		}

		def f1(n) = {
		   if (isPrime(n)) c += 1
                   n * 2 
		}	

		val rdd1 = sc.paralleize( 1 to 4000, 4 )

		val rdd2 = rdd1.map( f1 )

		rdd2.collect		

		println( c )   // c = 0   

	
	Limitation: We can not use 'local variable' which are part of the closure to implement
		    global counter. 

	Solution: Use "Accumulator" variable. 



  Spark Shared Variables
  -----------------------

      1. Accumulator variable
      =======================
	=> Is not part of function closure, hence it is not a local copy.
	=> Maintained by the driver	
	=> Accumulators are used to implement global counters.

	var c = sc.longAccumulator("counter")     // count of prime numbers

	def iSPrime( n: Int ) : Boolean = {
	    return true if n is prime
	    else return false
	}

	def f1(n: Int) = {
            if ( isPrime(n) ) c.add(1)
	    n * 2
	}

	val rdd1 = sc.parallelize( 1 to 4000, 4 )
	val rdd2 = rdd1.map( f1 )

	rdd2.collect

	println(s"c = ${c.value}")  // c = 0


      2. Broadcast variable
      ======================
	=> Only one copy of the broadcast variable is sent to every executor node
	=> All tasks running in that executor, will lookup from that copy.
	=> Use it, to broadcast large immutbale lookup tables/maps etc to save memory.

           val map = sc.broadcast(Map(1 -> Emp(1), 2 -> Emp(2), 3 -> Emp(3), .....))     # 100 MB
		
	   def f1(n: Int) : Option[Emp] = {              
               map.get(n)     
	   }

	   val rdd1 = sc.parallelize( 1 to 4000, 4 )
	   val rdd2 = rdd1.map( f1 )

	   rdd2.collect
		






































