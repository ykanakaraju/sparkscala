
  Agenda (Spark using Scala)
  --------------------------
   -> Scala Refresher 
   -> Spark - Basics & Architecture
   -> Spark Core API
	-> RDD - Transformations and Actions
	-> Spark Shared Variable
        -> Spark-Submit
   -> Spark SQL
	-> DataFrame Operations
	-> Integration with MySQL & Hive
	-> SQL Optimizations & Tuning
   -> Spark Streaming
	-> DStreams API (introduction)
	-> Structured Streaming

  Materials
  ---------
	=> PDF Presentations
	=> Code Modules
	=> Class Notes
	=> Github: https://github.com/ykanakaraju/sparkscala


   Getting Started with Spark & Scala
   ----------------------------------

     1. vLab - Lab allocated to you. 
	
	 -> Follow the instructions given in the attached document.

	 -> You will be loging into a Window Server
	     -> Here you find a document on the desktop with useris and password. 

	 -> Click on the "Oracle VM Virtualbox" and connect to Ubuntu lab. 
	 => here you can open a terminal and connect to Spark shell (type "spark-shell")
	 => You can also launch "Eclipse" 

     2. Setting up your own environemnt on your personal machine. 

	   Pre-requisite: Java 8
	   => Open a terminal and type "java -version" (it has to be 1.8.xxx or up)
		
	   1. Scala IDE (version of Eclipse)

		URL: http://scala-ide.org/download/sdk.html

		Download Scala IDE for your OS and unzip it to a suitable location
		Navigate into the unzipped folder and click on "Eclipse" application icon to launch Scala IDE.

	   2. IntelliJ

	       Follow the instructions @ https://docs.scala-lang.org/getting-started/index.html					 
		
	       Two build tools for Scala:
		-> Maven (Scala IDE + Maven)  
		-> SBT  (IntelliJ + SBT)
		
     3. Signup to "Databricks Community" Edition Free account.

		URL to Signup : https://databricks.com/try-databricks
		URL to Login: https://community.cloud.databricks.com/login.html


  Scala Refresher
  ---------------
  
   => Scala : SCAlable LAnguage
	      Compiler based language (based on Java)
	      Interoperable with Java

   => Scala is a multi-paradigm programming language
	 -> Scala is a "Pure" object oriented prog. lang.
	 -> Scala is a functional programming language

  Pure OOP => Scala does not have primitives and operators

  -> Scala Variables :  immutables -> val
		      mutables  -> var
	-> Scala prefers immutables. 

  -> Scala has implicit type inference

  -> Scala uses infix notation
	obj.method(param) => obj method param

  -> Scala is a statically typed language
	-> Need to know the type of every object at compile type

  Scala is PURE object oriented language
  --------------------------------------
	
      -> Scala does not have primitives or operators.	
      -> In scala, all data is objects and operations are method invocations.

	  val i = 10.*(40)  
   
             -> 10 is an Int object
	     -> * is a method invoked on 10 (Int object)
	     -> 40 is an Int object passed as a parameter
	     -> i is an Int object returned by the * method.
		
      -> <obj>.method<param1> => <obj> method<param1> => <obj> method param1


  -> Blocks
	 => A block any code enclosed in  { }
	 => A block is scala has a return value.
	 => The block returns the value of the last expression that is executed.

         Scala Unit => In Scala "Unit" is an object that represents "no value"
		       prined as "()"

   -> Unit : Is a value class in Scala which represents 'no value'
             Printed as "()"

   -> Scala imports
	
        import scala.io.StdIn		  => single class
        import scala.io.{StdIn, Source}   => multiple classes
	import scala.io._                 => all classes

   -> User Input

		val name = StdIn.readLine("What is your name ?")
    		println("Hello " + name)     
    		println("What is your age ?")
    		val age = StdIn.readInt()
    		println("Age : " + age) 

    -> Output

		printf("Name: %s, age: %2.2f", "Kanakaraju", 47d)
		println(name, age)

    -> String interpolations

		s interpolator: s"Name: $name, Age: ${age + 10}"
		f interpolator: s interpolator + formting chars
				 f"Name: $name, Age: ${age + 10}%2.2f"
		raw interpolator : will escape the escape chars

		 	val filePath = raw"E:\newdir\total\red.txt"
    		 	println(filePath)


   Flow control constructs
   -----------------------

   if..else if.. else

	if statement can return a value
	val x = if (i > 100) "> 100" else if(i < 100)  "< 100" else "== 100"

   match ..case

	   val output = i match {
     		case 10 => "ten"
     		case 20 => "twenty"
     		case 30 => "thirty"
     		case 40 => "fourty"
     		case x if (x % 2 == 0) => s"even number"
     		case y if (y % 2 != 0) => s"odd number"
     		case _  => "no match"
   	   }

  Scala Class Hierarchy
  ---------------------
	Any =>  AnyVal => Int, Double, Long, Char, Byte, Unit, Boolean, ...
            =>  AnyRef => String, List[U], all other classes

  Tuple
  ------

    -> Is a class which can hold multiple elements of different type.
	
	val t1 = (10, 20, 10.5, true, "Hello")
	t1: (Int, Int, Double, Boolean, String) = (10,20,10.5,true,Hello)


	val t2 = (10, List(10, 20, 30), Seq(10.5, 12.2), "hello")
	t2: (Int, List[Int], Seq[Double], String) = (10,List(10, 20, 30),List(10.5, 12.2),hello)

      ** A tuple with two elements is called a "Pair"


  Collections
  -----------

     Array           -> mutable collection, fixed-length
     ArrayBuffer     -> mutable collection, variable-length

     => Array: mutable & fixed length
     => ArrayBuffer: mutable with variable length

     Immutable Collections
    
        -> Seq  (Sequences)
	     -> Ordered collections and elements can be accessed using an index.

	     -> Indexed Sequences
		 -> Vector
		 -> Range

		 => Optimized for fast random-acccess		

	     -> Linear Sequences
		-> List
		-> Queue
		-> Stream 

		=> Optimized for visiting the elements linearly (i.e in a loop)
		=> They are organized as linked lists
		
		List(1,2,3,4,5,6) => List(1, List(2,3,4,5,6))   
			// here 1 is head, List(2,3,4,5,6) is tail
		        => List(1, List(2, List(3, List(4, List(5, List(6, List())))))) 

		list => List(head, tail)

	-> Set
	     -> Is a unordered collection of unique values.
	     -> We can NOT access the elements using an index.
	     -> SortedSet, BitSet

	-> Map
	     -> A collection of (K, V) pairs

		val m1 = Map( (1, 'A'), (2, 'B'), (3, 'C') )
		val m1 = Map( 1 -> "A", 2 -> "B", 3 -> "C" )

                m1(1) -> "A"
		m1(10) -> raise java.util.NoSuchElementException


  Range
  ------
		Range(0, 100, 10)  => 0: start, 100: end (excluded), 10: step
		Range(0, 10)  => step is one if not specified
		Range(10, 1, -1) =>  10, 9, 8, 7, 6, 5, 4, 3, 2
		Range(100, 0, -10) => 100, 90, 80, 70, 60, 50, 40, 30, 20, 10
		Range(0, 100, -2) => Empty
		
		0 until 10 by 2 => Range(0, 10, 2)
		0 to 10 by 2    => Range(0, 11, 2)

  
  Option
  -------
        -> reprsents an option (i.e value may be or may not be there)
	-> returns instances of 'Some' class or 'None' class


  Loops
  ------

     while

		while( i > 0 ){
            		print(i + " ")         
            		i -= 1
        	}

     do..while

		do {
         		print(i + " ")         
         		i -= 1
      		} while( i > 0 )

     foreach

	        <collection>.foreach( function )
		(1 to 10).foreach( println )

     for

		for(<generator>, <generator>, ...) { .. }

		for( i <-  1 to 10 by 2){
        	    println(s"i = $i")
     		}

		for( i <-  1 to 10 by 2; j <- 1 to 20 by 4 ){
        	    println(s"i = $i, j = $j")
     		}

		for( i <-  1 to 10 by 2 if (i != 3); j <- 1 to 20 by 4 if (j > i) ){
       		    println(s"i = $i, j = $j")
     		}

		for comprehension
		-----------------
		val l1 = for(i <- 1 to 100 by 2) yield(i)


    Exception Handling
    ------------------

        try {
		<some code that could throw an exception>
        }
        catch {
	    case e: ArrayIndexOutOfBoundException  => { .... }
	    case e: FileNotFoundException => { .... }
	    case _: Exception => { .... }
			
        }
        finally {
		<some code that is always executed
        }

     Example
     =======

      try {
         println("Reading from a file ...")   
         val f = new FileReader("file1rtyryr.txt")            
      } 
      catch {
         case ex: FileNotFoundException => {
            println("FileNotFoundException exception")
            println( ex.getMessage )
            println( ex.getStackTrace )
         }         
         case _: IOException => {
            println("IO Exception")
         }
         case _: Exception => {
            println("generic exception")
         }
      } 
      finally {
         println("Exiting finally...") 
      }  

       
   Methods
   --------
	
     => method is a reusable code blocks with a name declared using def keyword.

	-> methods can be called by positional arguments
	-> methods can be called by named arguments
	-> method arguments can have default values
	-> methods can take parameter lists

		def f1(a: Int, b: Int)(c: Int) = ( a + b ) * c     
     		val i = f1(10, 20)(30)     
     		println(i)

	-> methods can have one variable length argument (last argument)
		-> You can have default values for args if you are using veriable-length argument

		def sum(a: Int, b: Int*) : Int = {
        		var s = 0
        		for (i <- b) s += i
        		a + s
     		}       
     		val s1 = sum(10)

	-> methods can be called recursivly

		def factorial(n: Int) : Int = {
        		if (n < 2) 1
        		else n * factorial(n - 1)
     		}
     
     		println( factorial(6) )

	-> methods can be nested 

  Procedure
  ---------
     -> Procedure is similar method, but always returns "Unit"
     -> Syntactically there is no "=" symbol in the definition  

	def box(name: String) {
        	val line = "-" * name.length + "----"        
        	println(line + "\n| " + name + " |\n" + line)        
     	}     
     
     	box("Scala is a programming language")


  Function
  --------

	=> A function is treated as a literal value (such as 10, "hello" etc)
	=> A function has a value (of its own)
	=> A function has a type
	=> A function is anonymous (by nature)

	=> A function can be assigned to a variable
		val add = (x: Int, y: Int) => x + y

	function literal			function type
        -----------------------------------------------------
	(x: Int, y: Int) => x + y		(Int, Int) => Int
	(x: String, y: Int) => x * y		(String, Int) => String
	() => "Windows 10"			() => String
	(name: String) => println(name)		String => Unit
	(l: List[Int]) => l.sum			List[Int] => Int
	(t: (Int, Int)) => t._1 + t._2		 ((Int, Int)) => Int


        => A function can be passed as argument to another method/function

		def calculate(a: Int, b: Int, f: (Int, Int) => Int) = {
       			f(a, b)
     		}
		val result = calculate(345, 20, (x: Int, y: Int) => x % y)     
     		println( result )

       => A method/function can return a function as its return value
		-> A block can return a function as final value
	
		def compute(op: String) = {       
       			op match {
          			case "+" => (x : Int, y: Int) => x + y
          			case "-" => (x : Int, y: Int) => x - y
          			case "*" => (x : Int, y: Int) => x * y
          			case "/" => (x : Int, y: Int) => x / y
          			case _ => (x : Int, y: Int) => x % y
       			}       
     		}        
     		println( compute("+")(321, 10) )


   Higher Order Functions
   ----------------------
	=> Take a function as an argument
	=> Applied on some Iterable object. 

   1. map		P: U => V
			Transforms each object of the input to another obejct in the output
			input: N elements, output: N element

		l1.map( s => s.split(" ").length )

   2. filter		P: U => Boolean
			The output collection will have only those objects for which the funtion
			returns true. 
			input: N elements, output: <= N element

		l4.filter( t => t._1 + t._2 >= 10 )

   3. flatMap		P: U => GenTraversableOnce[V]    (U => Collection)
			flattens the output of the function
			input: N elements, output: >= N element

		words.flatMap(x => x.toUpperCase)

    4. reduce        => reduceLeft (reduce) & reduceRight
			P: (U, U) => U
			Reduces the entire input collection to one value of the 'same type' by 'iterativly
			applying' the function

		List(2,1,2,4,5,4,3,2).reduce( (x, y) => x - y )  => 23
		
    5. sortWith		P: binary sorting function
			Elements of the collections are sorted based on the sorting function.

		l4.sortWith( (m,n) => m._1 > n._1 )  
    
     6. groupBy		P: U => V
			Returns a Map objects, where
			   key: Each unique value of the function output
			   value: List object containing all the objects that produced the key.

			List[U].groupBy( U => V ) => Map[ V, List[U] ]

     7. mapValues	P: U => V
			Applied only on (key, value) pairs i.e Map objects
			It will transform only the value of the (K, V) pairs

			val m2 = m1.mapValues( x => (x,x) )

     8. fold		=> foldLeft (fold) & foldRight
		
  
  WordCount Program
  -----------------

     val filePath = "E:\\Spark\\wordcount.txt"
     
     val wc = Source.fromFile(filePath)
              .getLines()
              .toList
              .flatMap( x => x.split(" ") )
              .groupBy( x => x )
              .toList
              .map( x => (x._1, x._2.length ) )
              .sortWith( (a, b) => (a._2 > b._2) )

    val wc = Source.fromFile(filePath)
              .getLines()
              .toList
              .flatMap( x => x.split(" ") )
              .groupBy( x => x )
              .mapValues( x => x.length )
              .toList
              .sortWith( (a, b) => (a._2 > b._2) )
              
     wc.foreach(println)

  ========================================================  
    Spark
  ========================================================

   => Spark is an in-memory distributed computing framework.

   	 Cluster => A unified entity containing many nodes whose cumulative resources can be used to
               	    distribute store and/or computations of big data. 

     	in-memory computing -> The intermediate results of distributed computing can be saved in-memory
			       and subsequent tasks can run on these saved partitions. 

        => Spark is 100x faster than MapReduce if you use 100% in-memory processing
	   Spark is 6 t 7x faster than MapReduce even if you use disk based processing

   => Spark is written in Scala. 

   => Spark is a polyglot
	-> Support supports Scala, Java, Python and R

   => Spark provides a "Unified framework" for analytical workloads

	Spark provides a set of consistent APIs that can used to process different analytical workloads
	based on the same execution engine. 

	 -> Batch Analytics on unstructured data	: Spark Core API
	 -> Batch Analytics on structured data		: Spark SQL
	 -> Stream Analytics (real-time processing)	: Spark Streaming, Strcutured Streaming
	 -> Predistive analytics (using ML)		: Spark MLlib
	 -> Graph parallel computations			: Spark GraphX

   => Spark applications can be submitted to multiple cluster managers
	=> local, spark standalone, YARN, Mesos, Kubernetes 
	

    Spark Architecture
    ------------------

    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.


   Getting started with Spark
   --------------------------	
	Popular Options : 
	
	=> Scale-IDE for Eclipse + Maven
		=> Add required dependencies from Central maven repo
		=> The required packges are download and added to your project.
	=> IntelliJ + SBT

        Central maven repository: https://mvnrepository.com/ (search for Spark)

	Downloading "winutils.exe" to your local directory
		=> Create the following folder: C:\hadoop\bin
		=> Download an appropriate version of winutils.exe from the following url;
			https://github.com/steveloughran/winutils
    

   RDD (Resilient distributed dataset)
   -----------------------------------
    -> RDD is the fundamental data abstraction for in-memory data in Spark core API
    -> RDD is a collection of distributed in-memory partitions
	-> A partition is a collection of objects (of any type)

    -> RDDs are immutable

    -> RDDs are lazily evaluated
	-> Transformations does not cause execution. 
           They only create lineage LAGs
        -> Action commands trigger execution.

    -> RDD are resilient
	-> RDDs can create any missing partitions on the fly. 

  Creating RDD
  ------------
	Three ways:

	1. Create an RDD from external data file such as text files

		val rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)


	2. Create an RDD from programming data

		val rdd1 = sc.parallelize( List(1,2,1,3,2,4,5,3,5,6,7,8,9,8,9,6,7,1,2,4,2,3,4,5,6,2,1), 3 )


        3. By applying transformations on existing RDDs

		val rddWords = rddFile.flatMap(x => x.split(" "))


  RDD Operations
  --------------

    Two things:

	1. Transformations	
		-> Returns an RDD
		-> Does not cause execution
		-> Transformation only create RDD Lineage DAGs

        2. Actions
		-> Trigger execution of the RDD
		-> Produces output
		-> Converts thr logical Plan into a physical execution plan.


  RDD Lineage DAG
  ----------------

   => RDD Lineage is a logical plan maintained by the driver
   => RDD lineage DAG contains the hierarchy of dependencies all the way from the very first RDD.
	

	 val rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

(4) E:\Spark\wordcount.txt MapPartitionsRDD[7] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[6] at textFile at <console>:24 []


	val rddWords = rddFile.flatMap(x => x.split(" "))

(4) MapPartitionsRDD[9] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[7] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[6] at textFile at <console>:24 []

 
	val rddPairs = rddWords.map(x => (x,1))

(4) MapPartitionsRDD[10] at map at <console>:25 []
 |  MapPartitionsRDD[9] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[7] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[6] at textFile at <console>:24 []

    
	val rddWc = rddPairs.reduceByKey( (a, b) => a + b )
   
(4) ShuffledRDD[11] at reduceByKey at <console>:25 []
 +-(4) MapPartitionsRDD[10] at map at <console>:25 []
    |  MapPartitionsRDD[9] at flatMap at <console>:25 []
    |  E:\Spark\wordcount.txt MapPartitionsRDD[7] at textFile at <console>:24 []
    |  E:\Spark\wordcount.txt HadoopRDD[6] at textFile at <console>:24 []

   

  RDD Persistence
  ---------------

	val rdd1 = sc.textFile( <file>, 4 )
	val rdd2 = rdd1.t2( ... )
	val rdd3 = rdd1.t3( ... )
	val rdd4 = rdd3.t4( ... )
	val rdd5 = rdd3.t5( ... )
	val rdd6 = rdd5.t6( ... )
	rdd6.persist( StorageLevel.DISK_ONLY )  --> instruction to spark to save rdd6 partitions.
	val rdd7 = rdd6.t7( ... )

	rdd6.collect()

	Lineage of rdd6 => rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[sc.textFile, t3, t5, t6] -> collected

	rdd7.collect()
	
	Lineage of rdd7 => rdd7 -> rdd6.t7
		[t7] -> collected

	rdd6.unpersist()


	StorageLevels
        -------------
	1. MEMORY_ONLY	       -> default, Memory deserialized 1x replicated
	2. MEMORY_AND_DISK     -> Disk Memory deserialized 1x replicated
	3. DISK_ONLY	       -> Disk serialized 1x replicated
	4. MEMORY_ONLY_SER     -> Memory serialized 1x replicated
	5. MEMORY_AND_DISK_SER -> Disk Memory serialized 1x replicated
	6. MEMORY_ONLY_2       -> Memory deserialized 2x replicated
	7. MEMORY_AND_DISK_2   -> Disk Memory deserialized 2x replicated

	Commands
	--------
		rdd1.cache()    => memory-only
		rdd1.persist()	=> memory-only

		import org.apache.spark.storage.StorageLevel
		rdd1.persist( StorageLevel.MEMORY_AND_DISK)

		rdd1.unpersist()

   Executor memory structure
   ==========================

   	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   Types of Transformations
   ------------------------

	-> Narrow Transformations
		-> is a transformation that does not cause data shuffling
		-> partition to partition transformations
		-> number of partitions of the output rdd is equal to that of input rdd

	-> Wide Transformations
		-> cause shuffling of the data across various input partitions
		-> number of partitions of the output rdd can be different than that of input rdd

  RDD Transformations
  -------------------

  1. map		P: U => V
			Object to object transformation
			input RDD: N objects; output RDD: N objects
	
   	rddFile.map(x => x.split(" ").length ).collect

  2. filter		P: U => Boolean
			Filter only those objects for which the function returns true
			input RDD: N objects; output RDD: <= N objects
	
	rddFile.filter(x => x.split(" ").length > 8).collect

  3. glom		P: None
			Returns one Array per partition with all the objects of the partition.

		rdd1		      rdd2 = rdd1.glom()
		P0: 3,2,1,3,2,4,5,6 -> glom -> P0: Array(3,2,1,3,2,4,5,6)
		P1: 4,3,1,3,6,7,8,1 -> glom -> P1: Array(4,3,1,3,6,7,8,1)
		P2: 3,2,5,7,5,8,9,1 -> glom -> P2: Array(3,2,5,7,5,8,9,1)
	
		rdd1.count = 24 (Int)		rdd2.count = 3 (Array[Int])

		rdd1.glom.map(x => x.sum).collect

  4. flatMap		P: U -> TraversableOnce[V]
			flattens the iterables produced by the function
			input RDD: N objects; output RDD: >= N objects

		rddWords.flatMap(x => x.toUpperCase).collect

  5. mapPartitions	P: Iterator[U] => Iterator[V]
			Transforms each input partition to corresponding output partition

		rdd1		   rdd2 = rdd1.mapPartitions( p => p.map(x => x*10) )
		P0: 3,2,1,3,2,4,5,6 -> mapPartitions -> P0: 30,20,10,30,20,40,50,60
		P1: 4,3,1,3,6,7,8,1 -> mapPartitions -> P1: 40,30,10,30,60,70,80,10
		P2: 3,2,5,7,5,8,9,1 -> mapPartitions -> P2: 30,20,50,70,50,80,90,10
   

   6. mapPartitionsWithIndex	P: (Int, Iterator[U]) => Iterator[V]
				Same as mapPartitions but you get partition index as an additional parameter.

		rdd1.mapPartitionsWithIndex( (i, p) => List((i, p.sum)).iterator ).collect

		rdd1.mapPartitionsWithIndex((i, p) => p.map(x => (i, x))).filter(x => x._1 == 0).map(x => x._2).collect


    7. distinct			P: None, Optional: numPartitions
				Returns an RDD with unique objects of the input RDD.

   		rddWords.distinct.collect
		rddWords.distinct(5).collect


    Types of transformations   => to be discussed ...







