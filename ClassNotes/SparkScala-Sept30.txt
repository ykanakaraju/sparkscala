
  Agenda (Spark using Scala)
  --------------------------
   -> Scala Refresher
   -> Spark - Basics & Architecture
   -> Spark Core API
	-> RDD - Transformations and Actions
	-> Spark Shared Variable
        -> Spark-Submit
   -> Spark SQL
	-> DataFrame Operations
	-> Integration with MySQL & Hive
	-> SQL Optimizations & Tuning
   -> Spark Streaming
	-> DStreams API (introduction)
	-> Structured Streaming

  Materials
  ---------
	=> PDF Presentations
	=> Code Modules
	=> Class Notes
	=> Github: https://github.com/ykanakaraju/sparkscala

  Scala
  -----
	
   -> Scala is Java based compiler language
        -> Scala compiles to Java byte code
	-> Interoperable with Java language
	
    -> Scala is multi-paradigm programming language	
	 -> Pure OOP language
         -> Functional programming language

    -> Pure OOP language
	 -> Scala does not have primitive and operator

   -> Scala Variables: immutables -> val
		       mutable    -> var

   -> Scala infers the type automatically based on the assigned value.

   -> Scala infix notation : 
		
	val j = i.+(10)  can be written in scala as:
	val j = i + 10

   -> Scala is a statically typed language
	-> The type of every object/variable is know at compile time. 
	-> Once assigned a type can not be changed. 

  Blocks
  ------

	-> One or more statements enclosed in { .. }
	    -> If there is only one statement/expression then you may omit the { .. }
	-> A block returns an ouput
	    -> The output is the value of the last statement that is executed in the block.
	-. Scala does not use "return" keyword

  Unit
  ----
	-> Is a class that represents 'no value'
        -> Printed as "()"  	

  Flow Control Statements
  -----------------------

   if..else
   ----------

	if (<boolean>) { ... }
	else if (<boolean>) { ... }
	else { ...}

	=> If statement in Scala, retruns a value

   match..case
   -----------
	
	 output = i match {
     		case 10 => "ten"
     		case 20 => "twenty"
     		case _ if (i % 2 == 0) => "even number"
     		case _  => "default output" 
  	}
  

   Scala Class Hierarchy
   ---------------------

	Any => AnyVal => Int, Long, Float, Double, Boolean, Unit, Byte, Char
            => AnyRef => String, <all other classes>


   Range
   -----
	Is a collection which store the starting value, end value and a step.

	Range(1, 10)       => 1,2,3,4,5,6,7,8,9
	Range(1, 10, 2)    => 1,3,5,7,9
	Range(100, 0, -20) => 100,80,60,40,20
	Range(1, 10, -2)   => Empty 

	1 to 10 by 2       => 1, 3, 5, 7, 9
	100 to 0 by -20	   => 100,80,60,40,20,0   ('to' returns Range.Inclusive)
        100 until 0 by -20 => 100,80,60,40,20,0 
	     
   Loops
   -----
   1. while

	var i = 0
   
   	while( i < 10 ) {
     	   println(s"i = $i")     
     	  i = i + 1     
   	}

   2. do while

	 do {
           println(s"i = $i")     
          i = i + 1     
        } while( i < 10 )

   3. foreach

	<collection>.foreach( fn )

	function "fn" is executes on all the objects of the collection.

	def f1(i: Int) = { println(i) }    
    	List(1,2,3,4,5,6,7,8,9).foreach( f1 )

   4. for 

	for( <generator(s)> ) {
        }

	for( x <- 1 to 10 by 2 ){
      		println(x)
    	}

	for( x <- 1 to 10 by 2; y <- 1 to 20 by 4){
      	    println(x, y)
        }

        for( x <- 1 to 10 by 2 if (x != 5); y <- 1 to 20 by 4 if (y > x)){
           println(x, y)
        }

        for comprehension: Collects the data looped by for loop using "yield" function and returns a collection.  

		val v1 = for( x <- 1 to 10 by 2 if (x != 5); y <- 1 to 20 by 4 if (y > x)) yield( (x, y) )
		println( v1 )

		Output:
		Vector((1,5), (1,9), (1,13), (1,17), (3,5), (3,9), (3,13), (3,17), (7,9), (7,13), (7,17), (9,13), (9,17))



   Interpolators
   --------------
      => Interpolator evaluates a string.

	1. 's' interpolator
		s"x = $x, x+1 = ${x+1}, y = $y"

        2. 'f' interpolator  => s interpolator + fromatting chars
		f"x = $x%1.1f, x+1 = ${x+1}, y = $y"

        3. 'raw' interpolator => will escape the escape chars

		val filePath = raw"E:\Spark\new\wordcount.txt"

  Exception Handling
  ------------------
	try {
	    // write you that could throw an exception. 
	}
	catch {		
	   case e1: FileNotFoundException => { .... }
           case e2: ArrayIndexOutOfBoundsException => { .... }
	   case _:Exception => { .... }
	}
	finally {
	    // write code that is always executed.
	}
       

   Getting started with Scala
   --------------------------

     1. Working in your vLab
	   => Follow the instructions on the attached document that you receive by email.
           => You log into a Ubuntu/CentOS VM
	   => You can launch Scala Shell & can start Scala IDE

      2. Installing Scala IDE on your personal machine. 

	  2.1 Scala IDE for eclipse

	  	=> Make sure you have Java 8 (jdk 1.8 or up) installed.

	  	=> Download and extract Scala IDE for eclispe from the following url link:
			http://scala-ide.org/download/sdk.html
	     	=> Nivagate inside the extracted folder (such as scala-SDK-4.7.0-vfinal-2.12-win32.win32.x86_64)
			and click in 'scala ide' icon to launch the IDE.

	  2.2 IntelliJ

		Instruction on how to install and setup IntelliJ for Scala are described here:
		https://docs.scala-lang.org/getting-started/index.html (near to 'Open hello-world project')

      3. Signup to Databricks community account
		=> https://www.databricks.com/try-databricks

      4. Online Scala Compilers
		https://scastie.scala-lang.org/



   Tuple
   ------
     => Tuple is an object that can hold multiple objects of different type
	-> A tuple with two objects is called a Pair

	 val t1 = (10, 10.5, "Hello")
   	 -> t1: (Int, Double, String) = (10,10.5,Hello)

	print( t1._1, t1._2, t1._3 )


   Methods
   -------
    -> Callable/reusable code block


	def add(a: Int, b: Int, c: Int) : Int = {
		a + b + c
	}

        -> Methods can be called by position
		val s = add(10, 20, 30)

        -> Methods can be called using named arguments
		val s = add(b=10, c=20, a=30)
  
	-> Method arguments can have default values.	
	
		def add(a: Int, b: Int = 0, c: Int = 0) : Int = {
			a + b + c
		}
  		val s = add(10, 20, 30)
		val s = add(10, 20)
		val s = add(10)

	-> Methods can have variable-lenght arguments
		-> Only one variable-lengh arg. is allowed and it should be the last argument.
		-> If can ot have default values when using variable-lengh arguments

		def add(a: Int, b: Int*) : Int = {
       			var s = a
       			for (i <- b) s += i
       			s
    		}   
    
    		val s = add(10, 20, 30, 40, 50)

       -> Methods can be called recursivly

		def factorial(n : Int) : Int = {
       		    if (n == 1) 1
       		    else n * factorial(n-1)
    		}

       -> Nested-methods
	
		def factorial2(i: Int): Int = {
      		   def fact(i: Int, accumulator: Int): Int = {
         		if (i <= 1)
            			accumulator
         		else
            			fact(i - 1, i * accumulator)
      		   }
      		   fact(i, 1)
   		}

       -> Methods can have multiple parameter lists

		def add(a: Int, b: Int)(c: List[Int]) = {
        		a + b + c.sum
     		}
     
     		val s = add(10, 20)( List(10,20,30) )

   Procedure
   ---------
	=> A procedure always returns Unit. 

		def box(name: String) {
      		   val line = "-" * name.length + "----"       
      		   println( line + "\n| " + name.toUpperCase + " |\n" + line )
                }

   Functions
   ---------
	=> In FP languages, function is treated as a literal (just like 10, "Hello", true)
	=> A function, by nature, is anonymous

	=> A function can be assigned to a variable
               val f1 = (a: Int, b: Int) => a + b
	
	=> A function can be passed as a parameter to another method/function

		def m1(a: Int, b: Int, f: (Int, Int) => Int ) = { a + b + f(a, b) }     
   		val x = m1(10, 20, (a, b) => a + b )

	=> A function/method can return a function as the final value.

		def compute(op: String) : (Int, Int) => Int = {
         		op match {
           			case "+" => (a: Int, b: Int) => a + b
           			case "-" => (a: Int, b: Int) => a - b
           			case "*" => (a: Int, b: Int) => a * b
           			case "/" => (a: Int, b: Int) => a / b
           			case _ => (a: Int, b: Int) => a % b
         		}
      		}
      
      		val f1 = compute("blah")     
            
      		println( f1, f1(100, 15) )

	
	Function Literal				Function Type
	--------------------------------------------------------------------
	(n: String) => n.toUpperCase			String => String
	(i: Int, j: Int) => i + j			(Int, Int) => Int
	(s: String, i: Int) => s * i			(String, Int) => String
	(S: String) => print(s)				String => Unit
	() => "Windows 10"				() => String
	(p: (Int, Int)) => p._1 + p._2			((Int, Int)) => Int
	(l: List[Int]) => l.length			List[Int] => Int



   Higher Order Functions
   ----------------------
       => Are functions/methods that take a function as an argument (or return function as a return value)
       => Operated on some collections
               <collection>.higher-order-fn( fn ) => <modifed-collection>
		
	  
    1. map	  	P: U => V
			map transforms the input elements by applying the function.
			input: N objects, output: N objects

		List(1,2,1,6,3,7,8,5,7,6,9,0,7,4).map( x => (x, x*x) )

    2. filter		P: U => Boolean
			Only those elements from the input collection for which the function returns true
			will be there in the output collection.
			input: N objects, output: <= N objects

		l1.filter(x => x.length > 51)

    3. flatMap		P: U => GenTraversableOnce[V]
			flatMap flattens the collection objects returned by the function.
			input: N objects, output: >= N objects

    		val words = l1.flatMap(s => s.split(" "))

     4. reduce  (reduceLeft/reduce & reduceRight)
			P: (U, U) => U
			Will reduce the entire input collection to one object of the same type
			by iterativly applying the function. 

	 	l1 => List(2,1,3,2,4,5,7,4)   => 4
		l1.reduceRight( (a, b) => a - b )

     5. sortWith	P: Binary sorting function

		t1.sortWith( (a, b) => a._2 > b._2 )
		words.sortWith( (a, b) => a(a.length - 1) < b(b.length - 1))

     6. groupBy		P: U => V
			Objects of the input collection are grouped based on the function output
			Returns scala.collection.immutable.Map[V,List[U]] where:
				each unique value of the fn output is the key (K)
				all objects of the collection that produvced the key forms the value (V)

			List[U].groupBy( U => V ) => List[ (V, Iterable[U]) ]

			words.groupBy( x => x ).toList.map(t => (t._1, t._2.length))

     7. foldLeft & foldRight	=> reduces the entire collection to a type that is different that the type of objects

			Two parameters as param-list
			
			1. zero-value : starting value of the type of output you want as output
			2. Sequence Function : is a function that iterativly merges all the objects of the 
			   collection with the zero-value. 

			List[U].foldLeft( (V, U) => V )  => V
	
				
		l1.foldRight( (0,0) )( (v, z) => (z._1 + v, z._2 + 1) )   

   Wordcount Program
   ------------------
	val filePath = "E:\\Spark\\wordcount.txt"
     
     	val output = Source
                   .fromFile(filePath)
                   .getLines()
                   .toList
                   .flatMap(x => x.split(" "))
                   .groupBy( x => x )
                   .toList
                   .map( p => (p._1, p._2.length) ) 
                   .sortWith( (a, b) => a._2 > b._2 )
     
     	println( output )


  Collections
  -----------
    
   1. Array & ArrayBuffer => Both are mutable collections
	Array 		-> fixed length collection
	ArrayBuffer 	-> Variabl length collection

   2. Seq => An ordered collection
	
	2.1 IndexedSeq -> Optimal for random access of data
		=> Vector
		=> Range

	2.2 LinearSeq  -> Optimal for iteration of data
		=> List
		=> Queue
		=> Stream

   3. Map => A collection of (k, v) pairs

	val m1 = Map( 1 -> 10, 2 -> 20, 3 -> 30)
	val m1 = Map( (1,10), (2,20), (3,30) )

	m1(<key>)  
	m1.get(<key>)  -> Returns an Option object
	m1.getOrElse(<key>, <default-value>)

   4. Set => Is an unordered collection of unique elements
	

  Option
  -------
    => represents an object that may be there or may not be there. 
    
       Option[U] => Some[U]   if there is a value to return
		    None      if there is no value to return. 

     
 ==========================================
   Spark - Basics & Architecture
 ==========================================     
   
   Spark is a framework written in Scala

   Spark is a unified in-memory distributed computing framework for big-data analytics 
	=> Spark is 100x faster than MapReduce if you use 100% in-memory computations.
	=> Spark is 6 to 7 times fater than MapReduce even if you use disk-based computations.
   
   Spark is a polyglot
	=> Spark supports multiple programming language
	=> Scala, Java, Python, R (and SQL)

   Spark can run on multiple cluster managers
	=> local, Spark Standalone, YARN, Mesos, Kubernetes


   Unified Framework
   -----------------
	Spark provides a set of consistent APIs for processing different analytical workloads
	using the same execution engine.

         => Batch analytics of unstructured data	: Spark Core API (low-level)
	 => Batch analytics of structured data		: Spark SQL
	 => Stream analytics (real-time)		: Spark Streaming & Structured Streaming
	 => Predictive analytics (using ML)		: Spark MLlib
	 => Graph parallel computations			: Spark GraphX


   Getting started with Spark
   --------------------------	
	Popular Options : 
	
	=> Scale-IDE for Eclipse + Maven
		=> Add required dependencies from Central maven repo
		=> The required packges are download and added to your project.
	=> IntelliJ + SBT

        Central maven repository: https://mvnrepository.com/ (search for Spark)

	Downloading "winutils.exe" to your local directory
		=> Create the following folder: C:\hadoop\bin
		=> Download an appropriate version of winutils.exe from the following url;
			https://github.com/steveloughran/winutils

   Spark Architecture
   ------------------

    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.


   ===========================================
    Spark Core API (Low level API)
   ===========================================

    => Spark Core is used to analyse unstructured data and is a low-level API
    => Uses RDD as fundamental data abstraction

    RDD (Resilient Distributed Dataset)
    -----------------------------------

	-> RDD is a collection of distributed in-memory partitions
             -> A partition is a collection of objects of some type.

	-> RDDs are immutable

        -> RDDs are lazily evaluated
		=> Transformations does not cause execution
		=> Execution will be triggered only by action command

	-> RDDs are resilient 
		-> RDDs are resilient to missing in-memory partitions


   Creating RDDs
   -------------
	
    Three ways:

	1. Create an RDD from some external data source (such as a text-file)

		val rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)	

	2. Create an RDD from programmatic data

		val rdd1 = sc.parallelize( 1 to 100, 3 )

	3. By applying transformations on existing RDDs
		
		val rdd2 = rdd1.flatMap( x => x.split(" "))

   RDD Operations
   --------------

	To things:

	1. Transformations	
		-> Returns an RDD
		-> Does not cause execution
		-> Transformation only create RDD Lineage DAGs

        2. Actions
		-> Trigger execution of the RDD
		-> Produces output
		-> Converts thr logical Plan into a physical execution plan.


   RDD Lineage DAG
   ---------------
   -> RDD Lineage is alogical-plan, maintained by the driver
   -> RDD Lineage maintains the hierarchy of all parent RDDs that caused the creation of this RDD.
  
    	val rddFile = sc.textFile(filePath, 4)
	
(4) E:\Spark\wordcount.txt MapPartitionsRDD[1] at textFile at <console>:26 []
 |  E:\Spark\wordcount.txt HadoopRDD[0] at textFile at <console>:26 []

	val rddWords = rddFile.flatMap( x => x.split(" ") )
	
(4) MapPartitionsRDD[2] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[1] at textFile at <console>:26 []
 |  E:\Spark\wordcount.txt HadoopRDD[0] at textFile at <console>:26 []

	val rddPairs = rddWords.map( x => (x, 1) )

(4) MapPartitionsRDD[3] at map at <console>:25 []
 |  MapPartitionsRDD[2] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[1] at textFile at <console>:26 []
 |  E:\Spark\wordcount.txt HadoopRDD[0] at textFile at <console>:26 []

	val rddWc = rddPairs.reduceByKey( (a, b) => a + b )

(4) ShuffledRDD[4] at reduceByKey at <console>:25 []
 +-(4) MapPartitionsRDD[3] at map at <console>:25 []
    |  MapPartitionsRDD[2] at flatMap at <console>:25 []
    |  E:\Spark\wordcount.txt MapPartitionsRDD[1] at textFile at <console>:26 []
    |  E:\Spark\wordcount.txt HadoopRDD[0] at textFile at <console>:26 []


   Getting started with Spark
   --------------------------	
	Popular Options : 
	
	=> Scale-IDE for Eclipse + Maven
		=> Add required dependencies from Central maven repo
		=> The required packges are download and added to your project.
	=> IntelliJ + SBT

        Central maven repository: https://mvnrepository.com/ (search for Spark)

	Downloading "winutils.exe" to your local directory
		=> Create the following folder: C:\hadoop\bin
		=> Download an appropriate version of winutils.exe from the following url;
			https://github.com/steveloughran/winutils

  
  RDD Persistence
  ---------------
     	val rdd1 = sc.textFile( <file> , 3 )
	val rdd2 = rdd1.t2(...)
	val rdd3 = rdd1.t3(...)
	val rdd4 = rdd3.t4(...)
	val rdd5 = rdd3.t5(...)
	val rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_ONLY )  ===> instruction to spark to save rdd6 partitions
	val rdd7 = rdd6.t7(...)

	rdd6.collect()	
	Lineage of rdd6 => rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	Tasks: [sc.textFile, t3, t5, t6] -> collect

	rdd7.collect()
	Lineage of rdd7 => rdd6.t7
	Tasks: [t7] -> collect

	rdd6.unpersist()
	
	StorageLevels
        -------------
	1. MEMORY_ONLY		=> default, in-memory & deserialized
	2. MEMORY_ONLY_SER	=> in-memory & serialized
	3. MEMORY_AND_DISK	=> in-memory or on-disk, deserialized
	4. MEMORY_AND_DISK_SER	=> in-memory or on-disk, serialized
	5. DISK_ONLY		=> on-disk, serialized
	6. MEMORY_ONLY_2	=> 2 coies on different executors are stored
	7. MEMORY_AND_DISK_2	=> 2 coies on different executors are stored

       Commands
       ---------
	  => <rdd>.cache()  // can not define storage-level. It is MEMORY_ONLY 
	     <rdd>.persist()
	     <rdd>.persist( StorageLevel.DISK_ONLY )

	     <rdd>.unpersist()

	     <rdd>.toDebugString  => to check the lineage DAG of the rdd.
	     

   Executor memory structure
   ==========================

   	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   Types of Transformations
   =========================

	-> Narrow Transformations
		-> is a transformation that does not cause data shuffling
		-> partition to partition transformations
		-> number of partitions of the output rdd is equal to that of input rdd

	-> Wide Transformations
		-> cause shuffling of the data across various input partitions
		-> number of partitions of the output rdd can be different than that of input rdd

  RDD Transformations
  ====================

   1. map		P: U => V
			Transforms each input object to the corresponding output object by applying the
			function. 
			Object to object transformation
			input RDD: N objects, output RDD: N objects

		rddFile.map(x => x.split(" "))

   2. filter		P: U => Boolean
			Filters the input objects based on the function.
			input RDD: N objects, output RDD: < N objects

		rddWords.filter(w => w(0) == 's').collect

   3. glom		P: None
			Create one Array object per partition with all the elements of the partition.

			rdd1			val rdd2 = rdd1.glom()

			P0: 2,1,3,2,4,5 -> glom -> P0: Array(2,1,3,2,4,5)
			P1: 4,6,4,6,5,7 -> glom -> P1: Array(4,6,4,6,5,7)
			P2: 8,0,7,9,5,6 -> glom -> P2: Array(8,0,7,9,5,6)

			rdd1.count : 18	(Int)    rdd2.count : 3  (Array[Int])

		rdd1.glom().map( x => x.sum ).collect()

    4. flatMap		P: U -> TraversableOnce[V] 
			flattens the elements of the iterables produced by the function

		val rddWords = rddFile.flatMap(x => x.split(" "))

    5. mapPartitions		P: Iterable[U] => Iterator[V]
				partition to partition transformation
				Takes entire partition as function input and transforms it to another iterable

		rdd1.mapPartitions( p => p.map(x => (x, x>5))).glom.collect
		rdd1.mapPartitions( p => List(p.sum).iterator ).glom.collect

    6. mapPartitionsWithIndex	P: (Int, Iterable[U]) => Iterator[V]
				Similar to mapPartitions but we get partition-index as additional function
				parameter.
			
		rdd1.mapPartitionsWithIndex( (i, p) => List( (i, p.sum) ).iterator ).collect

    7. distinct		P: None, Optional: numPartitions
			Returns an RDD with distinct elements

		rdd1.distinct().collect

    8. sortBy		P: U => V, Optional: ascending (true/false), numPartitions
			Elements of the RDD are sorted based on the function output

		rddWords.sortBy(w => w(0)).collect
		rddWords.sortBy(w => w(0), false).collect
		rddWords.sortBy(w => w(0), true, 5).collect

    Types of RDDs
    -------------
	-> Generic RDD: RDD[U]
	-> Pair RDD:	RDD[(K, V)]


    9. groupBy		P: U => V, Optional: numPartitions
			Returns a Pair RDD, where
			   key: Unique-value of the function output
			   value: CompactBuffer containing all the elements that returned the key


		rddWords.groupBy(x => x.length).map(p => (p._1, p._2.toList.length)).collect
		rddWords.groupBy(x => x).map(p => (p._1, p._2.toList.length)).collect


		val rddFile = sc.textFile("data/wordcount.txt", 4)
                      .flatMap(x => x.split(" "))
                      .groupBy(x => x)
                      .map(x => (x._1, x._2.toList.length))
                      .sortBy(x => x._2, false, 1)

   10. mapValues 	P: U => V
			Applied to Pair RDDs only
			Transforms thh 'value' part of the (K, V) bu applying the function.

		val rdd2 = rdd1.map(x => (x, x+10))
		Note: In the above function 'x' is the 'value' part of the (K,V) pairs.

   11. randomSplit	P: Array of weights
			Returns an array of RDDs split randomly approximatly in the weights specified.

		val rddArr = rdd1.randomSplit( Array(0.3, 0.2, 0.5) )
		val rddArr = rdd1.randomSplit( Array(0.3, 0.2, 0.5), 546435 )

   12. repartition	P: numPartitions
			Is used to increase or decrease the number of partitions of the output RDD. 
			Cause global shuffle.

   13. coalesce		P: numPartitions
			Is used to only decrease the number of partitions of the output RDD. 
			Cause partition merging

	Recommendations for better performance from RDDs
        ------------------------------------------------
	-> The size of each partitions should be around 128 MB (between 100MB to 1000MB)
	-> The number of partitions should be a multiple of number of cores
	-> If the number of partitions is less than but close to 2000, bump it up to 2000 partitions
	-> The number of cores in each executor should be 5 (4 to 6)

		
   14. partitionBy	P: partitioner
			Applied only to Pair RDDs
			Is used to control which (K,V)s go to which partition. 

		1. Range Partitioner

			val rangePartitioner = new RangePartitioner(5, rdd)    
    			val partitionedRdd = rdd.partitionBy(rangePartitioner)

		2. Hash Partitioner

			val hashPartitioner = new HashPartitioner(3)    
    			val partitionedRdd = rdd.partitionBy(hashPartitioner)

		3. Custom partitioner
			=> User can define custom partitioning logic.

   15. union, intersection, subtract & cartesian
			=> They are applied on two RDDs
			=> ex: rdd1.union(rdd2)

	  Let us say rdd1 has M partitions & rdd2 has N partitions, then

	  command			partitions
          ----------------------------------------
	  rdd1.union(rdd2)		M + N, narrow
	  rdd1.intersection(rdd2)	Bigger of M & N, wide
	  rdd1.subtract(rdd2)		M, wide
	  rdd1.cartesian(rdd2)		M * N


   ..ByKey Transformations
   ------------------------
	=> Wide transformations
	=> Applied only on Pair RDDs
	=> Does some computation based on the key

    16. sortByKey		P: None, Optional: ascending (true/false), numPartitions
				Sorts the objects of the RDD by key

			rddPairs.sortByKey().collect()
			rddPairs.sortByKey(false).collect()
			rddPairs.sortByKey(true, 6).collect()

    17. groupByKey		P: None, Optional: numPartitions
				Returns a pair RDD where
				  key: Each unique key
				  value: CompactBuffer containg all values.

				WARNING: Avoid "groupByKey"

			rddPairs.groupByKey().collect

		val rddFile = sc.textFile("data/wordcount.txt", 4)
                      .flatMap(x => x.split(" "))
                      .map(x => (x, 1))
                      .groupByKey()
                      .mapValues(x => x.sum)
                      .sortBy(x => x._2, false, 1)

     18. reduceByKey		P: (U, U) -> U, Optional: numPartitions
				Reduces all the values of each unique key by iterativly applying the function
				on all partitions (narrow op) and then across the output generated at each 
				partition (shuffle op).
       
		val rddFile = sc.textFile("data/wordcount.txt", 4)
                      .flatMap(x => x.split(" "))
                      .map(x => (x, 1))
                      .reduceByKey((a,b) => a + b)
                      .sortBy(x => x._2, false, 1)


     19. aggregateByKey	  => Reduces the values of each unique key to a value of type zero-value. 

					Three parameters:

					1. zero-value:  the final value of each unique-key is if type zero-value
					2. sequence function
					3. combine function
		
		val rdd_students = sc.parallelize(students_list, 3)
                            .map(x => (x._1, x._3))
                            .aggregateByKey( (0,0) )(seq_fun, comb_fun)
                            .mapValues(x => x._1.toDouble/x._2)

	val students_list = List(
  ("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  ("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  ("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  ("Keerti", "Maths", 87), ("Keerti", "Physics", 93), ("Keerti", "Chemistry", 91), ("Keerti", "English", 74), 
  ("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  ("Anitha", "Maths", 86), ("Anitha", "Physics", 62), ("Anitha", "Chemistry", 75), ("Anitha", "English", 83), 
  ("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60))
  
    val student_rdd = sc.parallelize(students_list , 3)
    
    val seqFn = (z:(Int, Int), v: Int) => (z._1 + v, z._2 + 1)
    val combFn = (a: (Int, Int), b: (Int, Int)) => (a._1 + b._1, a._2 + b._2)
    
    val avg_rdd = student_rdd.map( x => (x._1, x._3) )
                     .aggregateByKey((0,0))(seqFn, combFn)
                     .mapValues(p => p._1.toDouble/p._2)
    
    avg_rdd.collect.foreach(println)



	20. joins => join, leftOuterJoin, rightOuterJoin, fullOuterJoin
		
		RDD[(K, V)].join( RDD[(K, W)] ) => RDD[(K, (V, W))]

		val join = names1.join(names2) 
		val leftOuterJoin = names1.leftOuterJoin(names2)
		val rightOuterJoin = names1.rightOuterJoin(names2)
		val fullOuterJoin = names1.fullOuterJoin(names2)

        21. cogroup	


  Action Commands
  ---------------

   1. collect

   2. count

   3. saveAsTextFile

   4. reduce		P: (U, U) => U
			Reduces the entire RDD to one final value of the same type as the RDD.
			Reduces each partition in the first stage and reduces the outputs of each partition further
			to produce the final output in the second stage

		P0: 2, 1, 3, 2, 4, 5, 6, 6	-> reduce -> 29 -> reduce => 116
		P1: 4, 5, 7, 8, 9, 0, 9, 7, 5   -> reduce -> 54
		P2: 7, 1, 2, 3, 2, 3, 5, 4, 6	-> reduce -> 33

		rdd1.reduce( (a,b) => a + b )


   5. aggregate  	   -> Reduces the entire RDD to a type different than the type of elements using
			      a zero-value. The final output is of the type of zero-value (not of the type 
			      of elements)

			      Three parameters:  RDD[U]

			      1. zero-value : Z (type of zero-value)
			      2. seq-operation:  Operates on each partition and folds the elements with the 
						 zero-value.
					         (Z, U) => Z     (similar to scala 'fold' HOF)
			      3. combine operation: Reduces all the values of each partition produced by 
						    seq-operation using a reduce function.
						  (Z, Z) => Z						
			rdd1: 
			P0: 8, 3, 8, 9, 8, 3   => (39, 6)  => (100, 18)
			P1: 4, 2, 1, 4, 6, 7   => (24, 6)
			P2: 8, 9, 8, 5, 6, 1   => (37, 6)

			
			rdd1[U].aggregate(zv: Z)(seq-fn: (Z, U) => Z, comb-fn: (Z, Z) => Z)

			rdd1[U].aggregate( (0,0) )( (z, v) => (z._1 + v, z._2 + 1) , 
						    (a, b) => (a._1 + b._1, a._2 + b._2) )
	

   6. take(n)
		rdd1.take(10)

   7. takeOrdered(n)

		rdd1.takeOrdered(10)
		rdd1.takeOrdered(10)(Ordering[Int].reverse)	
		rddWords.takeOrdered(100)(Ordering[String].reverse)

   8. takeSample(<boolean>, n)

		rdd1.takeSample(true, 10)
		rdd1.takeSample(true, 10, 4564)
		rdd1.takeSample(false, 10, 4564)

  9. countByValue

	rddWords.countByValue
	res42: scala.collection.Map[String,Long] = Map(map -> 6, mapreduce -> 6, das -> 6, flatmap -> 12, sqoop -> 6, sadas -> 1, oozie -> 6, rdd -> 43, transformations -> 10, hadoop -> 25, asd -> 5, spark -> 40, hive -> 19, scala -> 28, actions -> 10, flume -> 6, groupby -> 6, hdfs -> 6, d -> 1)

  10. countByKey
	rdd2.collect
	res44: Array[(Int, Int)] = Array((10,1), (10,2), (10,1), (10,3), (20,4), (20,4), (20,5))
	rdd2.countByKey
	res46: scala.collection.Map[Int,Long] = Map(20 -> 3, 10 -> 4)

  11. first
	
	rdd1.first

  12. foreach => runs a function on all the objects of the RDD. returns Unit

  13. saveAsSequenceFile

  14. saveAsObjectFile


  Use-case
  --------
    Solve the following using RDD API

    Dataset: https://github.com/ykanakaraju/sparkscala/blob/master/SparkCore/data/cars.tsv
    
    From cars.tsv file find out the average weight of all models of each make of American origin cars
    -> Arrange the data in the DESC order of average weight
    -> Save the output as a single text file.

    => Please try to solve it.


  Closure
  -------
   In spark, a 'closure' represents all the variables and methods that must be visible for a task
   to perform its computations on RDD partitions.

   => The driver serializes the closure and separate copy wil be made availeable for every task.

	
	var c = 0

	def isPrime(n: Int) : Boolean = {
	   var output = true
	   for (i <- 2 to n-1){
		if (n % i == 0) output = false
	   }
	   output
        }

	def f1(n: Int) = {
	   if ( isPrime(n) ) c += 1
	   n * 2
        }	

	val rdd1 = sc.parallelize( 1 to 4000, 4 )
	val rdd2 = rdd1.map( f1 )

	rdd2.collect()

	prinln(c)  	//   c = 0

	Limitation: We can not use local variables that are part of closure to implement global counter.
	Solution: Use 'Accumulator' variable.
	
	
    
  Shared Variables
  ----------------

  1. Accumulator
	=> Is not part of function closure, hence it is not a local copy.
	=> Maintained by the driver	
	=> Accumulators are used to implement global counters.

	var c = sc.longAccumulator()

	def isPrime(n: Int) : Boolean = {
	   var output = true
	   for (i <- 2 to n-1){
		if (n % i == 0) output = false
	   }
	   output
        }

	def f1(n: Int) = {
	   if ( isPrime(n) ) c.add(1)
	   n * 2
        }	

	val rdd1 = sc.parallelize( 1 to 4000, 4 )
	val rdd2 = rdd1.map( f1 )

	rdd2.collect()

	prinln(c.value)  	//   c = 0


  2. Broadcast Variables
	=> Only one copy of the broadcast variable is sent to every executor node
	=> All tasks running in that executor, will lookup from that copy.
	=> Use it, to broadcast large immutbale lookup tables/maps etc to save memory.
	
	  val bc = sc.broadcast( Map( 1 -> a, 2 -> b, 3 -> c, 4 -> d, 5 -> e, ... ) )

	  def f1(n: Int) = {
		bc.value.getOrElse(n, 0)
	  }

	  val rdd1 = sc.parallelize( 1 to 4000, 4 )
	  val rdd2 = rdd1.map( f1 )


  =======================
   spark-submit command
  =======================

      -> 'spark-submit' is a single command for submit any spark application (scala, java, python, R) to any
	  supported cluster manager (local, yarn, mesos, k8s, spark standalone)s
	
	   spark-submit --master yarn
		--deploy-mode cluster
		--driver-memory 2G
		--driver-cores 2
		--executor-memeory 10G
		--executor-cores 5
		--num-executors 20
		--class <class-qualified-name>
		<jar-file-path> [app-args]


	spark-submit --master local 
		--class tekcrux.WordcountEx 
		'/home/cloudera/workspace_projects/SparkWordcount/target/spark_wordcount-0.0.1-SNAPSHOT.jar' 
		wordcount_input.txt 
		wcout

	spark-submit --master yarn 
		--class tekcrux.WordcountEx 
		'/home/cloudera/workspace_projects/SparkWordcount/target/spark_wordcount-0.0.1-SNAPSHOT.jar' 
		wordcount_input.txt 
		wcout2

 =====================================
    Spark SQL (org.apache.spark.sql)
 =====================================
  
   -> Is spark's structured data processing API. 

	   Structured file formats: Parquet (default), ORC, JSON, CSV (delimited text file)
	   JDBC format : RDBMS, NoSQL
	   Hive format : Can process data stored in Apache Hive

   -> SparkSession 
	  -> Starting point of execution
	  -> Represents a user-session (with its own configuration) within an application
	  -> Introduced in Spark 2.0

   	val spark = SparkSession
              	.builder
              	.master("local[2]")              
              	.appName("DataSourceBasic")
              	.getOrCreate()  


   -> Data Abstractions

	1. Dataset[U] 
	   -> Is a typed entity
	   -> Is a collection of distributed in-memory partitions that are immutable and lazily evaluated.
	   -> Dataset => Data (of any type)
			 Schema (is an object of "StructType")	   

	2. DataFrame
	    -> Alias of Dataset[Row]
	    -> DataFrame => Data (collection of 'Row' objects) 
			    Schema (is an object of "StructType")	

		StructType(
			StructField(age,LongType,true), 
			StructField(gender,StringType,true), 
			StructField(id,LongType,true), 
			StructField(name,StringType,true)
		)	

 	3. Table
	    -> Can be temporary or managed table.

    Row
    ---
	-> Is a collection of 'Columns' which are processed using Spark SQL internal types

   
   Spark SQL basic steps 
   ----------------------

	1. Read/load some data (external or programmatic) into a DataFrame

		val df1 = spark.read.format("json").load(inputFile)
		val df1 = spark.read.json(inputFile)

	2. Apply transformations on the DataFrame using API methods or using SQL

		Using DF transformation methods
		-------------------------------
		   val df2 = df1.where("gender = 'male' and gender is not null")
                 	 	.select("id", "name", "age", "gender")
                 	 	.orderBy("age", "name")
                 	 	.groupBy("age").count()
                 	 	.limit(4)

		Using SQL
		---------

		<sparkSession>.table(<tableName>) => returns a DF with the entire table's data
		<sparkSession>.sql(<query>) => returns a DF with the query results

		 	df1.createOrReplaceTempView("people")
    
    			spark.catalog.listTables().show()
   
    			val df3 = spark.sql("""select age, count(1) as count
	             				from people
	             				where age is not null
	             				group by age
	             				order by age""")

	3. Write/Save the DataFrame to some structured data format or a database. 

		df3.write.format("json").save(outputPath)
		df3.write.json(outputPath)

  Save Modes
  ----------
	1. ErrorIfExists (default)
	2. Ignore
	3. Append
	4. Overwrite

	df3.write.mode(SaveMode.Overwrite).json(outputPath)


  Local Temp Views
  -----------------
      df.createOrReplaceTempView("flights")
      spark.catalog.listTables().show()
     
      val qry = """select * from flights
                   where count > 100
                   order by count desc 
                   limit 30"""
     
      val df2 = spark.sql(qry)
      df2.show()

		
  Global Temp Views
  -----------------
     df.createGlobalTempView("g_flights")
     spark.catalog.listTables().show()
     
     val qry = """select * from global_temp.g_flights
                   where count > 100
                   order by count desc 
                   limit 30"""
     
     val df2 = spark.sql(qry)
     df2.show()     
     
     val spark2 = spark.newSession()
     spark2.catalog.listTables().show()
     
     val df3 = spark2.sql(qry)
     df3.show()


  DataFrame Transformations
  -------------------------

   1. select

	// select with String parameters
	val df2 = df1.select("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME", "count")
	
	// select with "Column" parameters
	val df2 = df1.select(col("DEST_COUNTRY_NAME") as "destination",
                         column("ORIGIN_COUNTRY_NAME") as "origin",
                         expr("count"),
                         expr("count + 10 as newCount"),
                         expr("count > 200 as highFrequency"),
                         expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic") )

   2. where / filter
		
	val df3 = df2.where("highFrequency = true")  
	val df3 = df2.filter("highFrequency = true")  

	val df3 = df2.where(col("highFrequency") === true)    
	val df3 = df2.filter(col("highFrequency") === true)    

   3. orderBy / sort

	  val df3 = df2.orderBy("count", "destination") 
	  val df3 = df2.sort("count", "destination") 

	  val df3 = df2.sort(desc("count"), asc("destination")) 

   4. groupBy	=> returns a "RelationalGroupedDataset" object
		   You have apply some aggregation method to return a DF

	  val df3 = df2.groupBy("highFrequency", "domestic").count()
	  val df3 = df2.groupBy("highFrequency", "domestic").avg("count")
          val df3 = df2.groupBy("highFrequency", "domestic").sum("count")
	  val df3 = df2.groupBy("highFrequency", "domestic").max("count")
          df3.show()

	  val df3 = df2.groupBy("highFrequency", "domestic")
                     .agg( count("count") as "count",
                           sum("count") as "sum",
                           max("count") as "max",
                           round(avg("count"), 2) as "avg" )

   5. limit

		val df2 = df1.limit(10)

   6. selectExpr

	   val df2 = df1.selectExpr("DEST_COUNTRY_NAME as destination",
                         "ORIGIN_COUNTRY_NAME as origin",
                         "count",
                         "count + 10 as newCount",
                         "count > 200 as highFrequency",
                         "DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic" )

   7. withColumn & withColumnRenamed

	   val df3 = df1.withColumn("newCount", col("count") + 10)
                 	.withColumn("highFrequency", expr("count > 200"))
                 	.withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME"))
                 	.withColumnRenamed("DEST_COUNTRY_NAME", "destination")
                 	.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

	  ------------------------------------------------------------
	  import spark.implicits._

	  val listUsers = Seq((1, "Raju", 5),
                       (2, "Ramesh", 15),
                       (3, "Rajesh", 18),
                       (4, "Raghu", 35),
                       (5, "Ramya", 25),
                       (6, "Radhika", 35),
                       (7, "Ravi", 70))

	  import spark.implicits._

          val usersDf = listUsers.toDF("id", "name", "age")

	  val df1 = usersDf.withColumn("ageGroup", when(col("age") < 13, "child")
                                             .when(col("age") < 20, "teenager")
                                             .when(col("age") < 60, "adult")
                                             .otherwise("seenior") )  

	 ---------------------------------------------------------------
	 val case_when = """case when age < 13 then 'child'
	                         when age < 20 then 'teenager' 
	                         when age < 60 then 'adult'
	                         else 'senior' 
	                     end"""
     
         val df1 = usersDf.withColumn("ageGroup", expr(case_when) )  


   8. udf  (user-defined-function)

		val get_age_group = (age: Int) => {
        		if (age <= 12) "child"
        		else if (age <= 19) "teenager"
        		else if (age < 60) "adult"
        		else "senior"            
     		}
     
     		val get_age_group_udf = udf(get_age_group)
     
     		val df1 = usersDf.withColumn("ageGroup", get_age_group_udf(col("age")) ) 

		------------------------------------------------

		spark.udf.register("get_age_group_udf", get_age_group)
     
     		spark.catalog.listFunctions().select("name").where("name like 'g%'").show()
     
     		val qry = "select id, name, age, get_age_group_udf(age) as ageGroup from users"     
     		val df2 = spark.sql(qry)
     
     		df2.show()

    9. drop => excude one or more columns in the output DF.

		val df4 = df3.drop("newCount", "highFrequency")    
    		df4.printSchema()

   10. dropDuplicates => excludes duplicate rows in the output dataframe
			 output df will have any duplicate rows.

		val userDf2 = usersDf.dropDuplicates()   // dedups entire rows
	    	val userDf2 = usersDf.dropDuplicates("name", "age")   // dedups specified columns
    	    	userDf2.show()


   11. NA functions => are used to work with the rows with null values in any or specified columns.

	 	val users4Df = spark.read.json("E:\\PySpark\\data\\users.json")
     	 	users4Df.show()
    
	 	val df1 = users4Df.na.drop()  			   => drops all rows with Nulls in any columns 
         	val df1 = users4Df.na.drop(Array("age", "phone")) => drops all rows with Nulls in the specified columns 

	 	val df1 = users4Df.na.fill("No Value").na.fill(0) => fills the columns with null with provided values


   12. distinct => returns a DF with distinct rows of the input DF

	  	val df2 = df1.select("ORIGIN_COUNTRY_NAME").distinct()
    		println(s"df2.count with ORIGIN_COUNTRY_NAME = ${df2.count()}")

   13. union, intersect

	   	val df4 = df2.union(df3)
    		df4.show()
    		println(s"UNION df4.count = ${df4.count()}")
    		println(s"UNION df4 partitions = ${df4.rdd.getNumPartitions}")
    
    		val df5 = df4.intersect(df3)
    		df5.show()
    		println(s"INTERSECT df5.count = ${df5.count()}")
    		println(s"INTERSECT df5 partitions = ${df5.rdd.getNumPartitions}")
    
   14. randomSplit	

		val dfArr = df1.randomSplit(Array(0.6, 0.4), 4456)
    		println( dfArr(0).count(), dfArr(1).count() )

   15. sample
		val df2 = df1.sample(true, 0.5, 35345)    // withReplacement : true
		val df2 = df1.sample(false, 0.5, 35345)   // withReplacement : false 
    		df2.show()

   16. repartition	

		val df2 = df1.repartition(4) 
    		println(s"df2 partitions = ${df2.rdd.getNumPartitions}")
    
    		val df3 = df2.repartition(3) 
    		println(s"df3 partitions = ${df3.rdd.getNumPartitions}")
    
    		val df4 = df3.repartition(col("DEST_COUNTRY_NAME"))
    		println(s"df4 partitions = ${df4.rdd.getNumPartitions}")
    
    		val df5 = df3.repartition(5, col("DEST_COUNTRY_NAME"))
    		println(s"df5 partitions = ${df5.rdd.getNumPartitions}")

   17. coalesce

		val df3 = df2.coalesce(2)
    		println(s"df3 partitions = ${df3.rdd.getNumPartitions}")
	
    18. join => discussed separatly.


   Show command
   ------------
	-> Prints the content of a DF in tabular format
	-> Returns "Unit"

	df1.show()    	      // prints 20 rows by default
	df1.show(10)
	df1.show(50, false)  // truncate: false
	df1.show(50, 35)     // truncate columns to 35 chars. 


   Working with different file formats
   ------------------------------------

    JSON
	read:
		val df1 = spark.read.format("json").load(inputFile)
		val df1 = spark.read.json(inputFile)
	write:
		df2.write.format("json").save(outputPath)
		df2.write.json(outputPath)

    Parquet
	read:
		val df1 = spark.read.format("parquet").load(inputFile)
		val df1 = spark.read.parquet(inputFile)
	write:
		df2.write.format("parquet").save(outputPath)
		df2.write.parquet(outputPath)

    ORC
	read:
		val df1 = spark.read.format("orc").load(inputFile)
		val df1 = spark.read.orc(inputFile)
	write:
		df2.write.format("orc").save(outputPath)
		df2.write.orc(outputPath)

    CSV (delimited text files)

	read
		val df1 = spark.read.option("header", true).option("inferSchema", true).csv(inputFile)
		val df1 = spark.read.option("header", true).option("inferSchema", true).option("sep", "|").csv(inputFile)
	
	write
		df2.write.mode(SaveMode.Overwrite).option("header", true).csv(outputPath)
		df2.write.mode(SaveMode.Overwrite).option("header", true).option("sep", "|").csv(outputPath)

  
   Creating a DataFrame from Programmatic data
   -------------------------------------------
  	val listUsers = Seq((1, "Raju", 5),
                       (2, "Ramesh", 15),
                       (3, "Rajesh", 18),
                       (4, "Raghu", 35),
                       (5, "Ramya", 25),
                       (6, "Radhika", 35),
                       (7, "Ravi", 70))
                       
     	val df1 = spark.createDataFrame(listUsers).toDF("id", "name", "age")


	import spark.implicits._
	val df1 = listUsers.toDF("id", "name", "age")

	// The above statement requires spark.implicits._ to be imported
   

   Creating a DataFrame from RDD
   -----------------------------

	import spark.implicits._

	val listUsers = Seq((1, "Raju", 5),
                       (2, "Ramesh", 15),
                       (3, "Rajesh", 18),
                       (4, "Raghu", 35),
                       (5, "Ramya", 25),
                       (6, "Radhika", 35),
                       (7, "Ravi", 70))
                       
     val rdd1 = spark.sparkContext.parallelize(listUsers)
     
     val df1 = rdd1.toDF("id", "name", "age")
     // val df1 = spark.createDataFrame(rdd1).toDF("id", "name", "age")

     df1.show()
     df1.printSchema()


    Applying programmatic schema on a DF
    ------------------------------------

	val mySchema = StructType(
                       Array(
                         StructField("id", IntegerType, true),
                         StructField("name", StringType, true),
                         StructField("age", IntegerType, true)
                       )
                    )
       
     	val rdd1 = spark.sparkContext.parallelize(listUsers) 
                   .map(t => Row(t._1, t._2, t._3))
                   
     	rdd1.foreach(println)
     
     	val df1 = spark.createDataFrame(rdd1, mySchema)

       ========================================================

	val filePath = "data/flight-data/json/2015-summary.json"
    
    	val mySchema = StructType(
                       Array(
                         StructField("ORIGIN_COUNTRY_NAME", StringType, true),
                         StructField("DEST_COUNTRY_NAME", StringType, true),
                         StructField("count", IntegerType, true)
                       )
                    )
                    
    	val df1 = spark.read.schema(mySchema).json(filePath)


   Spark SQL Joins
   ---------------
    Supported Joins: inner, left, right, full, left_semi, left_anti


     left_semi join
     ---------------
	-> Is similar to inner join, but data is fetched only from the left side table
	-> Equivalent to the following sub-query:

		select * from emp where deptid in (select deptid from dept)

     left_anti join
     --------------
	-> Equivalent to the following sub-query:

		select * from emp where deptid not in (select deptid from dept)


    	Using SQL Method
    	----------------
  		employee.createOrReplaceTempView("emp")
    		department.createOrReplaceTempView("dept")
    
    		spark.catalog.listTables().show()
    
    		val qry = """select emp.*
                 		from emp left anti join dept 
                 		on emp.deptid = dept.id"""
    
    		val joinedDf = spark.sql(qry)
    
    		joinedDf.show()

		//spark.catalog.dropTempView("emp")

        Using DF API method
        -------------------
		val joinCol = employee.col("deptid") === department.col("id")    
    		val joinedDf = employee.join(department, joinCol, "left_anti")
		joinedDf.show()


    => Join Strategies:
	   -> Shuffle Join (Big Table to Big Table)
		-> Shuffle-Hash Joins
		-> Sort-Merge Join

	   -> Broadcast Joins (Big Table to Small Table)

	=> Small Table is defined as a table/DF that is smaller than the value
	   defined by autoBroadcastJoinThreshold parameter (def: 10 MB)
     

     => Explicit Broadcast Join:
	employee.join(broadcast(department), joinEmpDept, "inner")
    

    Use-Case
    --------
	Datasets: https://github.com/ykanakaraju/sparkscala/tree/master/SparkSql/data/movielens
	
	From movies.csv and ratings.csv datasets, find the top 10 movies with highest average movie rating
	-> Consider only those movies that are rated by atleast 30 user
	-> Data: movieId, title, ratingCount, averageRating
	-> Arrange the data in the DESC order of averageRating
	-> Save the output with header in a single CSV file with pipe separator.
     
	=> Try to do it yourself


  To Download a file from databricks:
  -----------------------------------

  /FileStore/<FILEPATH>
  https://community.cloud.databricks.com/files/<FILEPATH>?o=4949609693130439#tables/new/dbfs

   Example:
  /FileStore/tables/wordcount-5.txt
  https://community.cloud.databricks.com/files/tables/wordcount-5.txt?o=4949609693130439#tables/new/dbfs


  Databricks Steps
  ----------------
   1. Login to Databricks account
   2. Create a Cluster (Create -> Cluster)   -> takes 5 to 10 min
   3. Upload data files (Data -> Craete table button -> Upload)
   4. Create a notebook (Create -> Notebook)


  SQL Optimizations & Tuning
  --------------------------

    1. Caching data in memory

	    Spark SQL can cache tables using an in-memory columnar format:

		spark.catalog.cacheTable("<tableName>")
		<dataframe>.cache()

		spark.catalog.uncacheTable("<tableName>")
		<dataframe>.uncache()

		spark.sql.inMemoryColumnarStorage.compressed => true (default) / false       
   		spark.conf.set("spark.sql.inMemoryColumnarStorage.compressed", "false")

		spark.sql.inMemoryColumnarStorage.batchsize (default: 10000)

    2. Join strategy hints for SQL queries

	 Join strategy hints => BROADCAST, MERGE, SHUFFLE_HASH, SHUFFLE_REPLICATE_NL

	  	<dataFrame1>.join(<dataFrame2>.hint("BROADCAST"), joinKey, joinType)

		select /*+ MERGE */ id, name, age from emp join dept on ....
		select /*+ BROADCAST */ id, name, age from emp join dept on ....
		select /*+ REPARTITION(3) */ id, name, age from emp join dept on ....

    3. Adaptive Query Execution (AQE) 
		-> introduced in Spark 3.0
		-> enabled by default from Spark 3.2.0 onwards

		spark.conf.set("spark.sql.adative.enabled", "true")

	    Three important features:
		
	    1. Optimizing skew joins
	    2. Coalescing shuffle partition
	    3. Chosing the optimizaed Join strategy.

   
   JDBC Format - Integrating with MySQL
   ------------------------------------

import org.apache.spark.sql.SparkSession
import java.util.Properties
import com.mysql.jdbc.Driver
import org.apache.spark.sql.SaveMode

object DataSourceJDBCMySQL {
  def main(args: Array[String]) {
    //System.setProperty("hadoop.home.dir", "C:\\hadoop\\");
    
    val spark = SparkSession
      .builder.master("local[2]")
      .appName("DataSourceJDBCMySQL")
      .getOrCreate()
      
    import spark.implicits._
    
    
    // Snippet 1: Reading from MySQL using JDBC
    val jdbcDF = spark.read
                    .format("jdbc")
                    .option("url", "jdbc:mysql://localhost:3306/sparkdb")
                    .option("driver", "com.mysql.jdbc.Driver")
                    .option("dbtable", "emp")
                    .option("user", "root")
                    .option("password", "cloudera")
                    .load()
                    
     jdbcDF.show()
      
     jdbcDF.createOrReplaceTempView("empTempView")
     spark.sql("SELECT * FROM empTempView").show()
     
     
     // Snippet 2:  Writing to MySQL using JDBC
     spark.sql("SELECT * FROM empTempView")
        .write
        .format("jdbc")
        .option("url", "jdbc:mysql://localhost:3306/sparkdb")
        .option("driver", "com.mysql.jdbc.Driver")
        .option("dbtable", "emp2")
        .option("user", "root")
        .option("password", "cloudera")
        .mode(SaveMode.Overwrite)
        .save()        
    
     // Snippet 3: Writing to MySQL using JDBC
     val ratingsCsvPath = "data/movielens/ratings.csv"
     val ratingsDf = spark.read
                            .format("csv")
                            .option("header", "true")
                            .option("inferSchema", "true")
                            .load(ratingsCsvPath)
     
     ratingsDf.printSchema()
     ratingsDf.show(10)
     
     ratingsDf.write
      .format("jdbc")
      .option("url", "jdbc:mysql://localhost:3306/sparkdb")
      .option("driver", "com.mysql.jdbc.Driver")
      .option("dbtable", "movielens_ratings2")
      .option("user", "root")
      .option("password", "cloudera")
      .mode(SaveMode.Overwrite)
      .save()
      
      
      spark.close()
      
     
  }
}


   Hive Format - Integrating with Hive
   -----------------------------------

   val warehouseLocation = new File("warehouse").getAbsolutePath
    println(warehouseLocation)

    val spark = SparkSession
            .builder()
            .appName("DataSourceHive")
            .config("spark.sql.warehouse.dir", warehouseLocation)
            .config("spark.master", "local[1]")
            .enableHiveSupport()
            .getOrCreate()      
      
    import spark.implicits._
  
    spark.sparkContext.setLogLevel("ERROR")
    
    spark.sql("drop database if exists sparkdemo cascade")
    spark.sql("create database if not exists sparkdemo")
    spark.sql("show databases").show()   
    
    spark.sql("use sparkdemo")
    
    println("Current database: " + spark.catalog.currentDatabase)
    
    spark.catalog.listTables().show()
     
    val createMovies = 
      """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
    val loadMovies = 
      """LOAD DATA LOCAL INPATH 'data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
    val createRatings = 
      """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
    val loadRatings = 
      """LOAD DATA LOCAL INPATH 'data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
        
    spark.sql(createMovies)
    spark.sql(loadMovies)
    spark.sql(createRatings)
    spark.sql(loadRatings)
    
    spark.catalog.listTables().show()
     
    // Queries are expressed in HiveQL
    val moviesDF = spark.table("movies")   //spark.sql("SELECT * FROM movies")
    val ratingsDF = spark.table("ratings")  //spark.sql("SELECT * FROM ratings")
           
    val summaryDf = ratingsDF
                      .groupBy("movieId")
                      .agg(count("rating") as "ratingCount", 
                           avg("rating") as "ratingAvg")
                      .filter("ratingCount > 25")
                      .orderBy(desc("ratingAvg"))
                      .limit(10)
              
    summaryDf.show()
    
    val joinStr = summaryDf.col("movieId") === moviesDF.col("movieId")
    
    val summaryDf2 = summaryDf
                     .join(moviesDF, joinStr)
                     .drop(summaryDf.col("movieId"))
                     .select("movieId", "title", "ratingCount", "ratingAvg")
                     .orderBy(desc("ratingAvg"))
    
    summaryDf2.show()
    
    summaryDf2.write.format("hive").saveAsTable("topRatedMovies")
    spark.catalog.listTables().show()
        
    val topRatedMovies = spark.table("topRatedMovies")  //spark.sql("SELECT * FROM topRatedMovies")
    topRatedMovies.show() 
    
    spark.stop()


  ==================================
    Spark Streaming
  ==================================

     Spark's real-time data processing API

     Two APIs are available:

	1. Spark Streaming a.k.a DStreams API    (out-dated)
	2. Structured Streaming	(current and preferred API)

      Spark Streaming (DStreams API)
      ------------------------------

     => microbatch based processing
     => Provides "seconds" scale latency.  (near-real-time processing)
     => does not support event-time processing

       StreamingContext :
	  -> Is the starting point of execution
	  -> Defines a micro-batch window
	  -> Each micro-batch is an RDD

       DStream (discretized stream)
	  -> Is a continuous flow of RDDs.
	  -> Each micro-batch is represented as an RDD.

	  
  	# Create a local StreamingContext with two threads and batch interval of 1 sec.
	sc = SparkContext("local[2]", "NetworkWordCount")
	ssc = StreamingContext(sc, 1)

	lines = ssc.socketTextStream("localhost", 9999)
	words = lines.flatMap(lambda line: line.split(" "))
	pairs = words.map(lambda word: (word, 1))
	wordCounts = pairs.reduceByKey(lambda x, y: x + y)
	wordCounts.print()

	ssc.start()             
	ssc.awaitTermination() 


   Spark Structured Streaming
   -------------------------- 

	-> Consider the input data stream as the 'Input Table'. 
	  Every data item that is arriving on the stream is like a new row being appended to the Input Table.

	=> DataFrame -> Unbounded Table

	-> A query on the input will generate the 'Result Table'. 

	-> Every trigger interval (say, every 1 second), new rows get appended to the Input Table, 
	   which eventually updates the Result Table. 

	-> Whenever the result table gets updated, we would want to write the changed result 
	   rows to an external sink.

	Output Modes
	------------
	The 'Output' is defined as what gets written out to the external storage. 
	The output can be defined in a different mode:

	* Complete Mode - The entire updated Result Table will be written to the external storage.

	* Append Mode - Only the new rows appended in the Result Table since the last trigger will 
		      be written to the external storage. 

		      This is applicable only on the queries where existing rows in the 
		      Result Table are not expected to change.

	* Update Mode - Only the rows that were updated in the Result Table since the last trigger 
		      will be written to the external storage. 

		      Note that this is different from the Complete Mode in that this mode 
		      only outputs the rows that have changed since the last trigger. 

		      If the query doesn't contain aggregations, it will be equivalent to Append mode.


	Sample Socket Stream Example
	----------------------------

	from pyspark.sql import SparkSession
	from pyspark.sql.functions import explode
	from pyspark.sql.functions import split

	spark = SparkSession \
    		.builder \
    		.appName("StructuredNetworkWordCount") \
    		.getOrCreate()

	lines = spark \
    		.readStream \
    		.format("socket") \
    		.option("host", "localhost") \
    		.option("port", 9999) \
    		.load()

	# Split the lines into words
	words = lines.select( explode(split(lines.value, " ")).alias("word"))

	# Generate running word count
	wordCounts = words.groupBy("word").count()

	query = wordCounts \
    		.writeStream \
    		.outputMode("complete") \
    		.format("console") \
    		.start()

	query.awaitTermination() 


	=> Sources: File, Socket, Rate, Kafka

	=> Sinks: File, Console, Memory, Kafka, ForEach, ForEachBatch


   kanakaraju.y@cognizant.com





   




 












