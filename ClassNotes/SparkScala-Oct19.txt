
  Agenda (Spark using Scala)
  --------------------------
   -> Scala Refresher 
   -> Spark - Basics & Architecture
   -> Spark Core API
	-> RDD - Transformations and Actions
	-> Spark Shared Variables
   -> Spark-Submit
   -> Spark SQL
	-> DataFrame Operations
	-> Integration with MySQL & Hive
	-> SQL Optimizations & Tuning
   -> Spark Streaming
	-> DStreams API (introduction)
	-> Structured Streaming

  Materials
  ---------
	=> PDF Presentations
	=> Code Modules
	=> Class Notes
	=> Github: https://github.com/ykanakaraju/sparkscala


   Getting Started with Spark & Scala
   ----------------------------------

     1. vLab - Lab allocated to you. 
	
	 -> Follow the instructions given in the attached document.

	 -> You will be loging into a Window Server
	     -> Here you find a document on the desktop with useris and password. 

	 -> Click on the "Oracle VM Virtualbox" and connect to Ubuntu lab. 
	 => here you can open a terminal and connect to Spark shell (type "spark-shell")
	 => You can also launch "Eclipse" 

     2. Setting up your own environemnt on your personal machine. 

	   Pre-requisite: Java 8
	   => Open a terminal and type "java -version" (it has to be 1.8.xxx or up)
		
	   1. Scala IDE (version of Eclipse)

		URL: http://scala-ide.org/download/sdk.html

		Download Scala IDE for your OS and unzip it to a suitable location
		Navigate into the unzipped folder and click on "Eclipse" application icon to launch Scala IDE.

	   2. IntelliJ

	       Follow the instructions @ https://docs.scala-lang.org/getting-started/index.html					 
		
	       Two build tools for Scala:
		-> Maven (Scala IDE + Maven)  
		-> SBT  (IntelliJ + SBT)
		
     3. Signup to "Databricks Community" Edition Free account.

		URL to Signup : https://databricks.com/try-databricks
		URL to Login: https://community.cloud.databricks.com/login.html


	To Download a file from databricks:
	-----------------------------------

	/FileStore/<FILEPATH>
	https://community.cloud.databricks.com/files/<FILEPATH>?o=4949609693130439#tables/new/dbfs

	Example:
	/FileStore/tables/wordcount-5.txt

        Download from this URL (paste in the browser window)
	https://community.cloud.databricks.com/files/tables/wordcount-5.txt?o=4949609693130439#tables/new/dbfs


  Scala Refresher
  ---------------
  
   => Scala : SCAlable LAnguage
	      Compiler based language (based on Java)
	      Interoperable with Java

   => Scala is a multi-paradigm programming language
	 -> Scala is a "Pure" object oriented prog. lang.
	 -> Scala is a functional programming language

  Pure OOP => Scala does not have primitives and operators

  -> Scala Variables :  immutables -> val
		      mutables  -> var
	-> Scala prefers immutables. 

  -> Scala has implicit type inference

  -> Scala uses infix notation
	obj.method(param) => obj method param

  -> Scala is a statically typed language
	-> Need to know the type of every object at compile type

  Scala is PURE object oriented language
  --------------------------------------
	
      -> Scala does not have primitives or operators.	
      -> In scala, all data is objects and operations are method invocations.

	  val i = 10.*(40)  
   
             -> 10 is an Int object
	     -> * is a method invoked on 10 (Int object)
	     -> 40 is an Int object passed as a parameter
	     -> i is an Int object returned by the * method.
		
      -> <obj>.method<param1> => <obj> method<param1> => <obj> method param1


  -> Blocks
	 => A block any code enclosed in  { }
	 => A block is scala has a return value.
	 => The block returns the value of the last expression that is executed.

         Scala Unit => In Scala "Unit" is an object that represents "no value"
		       prined as "()"

   -> Unit : Is a value class in Scala which represents 'no value'
             Printed as "()"

   -> Scala imports
	
        import scala.io.StdIn		  => single class
        import scala.io.{StdIn, Source}   => multiple classes
	import scala.io._                 => all classes

   -> User Input

		val name = StdIn.readLine("What is your name ?")
    		println("Hello " + name)     
    		println("What is your age ?")
    		val age = StdIn.readInt()
    		println("Age : " + age) 

    -> Output

		printf("Name: %s, age: %2.2f", "Kanakaraju", 47d)
		println(name, age)

    -> String interpolations

		s interpolator: s"Name: $name, Age: ${age + 10}"
		f interpolator: s interpolator + formting chars
				 f"Name: $name, Age: ${age + 10}%2.2f"
		raw interpolator : will escape the escape chars

		 	val filePath = raw"E:\newdir\total\red.txt"
    		 	println(filePath)


   Flow control constructs
   -----------------------

   if..else if.. else

	if statement can return a value
	val x = if (i > 100) "> 100" else if(i < 100)  "< 100" else "== 100"

   match ..case

	   val output = i match {
     		case 10 => "ten"
     		case 20 => "twenty"
     		case 30 => "thirty"
     		case 40 => "fourty"
     		case x if (x % 2 == 0) => s"even number"
     		case y if (y % 2 != 0) => s"odd number"
     		case _  => "no match"
   	   }

  Scala Class Hierarchy
  ---------------------
	Any =>  AnyVal => Int, Double, Long, Char, Byte, Unit, Boolean, ...
            =>  AnyRef => String, List[U], all other classes

  Tuple
  ------

    -> Is a class which can hold multiple elements of different type.
	
	val t1 = (10, 20, 10.5, true, "Hello")
	t1: (Int, Int, Double, Boolean, String) = (10,20,10.5,true,Hello)


	val t2 = (10, List(10, 20, 30), Seq(10.5, 12.2), "hello")
	t2: (Int, List[Int], Seq[Double], String) = (10,List(10, 20, 30),List(10.5, 12.2),hello)

      ** A tuple with two elements is called a "Pair"


  Collections
  -----------

     Array           -> mutable collection, fixed-length
     ArrayBuffer     -> mutable collection, variable-length

     => Array: mutable & fixed length
     => ArrayBuffer: mutable with variable length

     Immutable Collections
    
        -> Seq  (Sequences)
	     -> Ordered collections and elements can be accessed using an index.

	     -> Indexed Sequences
		 -> Vector
		 -> Range

		 => Optimized for fast random-acccess		

	     -> Linear Sequences
		-> List
		-> Queue
		-> Stream 

		=> Optimized for visiting the elements linearly (i.e in a loop)
		=> They are organized as linked lists
		
		List(1,2,3,4,5,6) => List(1, List(2,3,4,5,6))   
			// here 1 is head, List(2,3,4,5,6) is tail
		        => List(1, List(2, List(3, List(4, List(5, List(6, List())))))) 

		list => List(head, tail)

	-> Set
	     -> Is a unordered collection of unique values.
	     -> We can NOT access the elements using an index.
	     -> SortedSet, BitSet

	-> Map
	     -> A collection of (K, V) pairs

		val m1 = Map( (1, 'A'), (2, 'B'), (3, 'C') )
		val m1 = Map( 1 -> "A", 2 -> "B", 3 -> "C" )

                m1(1) -> "A"
		m1(10) -> raise java.util.NoSuchElementException


  Range
  ------
		Range(0, 100, 10)  => 0: start, 100: end (excluded), 10: step
		Range(0, 10)  => step is one if not specified
		Range(10, 1, -1) =>  10, 9, 8, 7, 6, 5, 4, 3, 2
		Range(100, 0, -10) => 100, 90, 80, 70, 60, 50, 40, 30, 20, 10
		Range(0, 100, -2) => Empty
		
		0 until 10 by 2 => Range(0, 10, 2)
		0 to 10 by 2    => Range(0, 11, 2)

  
  Option
  -------
        -> reprsents an option (i.e value may be or may not be there)
	-> returns instances of 'Some' class or 'None' class


  Loops
  ------

     while

		while( i > 0 ){
            		print(i + " ")         
            		i -= 1
        	}

     do..while

		do {
         		print(i + " ")         
         		i -= 1
      		} while( i > 0 )

     foreach

	        <collection>.foreach( function )
		(1 to 10).foreach( println )

     for

		for(<generator>, <generator>, ...) { .. }

		for( i <-  1 to 10 by 2){
        	    println(s"i = $i")
     		}

		for( i <-  1 to 10 by 2; j <- 1 to 20 by 4 ){
        	    println(s"i = $i, j = $j")
     		}

		for( i <-  1 to 10 by 2 if (i != 3); j <- 1 to 20 by 4 if (j > i) ){
       		    println(s"i = $i, j = $j")
     		}

		for comprehension
		-----------------
		val l1 = for(i <- 1 to 100 by 2) yield(i)


    Exception Handling
    ------------------

        try {
		<some code that could throw an exception>
        }
        catch {
	    case e: ArrayIndexOutOfBoundException  => { .... }
	    case e: FileNotFoundException => { .... }
	    case _: Exception => { .... }
			
        }
        finally {
		<some code that is always executed
        }

     Example
     =======

      try {
         println("Reading from a file ...")   
         val f = new FileReader("file1rtyryr.txt")            
      } 
      catch {
         case ex: FileNotFoundException => {
            println("FileNotFoundException exception")
            println( ex.getMessage )
            println( ex.getStackTrace )
         }         
         case _: IOException => {
            println("IO Exception")
         }
         case _: Exception => {
            println("generic exception")
         }
      } 
      finally {
         println("Exiting finally...") 
      }  

       
   Methods
   --------
	
     => method is a reusable code blocks with a name declared using def keyword.

	-> methods can be called by positional arguments
	-> methods can be called by named arguments
	-> method arguments can have default values
	-> methods can take parameter lists

		def f1(a: Int, b: Int)(c: Int) = ( a + b ) * c     
     		val i = f1(10, 20)(30)     
     		println(i)

	-> methods can have one variable length argument (last argument)
		-> You can have default values for args if you are using veriable-length argument

		def sum(a: Int, b: Int*) : Int = {
        		var s = 0
        		for (i <- b) s += i
        		a + s
     		}       
     		val s1 = sum(10)

	-> methods can be called recursivly

		def factorial(n: Int) : Int = {
        		if (n < 2) 1
        		else n * factorial(n - 1)
     		}
     
     		println( factorial(6) )

	-> methods can be nested 

  Procedure
  ---------
     -> Procedure is similar method, but always returns "Unit"
     -> Syntactically there is no "=" symbol in the definition  

	def box(name: String) {
        	val line = "-" * name.length + "----"        
        	println(line + "\n| " + name + " |\n" + line)        
     	}     
     
     	box("Scala is a programming language")


  Function
  --------

	=> A function is treated as a literal value (such as 10, "hello" etc)
	=> A function has a value (of its own)
	=> A function has a type
	=> A function is anonymous (by nature)

	=> A function can be assigned to a variable
		val add = (x: Int, y: Int) => x + y

	function literal			function type
        -----------------------------------------------------
	(x: Int, y: Int) => x + y		(Int, Int) => Int
	(x: String, y: Int) => x * y		(String, Int) => String
	() => "Windows 10"			() => String
	(name: String) => println(name)		String => Unit
	(l: List[Int]) => l.sum			List[Int] => Int
	(t: (Int, Int)) => t._1 + t._2		 ((Int, Int)) => Int


        => A function can be passed as argument to another method/function

		def calculate(a: Int, b: Int, f: (Int, Int) => Int) = {
       			f(a, b)
     		}
		val result = calculate(345, 20, (x: Int, y: Int) => x % y)     
     		println( result )

       => A method/function can return a function as its return value
		-> A block can return a function as final value
	
		def compute(op: String) = {       
       			op match {
          			case "+" => (x : Int, y: Int) => x + y
          			case "-" => (x : Int, y: Int) => x - y
          			case "*" => (x : Int, y: Int) => x * y
          			case "/" => (x : Int, y: Int) => x / y
          			case _ => (x : Int, y: Int) => x % y
       			}       
     		}        
     		println( compute("+")(321, 10) )


   Higher Order Functions
   ----------------------
	=> Take a function as an argument
	=> Applied on some Iterable object. 

   1. map		P: U => V
			Transforms each object of the input to another obejct in the output
			input: N elements, output: N element

		l1.map( s => s.split(" ").length )

   2. filter		P: U => Boolean
			The output collection will have only those objects for which the funtion
			returns true. 
			input: N elements, output: <= N element

		l4.filter( t => t._1 + t._2 >= 10 )

   3. flatMap		P: U => GenTraversableOnce[V]    (U => Collection)
			flattens the output of the function
			input: N elements, output: >= N element

		words.flatMap(x => x.toUpperCase)

    4. reduce        => reduceLeft (reduce) & reduceRight
			P: (U, U) => U
			Reduces the entire input collection to one value of the 'same type' by 'iterativly
			applying' the function

		List(2,1,2,4,5,4,3,2).reduce( (x, y) => x - y )  => 23
		
    5. sortWith		P: binary sorting function
			Elements of the collections are sorted based on the sorting function.

		l4.sortWith( (m,n) => m._1 > n._1 )  
    
     6. groupBy		P: U => V
			Returns a Map objects, where
			   key: Each unique value of the function output
			   value: List object containing all the objects that produced the key.

			List[U].groupBy( U => V ) => Map[ V, List[U] ]

     7. mapValues	P: U => V
			Applied only on (key, value) pairs i.e Map objects
			It will transform only the value of the (K, V) pairs

			val m2 = m1.mapValues( x => (x,x) )

     8. fold		=> foldLeft (fold) & foldRight
		
  
  WordCount Program
  -----------------

     val filePath = "E:\\Spark\\wordcount.txt"
     
     val wc = Source.fromFile(filePath)
              .getLines()
              .toList
              .flatMap( x => x.split(" ") )
              .groupBy( x => x )
              .toList
              .map( x => (x._1, x._2.length ) )
              .sortWith( (a, b) => (a._2 > b._2) )

    val wc = Source.fromFile(filePath)
              .getLines()
              .toList
              .flatMap( x => x.split(" ") )
              .groupBy( x => x )
              .mapValues( x => x.length )
              .toList
              .sortWith( (a, b) => (a._2 > b._2) )
              
     wc.foreach(println)

  ========================================================  
    Spark
  ========================================================

   => Spark is an in-memory distributed computing framework.

   	 Cluster => A unified entity containing many nodes whose cumulative resources can be used to
               	    distribute store and/or computations of big data. 

     	in-memory computing -> The intermediate results of distributed computing can be saved in-memory
			       and subsequent tasks can run on these saved partitions. 

        => Spark is 100x faster than MapReduce if you use 100% in-memory processing
	   Spark is 6 t 7x faster than MapReduce even if you use disk based processing

   => Spark is written in Scala. 

   => Spark is a polyglot
	-> Support supports Scala, Java, Python and R

   => Spark provides a "Unified framework" for analytical workloads

	Spark provides a set of consistent APIs that can used to process different analytical workloads
	based on the same execution engine. 

	 -> Batch Analytics on unstructured data	: Spark Core API
	 -> Batch Analytics on structured data		: Spark SQL
	 -> Stream Analytics (real-time processing)	: Spark Streaming, Strcutured Streaming
	 -> Predistive analytics (using ML)		: Spark MLlib
	 -> Graph parallel computations			: Spark GraphX

   => Spark applications can be submitted to multiple cluster managers
	=> local, spark standalone, YARN, Mesos, Kubernetes 
	

    Spark Architecture
    ------------------

    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.


   Getting started with Spark
   --------------------------	
	Popular Options : 
	
	=> Scale-IDE for Eclipse + Maven
		=> Add required dependencies from Central maven repo
		=> The required packges are download and added to your project.
	=> IntelliJ + SBT

        Central maven repository: https://mvnrepository.com/ (search for Spark)

	Downloading "winutils.exe" to your local directory
		=> Create the following folder: C:\hadoop\bin
		=> Download an appropriate version of winutils.exe from the following url;
			https://github.com/steveloughran/winutils
    

   RDD (Resilient distributed dataset)
   -----------------------------------
    -> RDD is the fundamental data abstraction for in-memory data in Spark core API
    -> RDD is a collection of distributed in-memory partitions
	-> A partition is a collection of objects (of any type)

    -> RDDs are immutable

    -> RDDs are lazily evaluated
	-> Transformations does not cause execution. 
           They only create lineage LAGs
        -> Action commands trigger execution.

    -> RDD are resilient
	-> RDDs can create any missing partitions on the fly. 

  Creating RDD
  ------------
	Three ways:

	1. Create an RDD from external data file such as text files

		val rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)


	2. Create an RDD from programming data

		val rdd1 = sc.parallelize( List(1,2,1,3,2,4,5,3,5,6,7,8,9,8,9,6,7,1,2,4,2,3,4,5,6,2,1), 3 )


        3. By applying transformations on existing RDDs

		val rddWords = rddFile.flatMap(x => x.split(" "))


  RDD Operations
  --------------

    Two things:

	1. Transformations	
		-> Returns an RDD
		-> Does not cause execution
		-> Transformation only create RDD Lineage DAGs

        2. Actions
		-> Trigger execution of the RDD
		-> Produces output
		-> Converts thr logical Plan into a physical execution plan.


  RDD Lineage DAG
  ----------------

   => RDD Lineage is a logical plan maintained by the driver
   => RDD lineage DAG contains the hierarchy of dependencies all the way from the very first RDD.
	

	 val rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

(4) E:\Spark\wordcount.txt MapPartitionsRDD[7] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[6] at textFile at <console>:24 []


	val rddWords = rddFile.flatMap(x => x.split(" "))

(4) MapPartitionsRDD[9] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[7] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[6] at textFile at <console>:24 []

 
	val rddPairs = rddWords.map(x => (x,1))

(4) MapPartitionsRDD[10] at map at <console>:25 []
 |  MapPartitionsRDD[9] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[7] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[6] at textFile at <console>:24 []

    
	val rddWc = rddPairs.reduceByKey( (a, b) => a + b )
   
(4) ShuffledRDD[11] at reduceByKey at <console>:25 []
 +-(4) MapPartitionsRDD[10] at map at <console>:25 []
    |  MapPartitionsRDD[9] at flatMap at <console>:25 []
    |  E:\Spark\wordcount.txt MapPartitionsRDD[7] at textFile at <console>:24 []
    |  E:\Spark\wordcount.txt HadoopRDD[6] at textFile at <console>:24 []

   

  RDD Persistence
  ---------------

	val rdd1 = sc.textFile( <file>, 4 )
	val rdd2 = rdd1.t2( ... )
	val rdd3 = rdd1.t3( ... )
	val rdd4 = rdd3.t4( ... )
	val rdd5 = rdd3.t5( ... )
	val rdd6 = rdd5.t6( ... )
	rdd6.persist( StorageLevel.DISK_ONLY )  --> instruction to spark to save rdd6 partitions.
	val rdd7 = rdd6.t7( ... )

	rdd6.collect()

	Lineage of rdd6 => rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[sc.textFile, t3, t5, t6] -> collected

	rdd7.collect()
	
	Lineage of rdd7 => rdd7 -> rdd6.t7
		[t7] -> collected

	rdd6.unpersist()


	StorageLevels
        -------------
	1. MEMORY_ONLY	       -> default, Memory deserialized 1x replicated
	2. MEMORY_AND_DISK     -> Disk Memory deserialized 1x replicated
	3. DISK_ONLY	       -> Disk serialized 1x replicated
	4. MEMORY_ONLY_SER     -> Memory serialized 1x replicated
	5. MEMORY_AND_DISK_SER -> Disk Memory serialized 1x replicated
	6. MEMORY_ONLY_2       -> Memory deserialized 2x replicated
	7. MEMORY_AND_DISK_2   -> Disk Memory deserialized 2x replicated

	Commands
	--------
		rdd1.cache()    => memory-only
		rdd1.persist()	=> memory-only

		import org.apache.spark.storage.StorageLevel
		rdd1.persist( StorageLevel.MEMORY_AND_DISK)

		rdd1.unpersist()

   Executor memory structure
   ==========================

   	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   Types of Transformations
   ------------------------

	-> Narrow Transformations
		-> is a transformation that does not cause data shuffling
		-> partition to partition transformations
		-> number of partitions of the output rdd is equal to that of input rdd

	-> Wide Transformations
		-> cause shuffling of the data across various input partitions
		-> number of partitions of the output rdd can be different than that of input rdd

  RDD Transformations
  -------------------

  1. map		P: U => V
			Object to object transformation
			input RDD: N objects; output RDD: N objects
	
   	rddFile.map(x => x.split(" ").length ).collect

  2. filter		P: U => Boolean
			Filter only those objects for which the function returns true
			input RDD: N objects; output RDD: <= N objects
	
	rddFile.filter(x => x.split(" ").length > 8).collect

  3. glom		P: None
			Returns one Array per partition with all the objects of the partition.

		rdd1		      rdd2 = rdd1.glom()
		P0: 3,2,1,3,2,4,5,6 -> glom -> P0: Array(3,2,1,3,2,4,5,6)
		P1: 4,3,1,3,6,7,8,1 -> glom -> P1: Array(4,3,1,3,6,7,8,1)
		P2: 3,2,5,7,5,8,9,1 -> glom -> P2: Array(3,2,5,7,5,8,9,1)
	
		rdd1.count = 24 (Int)		rdd2.count = 3 (Array[Int])

		rdd1.glom.map(x => x.sum).collect

  4. flatMap		P: U -> TraversableOnce[V]
			flattens the iterables produced by the function
			input RDD: N objects; output RDD: >= N objects

		rddWords.flatMap(x => x.toUpperCase).collect

  5. mapPartitions	P: Iterator[U] => Iterator[V]
			Transforms each input partition to corresponding output partition

		rdd1		   rdd2 = rdd1.mapPartitions( p => p.map(x => x*10) )
		P0: 3,2,1,3,2,4,5,6 -> mapPartitions -> P0: 30,20,10,30,20,40,50,60
		P1: 4,3,1,3,6,7,8,1 -> mapPartitions -> P1: 40,30,10,30,60,70,80,10
		P2: 3,2,5,7,5,8,9,1 -> mapPartitions -> P2: 30,20,50,70,50,80,90,10
   

   6. mapPartitionsWithIndex	P: (Int, Iterator[U]) => Iterator[V]
				Same as mapPartitions but you get partition index as an additional parameter.

		rdd1.mapPartitionsWithIndex( (i, p) => List((i, p.sum)).iterator ).collect

		rdd1.mapPartitionsWithIndex((i, p) => p.map(x => (i, x))).filter(x => x._1 == 0).map(x => x._2).collect


    7. distinct			P: None, Optional: numPartitions
				Returns an RDD with unique objects of the input RDD.

   		rddWords.distinct.collect
		rddWords.distinct(5).collect

	Types of RDDs
	-------------
	1. Generic RDD  => RDD[U]
	2. Pair RDD	=> RDD[(K, V)]


    8. mapValues		P: U => V
				Applied only on PairRDDs
				Transforms only the value part of the (k,v) pairs

		 RDD[(K, V)].mapValues( V => W ) => RDD[(K, W)]

		 val rdd3 = rdd2.mapValues(x => (x,x))

    9. sortBy			P: U => V, Optional: ascending (true or false), numPartitions
				Sorts the elements of the RDD based on the function output

		rddWords.sortBy(x => x(x.length - 1), true).collect()
		rdd1.sortBy(x => x%3, false, 1).glom.collect

    10. groupBy			P: U => V
				Returns a PairRDD where
					key: unique value of the function output
					value: compactBuffer of all the value of the RDD that produced the key.																																																																

		val wordcount = sc.textFile("data/wordcount.txt", 4)
                        .flatMap(s => s.split(" "))
                        .groupBy(x => x)
                        .mapValues(x => x.toList.length)
                        .sortBy(x => x._2, false, 1)      
           
      		wordcount.saveAsTextFile("output/wordcount")

   11. randomSplit		P: Array of weights (ex: Array(0.4, 0.3, 0.3) ), Optional: seed
				Splits the RDD randomly into multiple RDDs in the given weights

		val rddArr = rdd1.randomSplit( Array(0.6, 0.4))
		val rddArr = rdd1.randomSplit( Array(0.6, 0.4), 3454 )   //3454 is a seed


   12. repartition		P: numPartitions
				Used to increase or decrease the number of partitions of the output RDD
				Cause global shuffle

		val rddWords2 = rddWords.coalesce(8)


   13. coalesce			P: numPartitions
				Used to only decrease the number of partitions of the output RDD
				Causes partition-merging

		val rddWords2 = rddWords.coalesce(2)
			

    Recommendations for better performance
    ---------------------------------------
	=> The size of each partition should be ideally approx. 128 MB (100 MB to 1 GB is fine) 
	=> The number of partitions should be a multiple of number of cores allocated
	=> Number of CPU cores per executor should be 5 

  
    14. partitionBy		P: partitioner
				Applied ONLY to pair RDDs. 
				Partitioning happens based on the 'key'
				Is used to control which data goes to which partition.

		Built-in partitioners:

		1. Range Partitioner

			val rangePartitioner = new RangePartitioner(5, rdd)    
    			val partitionedRdd = rdd.partitionBy(rangePartitioner)

		2. Hash Partitioner

			val hashPartitioner = new HashPartitioner(3)    
    			val partitionedRdd = rdd.partitionBy(hashPartitioner)

		3. Custom partitioner
			=> User can define custom partitioning logic.


    15. union, intersection, subtract, cartesian

	Let us rdd1 has M partitions and rdd2 has N partition
	
	 command			output partitions etc.
	 -------------------------------------------------------
	 rdd1.union(rdd2)		M + N, narrow
	 rdd1.intersection(rdd2, [n])    Bigger of M & N, wide 
	 rdd1.subtract(rdd2, [n])	M, wide
	 rdd1.cartesian(rdd2)		M*N, wide



    ..ByKey Transformations
    -----------------------
        => Are wide transformations
	=> Are applied only to PairRDD

     16. keys, values	  P: None
		
		RDD[(K, V)].keys => RDD[K]
		RDD[(K, V)].values => RDD[V]		

		RDD[(Int, String)].keys => RDD[Int]
		RDD[(Int, String)].values => RDD[String]

     17. sortByKey	P: None, Optional: ascending (true/false), numPartitions
			The objects of the PairRDD are sorted based on the value of the key

		rddPairs.sortByKey().collect()
		rddPairs.sortByKey(false).collect()
		rddPairs.sortByKey(true, 1).collect()
		
     18. groupByKey	P: None, Optional: numPartitions
			Returns a PairRDD where
				key: each unique key of the input RDD
				value: CompactBuffer with all the values for that key

			*** WARNING: AVOID groupByKey if possible ***

		  val wordcount = sc.textFile("data/wordcount.txt", 4)
                        .flatMap(s => s.split(" "))
                        .map(x => (x, 1))
                        .groupByKey()
                        .mapValues(x => x.toList.length)
                        .sortBy(x => x._2, false, 1)

     19. reduceByKey		P: (U, U) -> U, Optional: numPartitions
				Reduces all the values of each unique key by iterativly applying the function
				on all partitions (narrow op) and then across the output generated at each 
				partition (shuffle op).

		val output = sc.textFile("E:\\Spark\\wordcount.txt", 4)
                     .flatMap(x => x.split(" "))
                     .map(x => (x, 1))
                     .reduceByKey( (x, y) => x + y )
                     .sortBy(x =>x._2, false, 1)

    20. aggregateByKey
		=> Reduces the values of each unique key to a value of type zero-value. 

					Three parameters:

					1. zero-value:  the final value of each unique-key is if type zero-value
					2. sequence function
					3. combine function
		
		val rdd_students = sc.parallelize(students_list, 3)
                            .map(x => (x._1, x._3))
                            .aggregateByKey( (0,0) )(seq_fun, comb_fun)
                            .mapValues(x => x._1.toDouble/x._2)



	val students_list = List(
  	("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  	("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  	("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  	("Keerti", "Maths", 87), ("Keerti", "Physics", 93), ("Keerti", "Chemistry", 91), ("Keerti", "English", 74), 
  	("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  	("Anitha", "Maths", 86), ("Anitha", "Physics", 62), ("Anitha", "Chemistry", 75), ("Anitha", "English", 83), 
  	("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60))
  
  
      	val seq_op = (z: (Int, Int), v: Int ) => (z._1 + v, z._2 + 1)      
      	val comb_op = (a: (Int, Int), b: (Int, Int)) => (a._1 + b._1, a._2 + b._2)      
      
      	val students_rdd = sc.parallelize(students_list, 3)
                           .map(x => (x._1, x._3) )
                           .aggregateByKey((0,0))(seq_op, comb_op)
                           .mapValues( x => x._1.toDouble/x._2 )
      
      	students_rdd.collect.foreach( println )


    21. joins => join, leftOuterJoin, rightOuterJoin, fullOuterJoin


    22. cogroup		=> Used when you want to Join RDDs with duplicate keys.
			=> Apply groupByKey on each RDD and then apply fullOuterJoin
      


  RDD Actions
  -----------

  1. collect

  2. count

  3. saveAsTextFile

  4. reduce		  	(U, U) => U
				reduces the entire RDD to one value of the same type by iterativly appyling the 
				function on all the partitions and then on the results of each partition.
             	rdd1		
		P0 : 5, 4, 3, 7, 8, 9, 0	=> reduce => 36 => 99
		P1 : 9, 0, 6, 7, 5, 4, 6	=> reduce => 37
		P2 : 1, 2, 3, 2, 4, 3, 5, 6	=> reduce => 26
			
			val s  = rdd1.reduce( (x, y) => x + y )	

   5. aggregate		   -> Reduces the entire RDD to a type different than the type of elements using
			      a zero-value. The final output is of the type of zero-value (not of the type 
			      of elements)

			      Three parameters:  RDD[U]

			      1. zero-value : Z (type of zero-value)
			      2. seq-operation:  Operates on each partition and folds the elements with the 
						 zero-value.
					         (Z, U) => Z     (similar to scala 'fold' HOF)
			      3. combine operation: Reduces all the values of each partition produced by 
						    seq-operation using a reduce function.
						  (Z, Z) => Z						
			rdd1: 
			P0: 8, 3, 8, 9, 8, 3   => (39, 6)  => (100, 18)
			P1: 4, 2, 1, 4, 6, 7   => (24, 6)
			P2: 8, 9, 8, 5, 6, 1   => (37, 6)

			
			rdd1[U].aggregate(zv: Z)(seq-fn: (Z, U) => Z, comb-fn: (Z, Z) => Z)

			rdd1[U].aggregate( (0,0) )( (z, v) => (z._1 + v, z._2 + 1) , 
						    (a, b) => (a._1 + b._1, a._2 + b._2) )

   6. first

   7. take
		rdd1.take(15)

   8. takeOrdered
		rdd1.takeOrdered(15)
		rdd1.takeOrdered(15)(Ordering[Int].reverse)

   9. takeSample
		 rdd1.takeSample(true, 10)
		 rdd1.takeSample(true, 10, 464)
		 rdd1.takeSample(true, 100, 464)

		 rdd1.takeSample(false, 10)
		 rdd1.takeSample(false, 10, 464)

   10. countByValue

		rdd1.countByValue()
		res106: scala.collection.Map[Int,Long] 
		    = Map(0 -> 2, 5 -> 3, 1 -> 1, 6 -> 3, 9 -> 2, 2 -> 2, 7 -> 2, 3 -> 3, 8 -> 1, 4 -> 3)
 
   11. countByKey      => Applied on pair RDD
			  Returns a Map with how many times each key is repeated. 
		rdd2.countByKey
		res110: scala.collection.Map[Int,Long] = Map(5 -> 7, 1 -> 7, 6 -> 7, 2 -> 7, 7 -> 7, 3 -> 7, 8 -> 7, 4 -> 7)


   12. foreach

   13. foreachPartition
		rdd1.foreachPartition(p => println(p.toList.sum))

   14. saveAsSequenceFile
		rddWc.saveAsSequenceFile("E:\\Spark\\output\\seq")

   15. saveAsObjectFile
		rddWc.saveAsObjectFile("E:\\Spark\\output\\obj")


   
   Jobs, Stages and Tasks
   =======================
	
	1. A single spark application can have multiple jobs.
	2. Each "Action command" launches a new Job
	3. Each job can have one or more stages that are executed sequentially (one AFTER another)
	4. Each stage will have one or more tasks that run in parallel
		(Each task may have several transformations)
	5. each "Wide transformation" will cause a new stage to be launched.


   Use-case
   --------
	
     Dataset: https://github.com/ykanakaraju/sparkscala/blob/master/data/cars.tsv

     => From cars.tsv dataset, find the average-weight of each make of American cars.
	 -> Arrange the data in the DESC order of average weight
	 -> Save the output as a single text file.

        => Try it yourself..


  Closure
  --------
	In Spark, a Closure is all the variables and methods that must be visible inside an executor for it to
	perform its computation on RDD partitions. 

	=> The Driver serliazes the closure and a separate copy is sent to ever executor.

   
		var c = 0

		def isPrime(n) = {		    
		    var flag = true
                    for(i <- 2 to n-1) {
			if (n % i == 0) flag = false
		    }
		    flag
		}

		def f1(n) = {
		   if (isPrime(n)) c += 1
                   n * 2 
		}	

		val rdd1 = sc.paralleize( 1 to 4000, 4 )

		val rdd2 = rdd1.map( f1 )

		rdd2.collect		

		println( c )   // c = 0   

	
	Limitation: We can not use 'local variable' which are part of the closure to implement
		    global counter. 

	Solution: Use "Accumulator" variable. 



  Spark Shared Variables
  -----------------------

      1. Accumulator variable
      =======================
	=> Is not part of function closure, hence it is not a local copy.
	=> Maintained by the driver	
	=> Accumulators are used to implement global counters.

	var c = sc.longAccumulator("counter")     // count of prime numbers

	def iSPrime( n: Int ) : Boolean = {
	    return true if n is prime
	    else return false
	}

	def f1(n: Int) = {
            if ( isPrime(n) ) c.add(1)
	    n * 2
	}

	val rdd1 = sc.parallelize( 1 to 4000, 4 )
	val rdd2 = rdd1.map( f1 )

	rdd2.collect

	println(s"c = ${c.value}")  // c = 0


      2. Broadcast variable
      ======================
	=> Only one copy of the broadcast variable is sent to every executor node
	=> All tasks running in that executor, will lookup from that copy.
	=> Use it, to broadcast large immutbale lookup tables/maps etc to save memory.

           val map = sc.broadcast(Map(1 -> Emp(1), 2 -> Emp(2), 3 -> Emp(3), .....))     # 100 MB
		
	   def f1(n: Int) : Option[Emp] = {              
               map.get(n)     
	   }

	   val rdd1 = sc.parallelize( 1 to 4000, 4 )
	   val rdd2 = rdd1.map( f1 )

	   rdd2.collect
		

 ==============================
   spark-submit
 ==============================
  
   spark-submit is a single command that is used to submit any spark application (Scala, Java, Python or R)
   to any cluster manager (local, standalone scheduler, YARN, Mesos, Kubernetes)
   

       spark-submit  --master yarn
		--deploy-mode cluster
		--driver-memory 3G
		--driver-cores 3
		--executor-memory 10G
		--executor-cores 5
		--num-executors 10
                --class <qualified-class-path>
		<jar-file-path> [app-args]



 ==============================
   Spark SQL
 ==============================
 
    => Structured data processing API

          Structured file formats: Parquet (default), ORC, JSON, CSV (delimited text file)
		      JDBC format: RDBMS & NoSQL databases
		      Hive format: Hive Warehouse

   => SparkSession	
	-> Starting point of execution for Spak SQL
	-> represents a user-session with its own configuration with in an application
  
	val spark = SparkSession
                  	.builder
                  	.master("local[2]")              
                  	.appName("DataSourceBasic")
                  	.getOrCreate() 
	
	spark.conf.set("spark.sql.shuffle.partitions", 10)

 
   => Dataset
	-> Dataset is a collection of distributed in-memory partitions that are immutable and lazily evaluated. 
	-> Dataset has two components:
		metadata : schema (is a StuctType object)
		data:  Some type of objects

		Dataset => RDD + schema

		Schema:  StructType(
				StructField(age,LongType,true), 
				StructField(gender,StringType,true), 
				StructField(id,LongType,true), 
				StructField(name,StringType,true)
			  )

   => DataFrame (DF)
	-> Alias of Dataset[Row]
	-> Collection of "Row" objects	

   

  Basic steps in a Spark SQL program
  -----------------------------------

   1. Read/load the data from some data source into a DataFrame

		val df1 = spark.read.format("json").load(inputPath)
		val df1 = spark.read.json(inputPath)

   2. Apply transformations on the DF using DF API methods or using SQL

	Using DF Transformations
        -------------------------

	    val df2 = df1.select("id", "name", "age", "gender")
                  	.where("age is not null")
                  	.orderBy("gender", "age")
                  	.groupBy("age").count()
                  	.limit(4)
									
	Using SQL
        ---------
		df1.createOrReplaceTempView("people") 
   
    		spark.catalog.listTables().show()
    
    		val df2 = spark.table("people")

		val qry = """select age, count(*) as count 
                 		from people
                 		where age is not null
                 		group by age
                 		order by age
                 		limit 4"""    
    
    		val df2 = spark.sql(qry)


   3. Write/save the DF into some structured destination.  

		df2.write.format("json").save(outpuPath)
		df2.write.json(outpuPath)


   SaveModes
   ---------
	1. ErrorIfExists (default)
	2. Ignore
	3. Append
	4. Overwrite

	df2.write.mode(SaveMode.Overwrite).json(outpuPath)


  LocalTempView & GlobalTempView
  ------------------------------
	LocalTempView 
		-> created at Session scope
		-> created using df1.createOrReplaceTempView("users")
		-> accessible only from its own SparkSession.

	GlobalTempView
		-> created at Application scope
		-> Accessible from all SparkSessions
		-> created using df1.createOrReplaceGlobalTempView("gusers")
		-> Attached to a temp database called "global_temp"

  DataFrame transformations
  -------------------------

   1. select

  		    val df2 = df1.select("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME")

		    val df2 = df1.select(col("DEST_COUNTRY_NAME") as "destination",
                         col("ORIGIN_COUNTRY_NAME") as "origin",
                         expr("count") cast "int",
                         expr("count + 10 as newCount"),
                         expr("count > 200 as highFrequency"),
                         expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic"),
                         lit("India") as "country" )

		    df2.select(avg("count") as "avg", 
               			max("count") as "max", 
               			lit(10) as "ten").show()

   2. where / filter

	 val df3 = df2.where("count > 1000 and domestic = false") 
	 val df3 = df2.filter("count > 1000 and domestic = false")   

	 val df3 = df2.filter( col("count") > 1000 )  


   3. orderBy / sort

	 val df3 = df2.orderBy("count", "destination")
	 val df3 = df2.orderBy(desc("count"), asc("destination"))

	 val df3 = df2.sort(desc("count"), asc("destination"))


   4. groupBy => Returns a "RelationalGroupedDataset" object
		 Apply some aggregation function to return a DataFrame


	val df3 = df2.groupBy("domestic", "highFrequency").count()
	val df3 = df2.groupBy("domestic", "highFrequency").sum("count")
	val df3 = df2.groupBy("domestic", "highFrequency").avg("count")
	val df3 = df2.groupBy("domestic", "highFrequency").max("count")

	val df3 = df2.groupBy("domestic", "highFrequency")
                 .agg( count("count") as "count",
                       sum("count") as "sum",
                       max("count") as "max",
                       avg("count") as "average" )

   5. limit

	  val df2 = df1.limit(10)

   6. selectExpr

	    val df2 = df1.selectExpr("DEST_COUNTRY_NAME as destination",
                         "ORIGIN_COUNTRY_NAME as origin",
                         "count",
                         "count + 10 as newCount",
                         "count > 200 as highFrequency",
                         "ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as domestic")


   7. withColumn & withColumnRenamed

	 val df3 = df1.withColumn("newCount", col("count") + 10)
                .withColumn("highFrequency", expr("count > 200 "))
                .withColumn("domestic", expr("ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME"))
                .withColumn("count", col("count") cast "int")
                .withColumnRenamed("DEST_COUNTRY_NAME", "destination")
                .withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

	        ------------------------

  	        val df2 = usersDf.withColumn("ageGroup", when(col("age") < 13, "Child")
                                              		.when(col("age") < 20, "Teenager")
                                              		.when(col("age") < 60, "Adult")
                                              		.otherwise("Senior") )
     		df2.show()

                -----------------------------------------------------
		val case_when = """case when age <= 12 then 'child'
	                         when age <= 19 then 'teenager' 
	                         when age <= 60 then 'adult'
	                         else 'senior' 
	                     end"""     
     
     		val df2 = usersDf.withColumn("ageGroup", expr(case_when) )


   8. udf (user-defined-function) 

	val get_age_group = (age: Int) => {
                if (age <= 12) "child"
                else if (age <= 19) "teenager"
                else if (age < 60) "adult"
                else "senior"            
     	}
     
     	val get_age_group_udf = udf(get_age_group)
     
     	val df2 = usersDf.withColumn("ageGroup", get_age_group_udf( col("age") ) )
     
        -------------------------------------------------

       spark.udf.register("get_age_group_udf", get_age_group )     
       spark.catalog.listFunctions().where("name like 'g%'").show()     
     
       val qry = "select id, name, age, get_age_group_udf(age) as ageGroup from users"
     
       val df2 = spark.sql(qry)
     
       df2.show()

   
   9. drop	
	
	val df3 = df2.drop("newCount", "highFrequency")    
    	df3.printSchema()

   10. na (NA Funtions)

	   usersDf.na.drop().show()     
           usersDf.na.drop( Array("age", "phone") ).show()
          usersDf.na.fill(0).na.fill("NONE").show()


   11. dropDuplicates

	  val users3 = Seq((1, "Raju", 5),
                     (1, "Raju", 5),
                     (3, "Raju", 5),
                     (4, "Raghu", 35),
                     (4, "Raghu", 35),
                     (6, "Raghu", 35),
                     (7, "Ravi", 70))

    	  val users3Df = spark.createDataFrame(users3).toDF("id", "name", "age")     
    	  users3Df.show()    

	  users3Df.dropDuplicates().show()
    	  users3Df.dropDuplicates("name", "age").show()


   12. distinct
		
	  users3Df.distinct().show()


   13. union, intersect

		val df2 = df1.where("count > 1000")
    		println("df2.count: " + df2.count )
    		println("partitions: " + df2.rdd.getNumPartitions )
    		df2.show()
    
    		val df3 = df1.where("DEST_COUNTRY_NAME = 'India'")
    		println("df3.count: " + df3.count )
   		println("partitions: " + df3.rdd.getNumPartitions )
    		df3.show()
    
    		val df4 = df2.union(df3)
    		println("df4.count: " + df4.count )
    		println("partitions: " + df4.rdd.getNumPartitions )
    		df4.show()
    
    		val df5 = df4.intersect(df3)    // common( [df2 + df3] & df3 )
    		println("df5.count: " + df5.count )
    		println("partitions: " + df5.rdd.getNumPartitions )
    		df5.show()

		--------------------

		println("spark.sql.shuffle.partitions => " + spark.conf.get("spark.sql.shuffle.partitions") )
    		spark.conf.set("spark.sql.shuffle.partitions", "5")
    		println("spark.sql.shuffle.partitions => " + spark.conf.get("spark.sql.shuffle.partitions") )

   14. randomSplit	

		val dfArr = df1.randomSplit(Array(0.6, 0.4), 4456)
    		println( dfArr(0).count(), dfArr(1).count() )

   15. sample
		val df2 = df1.sample(true, 0.5, 35345)    // withReplacement : true
		val df2 = df1.sample(false, 0.5, 35345)   // withReplacement : false 
    		df2.show()


   16. repartition

	 
    	val df2 = df1.repartition(4)
    	println("df2 partitions: " + df2.rdd.getNumPartitions )   
    
    	val df3 = df2.repartition(2)
    	println("df3 partitions: " + df3.rdd.getNumPartitions ) 
    
    	val df4 = df2.repartition(2, col("count"))
    	println("df4 partitions: " + df4.rdd.getNumPartitions ) 
    
    	val df5 = df2.repartition(col("ORIGIN_COUNTRY_NAME"))
    	println("df5 partitions: " + df5.rdd.getNumPartitions ) 

   17. coalesce

	val df3 = df2.coalesce(3)
        println("df3 partitions: " + df3.rdd.getNumPartitions )  

   
   18. join  => discussd as separate topic


  
   Working with different file formats
   -----------------------------------

  	1. JSON

		Read
		----
		val df1 = spark.read.format("json").load("people.json")
		val df1 = spark.read.json(inputFile)

		Write
		-----
		df2.write.format("json").save(outputDir)
		df2.write.json(outputDir)
		df2.write.mode(SaveMode.Overwrite).format("json").save(outputDir)

	2. CSV (delimited text file format)

		Read
		----
		val df1 = spark.read.format("csv").option("header", true).option("inferSchema", true).load(inputFile)  

		    val df1 = spark
              			.read
              			.option("header", true)
              			.option("inferSchema", true)
              			.csv(inputPath, inputPath2)


		Write
		-----
		df2.write.format("csv").save(outputDir)
		df2.write.mode(SaveMode.Overwrite).format("csv").save(outputDir)
		df2.write.mode(SaveMode.Overwrite).option("header", true).format("csv").save(outputDir)
		df2.write.option("header", true).option("sep", "\t").format("csv").save(outputDir)
     

	3. Parquet (default)

		Read
		----
		val df1 = spark.read.format("parquet").load(inputFile) 
		val df1 = spark.read.parquet(inputFile)  

		Write
		-----
		df2.write.save(outputDir)   // default format is parquet   
		df2.write.format("parquet").save(outputDir)
		df2.write.parquet(outputDir)


	4. ORC

		Read
		----
		val df1 = spark.read.format("orc").load(inputFile) 
		val df1 = spark.read.orc(inputFile)  

		Write
		-----
		df2.write.format("orc").save(outputDir)
		df2.write.orc(outputDir)
 

  
   Creating an RDD from a DF
   -------------------------

	val rdd1 = df1.rdd


   Creating a DataFrame from programmatic data
   --------------------------------------------
     	import spark.implicits._
          
     	val users = Seq(
            (1, "Raju", 25, 101),
            (2, "Ramesh", 26, 101),
            (3, "Amrita", 30, 102),
            (4, "Madhu", 32, 102),
            (5, "Aditya", 28, 102),
            (6, "Aditya", 28, 100))
            
     	// using spark.implicits._
     	val df1 = users.toDF("id", "name", "age", "deptid")

     	// without using spark.implicits._
     	val df1 = spark.createDataFrame(users).toDF("id", "name", "age", "deptid") 
     
     	df1.show()


    Creating a DataFrame from RDD
    -----------------------------

	import spark.implicits._

	val rdd1 = spark.sparkContext.parallelize(users)
            
     	val df1 = rdd1.toDF("id", "name", "age", "dept") 
     
     	df1.show()


   Creating a Dataset from DataFrame
   ----------------------------------
	case class User(id: Int, name:String, age:Int, dept: Int)

 	val users = Seq(
            (1, "Raju", 25, 101),
            (2, "Ramesh", 26, 101),
            (3, "Amrita", 30, 102),
            (4, "Madhu", 32, 102),
            (5, "Aditya", 28, 102),
            (6, "Aditya", 28, 100))     	
            
        val df1 = users.toDF("id", "name", "age", "dept") 
     
        val ds1 = df1.as[User]


   Applying programmatic schema on a DataFrame
   -------------------------------------------

     val inputPath = "data/flight-data/json/2015-summary.json"
     
     val mySchema = StructType(
                    Array(StructField("ORIGIN_COUNTRY_NAME", StringType, true),
                        StructField("DEST_COUNTRY_NAME", StringType, true),
                        StructField("count", IntegerType, true)))
     
     val df1 = spark.read.schema(mySchema).json(inputPath)


   Joins
   ======

     Supported Joins => inner, left, right, full, left_semi, left_anti


     left_semi join
     ==============
	Is like inner join, but get data only from left side table.

	Equivalent to the following sub-query
		=> select * from emp where deptid in (select id from dept)

     left_anti join
     ==============

	Equivalent to the following sub-query
		=> select * from emp where deptid not in (select id from dept)


     Join Strategies:
     ================

	   -> Shuffle Join (Big Table to Big Table)
		-> Shuffle-Hash Joins
		-> Sort-Merge Join

	   -> Broadcast Joins (Big Table to Small Table)

	=> Small Table is defined as a table/DF that is smaller than the value
	   defined by autoBroadcastJoinThreshold parameter (def: 10 MB)     

     	=> Explicit Broadcast Join:
		employee.join(broadcast(department), joinEmpDept, "inner")


   Use-Case
   ========
    
    Datasets: https://github.com/ykanakaraju/sparkscala/tree/master/data/movielens

    From movies.csv and ratings.csv datasets, fetch the top 10 movies with highest average user rating
	-> Consider only those movies with ratingCount >= 30
	-> Data: movieId, title, ratingCount, averageRating
	-> Sort the data in the DESC order of averageRating
	-> Save the output as a single pipe-separated CSV file with header.
     
	=> Try it yourself

  
   
  Databricks Steps
  ----------------
   1. Login to Databricks account
   2. Create a Cluster (Create -> Cluster)   -> takes 5 to 10 min
   3. Upload data files (Data -> Craete table button -> Upload)
   4. Create a notebook (Create -> Notebook)


  SQL Optimizations & Performance Tuning
  ---------------------------------------

    1. Caching data in memory

	    Spark SQL can cache tables using an in-memory columnar format:

		spark.catalog.cacheTable("<tableName>")
		<dataframe>.cache()

		spark.catalog.uncacheTable("<tableName>")
		<dataframe>.uncache()

		spark.sql.inMemoryColumnarStorage.compressed => true (default) / false       
   		spark.conf.set("spark.sql.inMemoryColumnarStorage.compressed", "false")

		spark.sql.inMemoryColumnarStorage.batchsize (default: 10000)

    2. Join strategy hints for SQL queries

	 Join strategy hints => BROADCAST, MERGE, SHUFFLE_HASH, SHUFFLE_REPLICATE_NL

	  	<dataFrame1>.join(<dataFrame2>.hint("BROADCAST"), joinKey, joinType)

		select /*+ MERGE */ id, name, age from emp join dept on ....
		select /*+ BROADCAST */ id, name, age from emp join dept on ....
		select /*+ REPARTITION(3) */ id, name, age from emp join dept on ....

    3. Adaptive Query Execution (AQE) 
		-> introduced in Spark 3.0
		-> enabled by default from Spark 3.2.0 onwards

		spark.conf.set("spark.sql.adative.enabled", "true")

	    Three important features:
		
	    1. Optimizing skew joins
	    2. Coalescing shuffle partition
	    3. Chosing the optimizaed Join strategy.

   
   JDBC Format - Integrating with MySQL
   ------------------------------------

import org.apache.spark.sql.SparkSession
import java.util.Properties
import com.mysql.jdbc.Driver
import org.apache.spark.sql.SaveMode

object DataSourceJDBCMySQL {
  def main(args: Array[String]) {
    //System.setProperty("hadoop.home.dir", "C:\\hadoop\\");
    
    val spark = SparkSession
      .builder.master("local[2]")
      .appName("DataSourceJDBCMySQL")
      .getOrCreate()
      
    import spark.implicits._
    
    
    // Snippet 1: Reading from MySQL using JDBC
    val jdbcDF = spark.read
                    .format("jdbc")
                    .option("url", "jdbc:mysql://localhost:3306/sparkdb")
                    .option("driver", "com.mysql.jdbc.Driver")
                    .option("dbtable", "emp")
                    .option("user", "root")
                    .option("password", "cloudera")
                    .load()
                    
     jdbcDF.show()
      
     jdbcDF.createOrReplaceTempView("empTempView")
     spark.sql("SELECT * FROM empTempView").show()
     
     
     // Snippet 2:  Writing to MySQL using JDBC
     spark.sql("SELECT * FROM empTempView")
        .write
        .format("jdbc")
        .option("url", "jdbc:mysql://localhost:3306/sparkdb")
        .option("driver", "com.mysql.jdbc.Driver")
        .option("dbtable", "emp2")
        .option("user", "root")
        .option("password", "cloudera")
        .mode(SaveMode.Overwrite)
        .save()        
    
     // Snippet 3: Writing to MySQL using JDBC
     val ratingsCsvPath = "data/movielens/ratings.csv"
     val ratingsDf = spark.read
                            .format("csv")
                            .option("header", "true")
                            .option("inferSchema", "true")
                            .load(ratingsCsvPath)
     
     ratingsDf.printSchema()
     ratingsDf.show(10)
     
     ratingsDf.write
      .format("jdbc")
      .option("url", "jdbc:mysql://localhost:3306/sparkdb")
      .option("driver", "com.mysql.jdbc.Driver")
      .option("dbtable", "movielens_ratings2")
      .option("user", "root")
      .option("password", "cloudera")
      .mode(SaveMode.Overwrite)
      .save()
      
      
      spark.close()
      
     
  }
}


   Hive Format - Integrating with Hive
   -----------------------------------

   val warehouseLocation = new File("warehouse").getAbsolutePath
    println(warehouseLocation)

    val spark = SparkSession
            .builder()
            .appName("DataSourceHive")
            .config("spark.sql.warehouse.dir", warehouseLocation)
            .config("spark.master", "local[1]")
            .enableHiveSupport()
            .getOrCreate()      
      
    import spark.implicits._
  
    spark.sparkContext.setLogLevel("ERROR")
    
    spark.sql("drop database if exists sparkdemo cascade")
    spark.sql("create database if not exists sparkdemo")
    spark.sql("show databases").show()   
    
    spark.sql("use sparkdemo")
    
    println("Current database: " + spark.catalog.currentDatabase)
    
    spark.catalog.listTables().show()
     
    val createMovies = 
      """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
    val loadMovies = 
      """LOAD DATA LOCAL INPATH 'data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
    val createRatings = 
      """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
    val loadRatings = 
      """LOAD DATA LOCAL INPATH 'data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
        
    spark.sql(createMovies)
    spark.sql(loadMovies)
    spark.sql(createRatings)
    spark.sql(loadRatings)
    
    spark.catalog.listTables().show()
     
    // Queries are expressed in HiveQL
    val moviesDF = spark.table("movies")   //spark.sql("SELECT * FROM movies")
    val ratingsDF = spark.table("ratings")  //spark.sql("SELECT * FROM ratings")
           
    val summaryDf = ratingsDF
                      .groupBy("movieId")
                      .agg(count("rating") as "ratingCount", 
                           avg("rating") as "ratingAvg")
                      .filter("ratingCount > 25")
                      .orderBy(desc("ratingAvg"))
                      .limit(10)
              
    summaryDf.show()
    
    val joinStr = summaryDf.col("movieId") === moviesDF.col("movieId")
    
    val summaryDf2 = summaryDf
                     .join(moviesDF, joinStr)
                     .drop(summaryDf.col("movieId"))
                     .select("movieId", "title", "ratingCount", "ratingAvg")
                     .orderBy(desc("ratingAvg"))
    
    summaryDf2.show()
    
    summaryDf2.write.format("hive").saveAsTable("topRatedMovies")
    spark.catalog.listTables().show()
        
    val topRatedMovies = spark.table("topRatedMovies")  //spark.sql("SELECT * FROM topRatedMovies")
    topRatedMovies.show() 
    
    spark.stop()


  ==================================
    Spark Streaming
  ==================================

     Spark's real-time data processing API

     Two APIs are available:

	1. Spark Streaming a.k.a DStreams API    (out-dated)
	2. Structured Streaming	(current and preferred API)

      Spark Streaming (DStreams API)
      ------------------------------

     => microbatch based processing
     => Provides "seconds" scale latency.  (near-real-time processing)
     => does not support event-time processing

       StreamingContext :
	  -> Is the starting point of execution
	  -> Defines a micro-batch window
	  -> Each micro-batch is an RDD

       DStream (discretized stream)
	  -> Is a continuous flow of RDDs.
	  -> Each micro-batch is represented as an RDD.

	  
  	# Create a local StreamingContext with two threads and batch interval of 1 sec.
	sc = SparkContext("local[2]", "NetworkWordCount")
	ssc = StreamingContext(sc, 1)

	lines = ssc.socketTextStream("localhost", 9999)
	words = lines.flatMap(lambda line: line.split(" "))
	pairs = words.map(lambda word: (word, 1))
	wordCounts = pairs.reduceByKey(lambda x, y: x + y)
	wordCounts.print()

	ssc.start()             
	ssc.awaitTermination() 


   Spark Structured Streaming
   -------------------------- 

	-> Consider the input data stream as the 'Input Table'. 
	  Every data item that is arriving on the stream is like a new row being appended to the Input Table.

	=> DataFrame -> Unbounded Table

	-> A query on the input will generate the 'Result Table'. 

	-> Every trigger interval (say, every 1 second), new rows get appended to the Input Table, 
	   which eventually updates the Result Table. 

	-> Whenever the result table gets updated, we would want to write the changed result 
	   rows to an external sink.

	Output Modes
	------------
	The 'Output' is defined as what gets written out to the external storage. 
	The output can be defined in a different mode:

	* Complete Mode - The entire updated Result Table will be written to the external storage.

	* Append Mode - Only the new rows appended in the Result Table since the last trigger will 
		      be written to the external storage. 

		      This is applicable only on the queries where existing rows in the 
		      Result Table are not expected to change.

	* Update Mode - Only the rows that were updated in the Result Table since the last trigger 
		      will be written to the external storage. 

		      Note that this is different from the Complete Mode in that this mode 
		      only outputs the rows that have changed since the last trigger. 

		      If the query doesn't contain aggregations, it will be equivalent to Append mode.


	Sample Socket Stream Example
	----------------------------

	from pyspark.sql import SparkSession
	from pyspark.sql.functions import explode
	from pyspark.sql.functions import split

	spark = SparkSession \
    		.builder \
    		.appName("StructuredNetworkWordCount") \
    		.getOrCreate()

	lines = spark \
    		.readStream \
    		.format("socket") \
    		.option("host", "localhost") \
    		.option("port", 9999) \
    		.load()

	# Split the lines into words
	words = lines.select( explode(split(lines.value, " ")).alias("word"))

	# Generate running word count
	wordCounts = words.groupBy("word").count()

	query = wordCounts \
    		.writeStream \
    		.outputMode("complete") \
    		.format("console") \
    		.start()

	query.awaitTermination() 


	=> Sources: File, Socket, Rate, Kafka

	=> Sinks: File, Console, Memory, Kafka, ForEach, ForEachBatch











