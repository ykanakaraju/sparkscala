 
  Agenda (Spark using Scala)
  -----------------------------------------
   Scala
	-> Language Basics
	-> Funtional Programming Basics
	-> OOP Basics
   Spark - Basics & Architecture
   Spark Core API
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
   Spark Streaming
	-> Structured Streaming

  Materials
  ---------
	=> PDF Presentations
	=> Code Modules 
	=> Class Notes 
        => Github: https://github.com/ykanakaraju/

 ===============================================================

   Getting started with Spark with Scala
   --------------------------------------


   1. Download Apache Spark and work with the Scala shell

	Url: https://spark.apache.org/downloads.html
	Download the .tgz file and extract it to a suitable place.

	Add the <Spark-Path>\bin to your PATH environment variable.

	Open a Command terminal and type "spark-shell"


   2. Installing the IDE	
	
	-> Scala IDE for Eclipse
		-> Make sure that you are running Java 8 or above (JDK 1.8.xx)
		-> Download Scala IDE for Eclispe and extract the zip file.
			https://scala-ide.org/download/sdk.html
		-> Extract the zip file to a suitable directory
			-> Navigate to 'eclispe' folder and launch the application	

	-> IntelliJ (with Scala Plugin)
		https://docs.scala-lang.org/getting-started/intellij-track/getting-started-with-scala-in-intellij.html


   3. Signup to 'Databricks Community Edition' (Free)

	Signup: https://www.databricks.com/try-databricks#account
	
		-> Fill the details with valid Email address and Next
		-> Select the "Get started with Community Edition" link (not 'Continue' button)
		-> Complete the process by following instruction the email sent to you

	Login: https://community.cloud.databricks.com/login.html


  Scala Programming language
  --------------------------

  -> Scala is a multi-paradigm programming language

	-> Scala is functional programming language
	-> Scala is a pure object oriented programming language
		    
     
  -> Scala is pure object oriented programming language

	-> Scala has no primitives
	-> Scala has no operators
	-> Everything in Scala is a 'instance' of some class/trait
	-> All operators are methods in scala.


  -> Scala Types

	-> immutable : unchangable, use 'val'
	-> mutable : changable, use 'var'


  -> Type declaration

	val i : Int = 10
	val name : String = "Raju"
	val flag : Boolean = true
	val hobbies : List[Int] = List("cricket", "chess", "reading")


  -> Scala infers the types based on the value assigned. 

	val i = 10
	val name = "Raju"
	val flag = true
	val hobbies = List("cricket", "chess", "reading")


  -> Scala is a statically typed language

	-> The data type of every variable is determined at compile time.
	-> Once declared, the type can not be changed.


  ->  Scala Blocks

	-> A block is a set of statements enclosed in  { .. }
	-> A block returns a value
	-> The return value of the block is the value of the last executed statement/expression.


  -> Scala 'Unit'

	-> In Scala, Unit is an object that has no value
        -> Printed as "()"
 

  -> Scala Class Heirarchy

	    Any => AnyVal => Int, Long, Double, Boolean, Char, Unit, ...
		=> AnyRef => String, List, Seq, .....


  -> String interpolations

	's' interpolator
	
		val str = s"Name: $name, Age:${age + 10}, Height: $height"
  		println(str)

	'f' interpolator => s + formatting chars

		 val str = f"Name: $name, Age:$age, Height: $height%2.2f"
  		println(str)

	'raw' interpolator => s + escapes the escape chars

  		val str = raw"E:\newdir\table1.txt"
  		println(str)


  -> Input & Output


	   val name = StdIn.readLine("What is your name?")   
	   println(name)
	   
	   print("What is your age?")
	   val age = StdIn.readInt()
		
           ---------------------------------

	   printf("Name: %s, Age: %d, Height: %4.2f", name, age, height)


  -> Flow Control Constructs

	
      	if..else

		val ageGroup = if (age < 13) "Child"
			  else if (age < 20) "Teenager"      
			  else if (age < 60) "Adult"      
			  else "Senior"
     

	match..case
	
		ageGroup = age match {
			case x if ( x < 13 ) => s"child ($x)"
			case x if ( x < 20 ) =>  s"teenager ($x)"
			case x if ( x < 60 ) =>  s"adult ($x)"
			case _ => { "senior" }
		  }     


  -> Range Object

	Range(1, 10) => 1,2,3,4,5,6,7,8,9
	Range(1, 10, 2) => 1,3,5,7,9
	1 until 10 by 2 => 1,3,5,7,9
	1 to 10 => 1,2,3,4,5,6,7,8,9,10 (Range.Inclusive)
        1 until 10 => 1,2,3,4,5,6,7,8,9
	1 to 11 by 2 => 1,3,5,7,9,11 (Range.Inclusive)
	1 until 11 by 2 => 1,3,5,7,9


  Loops
  ------
     
      	while
	------
		var x = 20
		  
		  
		  while ( x > 0 ) {
			println(x)
			
			x -= 1
		  }


	do..while
  	---------
		var x = 20  
		  do {
			println(x)    
			x -= 1
		  } while ( x > 0 )


	foreach
	--------
		List("Scala", "Python", "Java").foreach( x => println( x.toUpperCase ) )
		(1 to 100 by 2).foreach(println)


	for
	----
		
		// simpl e for loop
		for( a <- 1 to 10 ) {
		   println(s"a = $a")
		}
		
		// nested for loop
		for( a <- 1 to 10; b <- 1 until 20 by 5; c <- List("Scala", "Python") ) {
			println(s"a = $a, b = $b, c=$c")
		}

		// for loop with guards
		for( a <- 1 to 10 if (a%2 == 0); b <- 0 to 10 by 2 if (b > a) ) {
			println(s"a = $a, b = $b")
		}

		//for comprehension
		val v1 = for( a <- 1 to 10 ) yield((a, a*2))


  Exception handling
  ------------------

	try {
	    ... some code ...
	}
	catch {		
           case e1: FileNotFoundException => { ... }
	   case e2: IOException => { ... }
	   case e3: Exception => { ... }
	}
	finally {
		...
	}	


  Collections
  ------------

	Three types of collections

	1. Sequences -> Ordered collection whose objects can be accessed using index

		1.1 IndexedSeq => Array, ArrayBuffer, Vector, Range
			-> Good of random access of the elements

		1.2 LinearSeq => List, ListBuffer, Stream, Queue
			-> Good for iterations and loops


	2. Map -> Collection of (key, value) pairs

		val m1 = Map( 1 -> 10, 2 -> 20, 5 -> 50, 9 -> 90 )

		m1(1) => 10
		m1(10) => java.util.NoSuchElementException: key not found: 10

		m1.get(1) => Some(10)
		m1.get(10) => None

		m1.getOrElse(1, 0) => 10
		m1.getOrElse(2, 0) => 20
		m1.getOrElse(10, 0) => 0


	3. Set -> Unordered collection of unique objects


  Option  : represents an object which may or may-not have a value.

	If there is a value, Option will return "Some" object
	If there is no value, Option will return "None" object

	val x : Option[Int] => Some(Int) or None


  Tuple :    Is an object that holds multiple elements of different types
	     Tuple is not a collection
	     A tuple with two elements id called a 'Pair'   

	val t1 = (1, 1.5, true, 10, List(1,1), ("Hi", 10.5))
	
	t1._1   => 1
	t1._3 => true
	t1._6._2 => 10.5



  Methods and Procedures
  ----------------------
   
     => Reusable code blocks

     => Method returns somes output
	Procedure returns no output (Returns 'Unit')
   


      Methods
      -------

		 def sum(a: Int, b: Int, c: Int) : Int = {       
		   a + b + c 
		 }
		
              	 // calling by position
		 val s = sum(10, 20, 30)		 
		 println(s"s = $s")

		 // calling by name
		 val s = sum(b=10, a=20, c=30)

		 // mixing positional and named params
	         // postitional params must be given first
		 def sum(a: Int, b: Int, c: Int, d: Int) : Int = {   
		   println(s"a = $a, b = $b, c = $c, d = $d")   
		   a + b + c + d
		 }
		
		 val s = sum(10, 20, d=30, c=40)

		 // methods with default values
		 def sum(a: Int, b: Int, c: Int=0, d: Int=0) : Int = {   
		   println(s"a = $a, b = $b, c = $c, d = $d")   
		   a + b + c + d
		 }
		
		 val s = sum(10, 20, 30, 40)
		 val s = sum(10, 20, 30)
		 val s = sum(10, 20)
		 val s = sum(10, 20, d=50)

		// methods can be defined with multiple parameter lists

  		def compute(a: Int, b: Int)(c: Int) = (a + b) * c  
 		println( compute(10, 20)(3) )


		//methods with variable length arguments
		def sum(a: Int, b: Int*) : Int = {        
			  var sum = 0
			  var i = 1
			  
			  println(s"a = $a")
			  
			  for( x <- b ){
				println(s"b $i = $x")
				sum += x
				i += 1
			  }          
			  sum + a          
		 }
		
		 val s = sum(10)
		 
		 println(s"s = $s")

   		 // recursive methods
		 def factorial(n: Int): Long = {  
			  if (n <= 1)
				 1  
			  else    
				 n * factorial(n - 1)
		 }				
		 println( factorial(5) )


	Procedures
        ----------
		def printInABox(s: String) {     
			val line = "-" * s.length()
			println("----" + line + "\n| " + s + " |\n" + line + "----")
		}
	   
		printInABox("Spark is a framework") 


  Functions
  ---------
	
	-> A functional programming language treats a function like a literal.
	-> A function is anonymous by nature

		(<zero or more args>) => { ..some block of code }
		(a : Int, b: Int) => a + b 

		println( (a : Int, b: Int) => a + b,  ((a : Int, b: Int) => a + b)(10, 20)  )

	-> A function can be assigned to a variable (called named function)

	-> A function can be passed as an argument to a function or method

			def f1(a: Int, b: Int, f: (Int, Int) => Int) = f(a, b)
			
			val f2 = (a: Int, b: Int, f: (Int, Int) => Int) => f(a, b)
			
			val sum = (x: Int, y: Int) => x + y
			val prod = (x: Int, y: Int) => x * y
			val q = (x: Int, y: Int) => x / y
			
			def sum1(x: Int, y: Int) = x + y
				
			println( f2(20, 3, sum1) )

	-> A function/method can return a function

			def calculate(op : String) = {      
			  op match {
				case "+" => (a: Int, b: Int) => a + b
				case "-" => (a: Int, b: Int) => a - b
				case "*" => (a: Int, b: Int) => a * b
				case "/" => (a: Int, b: Int) => a / b
				case _ => (a: Int, b: Int) => a % b
			  }
			}
			 
			val s = calculate("+")
			val p = calculate("*")
			val m = calculate("blah")
			
			println( m(100, 30) ) 


	Function literal				Function Type
	--------------------------------------------------------------------
	(a: Int) => a * a				Int => Int
	(s: String) => s.length				String => Int
	(a: Int, b: Int) => a + b			(Int, Int) => Int
	(s: String) => println(s)			String => Unit
	() => "Windows 10"				() => String
	(a: Int, b: Int) => List(a, b)			(Int, Int) => List[Int]
	(a: (Int, Int), b: (Int, Int)) 			((Int, Int), (Int, Int)) => (Int, Int)
	   => (a._1 + b._1, a._2 + b._2)
	(a: Int, b: Int, f: (Int, Int) => Int) 		 (Int, Int, (Int, Int) => Int) => Int
			=> f(a, b)


  Higher Order Functions
  ----------------------
   => HOFs are those that take a function (or method) as a parameter or those that return a function.


  1. map		P: U => V
			Transforms an input collection to an output collection by applying the function
			object to object transformation.

			val l1 = List(1,2,3,6,4,3,5,6,9,6,5,8).map( x = x*10 )


  2. filter		P: U => Boolean
			Returns an output collection by filtering the input collection based on the function

			val l1 = List(1,2,3,6,4,3,5,6,9,6,5,8).filter( x = x > 4 )


  3. reduce		reduceLeft (reduce) & reduceRight
			P: (U, U) => U
			Reduces an entire collection to a single value of the same type by iterativly 
			appying the function

			List(1,2,3,6,4,3,5,6,9,6,5,8).reduceLeft( (x, y) = x + y )
			List(1,2,3,6,4,3,5,6,9,6,5,8).reduceRight( (x, y) = x + y )

		
  4. mapValues		P: U => V
			Operates only on map objects
			Transforms the value part of the key-value pairs		

			val m1 = Map( 1 -> 10, 2 -> 20, 3 -> 30, 4 -> 40 )
			val m2 = m1.mapValues(v => (v, 1))
			m2: scala.collection.immutable.Map[Int,(Int, Int)] = Map(1 -> (10,1), 2 -> (20,1), 3 -> (30,1), 4 -> (40,1))


  5. flatMap		P: U -> GenTranversableOnce[V]
			flatMap flattens the iterables returned by the function.

			val l1 = Source.fromFile("E:\\Spark\\wordcount.txt").getLines.toList
			val words = l1.flatMap(x => x.split(" ")) 
			


  6. sortWith		P: U => <binary sorting expression>
			Elements of the collection are sorted based on the binary sorting function

			words.sortWith( (x,y) => x.length < y.length )


  7. groupBy		P: U => V
			Returns a map, where
				key: each unique value of the function output
				values: Iterable with all the values that produced the key.

			words.groupBy(x => x).mapValues( _.length )
			


   Use-Case: Wordcount program using Scala HOFs

	val file = raw"E:\Spark\wordcount.txt"
     
        val data = Source.fromFile(file)
                 .getLines
                 .flatMap(x => x.split(" "))
                 .toList
                 .groupBy(w => w)
                 .mapValues(l => l.length)
                 .toList
                 .sortWith( (x,y) => x._2 > y._2 )
                 


   Object Oriented Programming in Scala
   -------------------------------------

	-> class   	=> class, case class, abstract class, sealed class
	-> object	=> object, case object
	-> trait
	-> package object


   Class
   -----
	-> In Scala, a class is not declared as public. 
	-> A Scala source file can contain multiple classes, and all of them have public visibility. 
	-> All class methods and properties are 'public' by default
	-> Scala provides getter and setter methods for every field.
	-> A class can inherit (extend) only one other class. 
	-> A class can extend multiple traits
	

   Constructors
   -------------

	-> In Scala, every class should have one primary constructor 
	    and optionally  additional auxiliary constructors.

	-> The auxiliary constructors are defined with method 'this'. 
	   Each auxiliary constructor must start with a call to a previously defined 
	   auxiliary constructor or the primary constructor.

	-> In Scala, every class has a primary constructor. 
	   The primary constructor is not defined with a this method. 
	   It is interwoven with the class definition.

	-> The parameters of the primary constructor are placed immediately after the class name. 
	   The primary constructor executes all statements in the class definition.



	import scala.io._
	import scala.math._

	object Demo1 extends App {
				
		val p1 = new Person()                	// calling PC
		p1.setName("Raju")
		p1.setAge(45)
		p1.printProperties()
			
		println("-------------------------")
		
		val p2 = new Person(2)              	// calling AC 1
		p2.setName("Ramesh")
		p2.setAge(35)
		p2.printProperties()
		
		println("-------------------------")   
		
		val p3 = new Person(3, "Ravi")     	// calling AC 2
		p3.setAge(35)
		p3.printProperties()
		
		println("-------------------------") 
		
		val p4 = new Person(3, "Rajesh", 25)     // calling AC 2
		p4.printProperties()
		
	}

	class Person {  
	   
	   private var _id = -1
	   private var _name = "Anonymous"
	   private var _age = -1  
	   
	   println("Executing primary constructor")
	   //println(_id, _name, _age)
	   
	   // AC - 1
	   def this(id: Int) {
		 this()       // calling PC
		 this._id = id
		 println("Executing auxiliary constructor with no parameters")
	   }
	   
	   // AC - 2
	   def this(id: Int, name: String) {
		 this(id)
		 this._name = name
		 println("Executing auxiliary constructor with 2 parameters")
	   }
	   
	   // AC - 3
	   def this(id: Int, name: String, age: Int) {
		 this(id, name)
		 this._age = age
		 println("Executing auxiliary constructor with 3 parameters")
	   }
		  
	   def getId = _id   
	   //def setId(id: Int) = this._id = id
	   
	   def getName = _name
	   
	   def setName(name: String) = this._name = name
	   
	   def getAge = _age
	   
	   def setAge(age: Int) = {
		 if (this._age < age) this._age = age     
	   }
	   
	   def printProperties() {
		 println(s"id = ${_id}, name=${_name}, age=${_age}")
	   }
	}


  Object
  ------
    
	-> Object is a singleton entity
	-> Objects will have only singleton methods
	-> Object can have only primary constructor (no aux. constructor)
	-> The PC of the object is invoked the first time the object is referenced.   

	object Address {
	  
	  println("==============================")
	  println("Execution Address constructor")
	  
	  def getAddress(location_id: Int) = {
		 if (location_id == 1) "Address 1"
		 else if (location_id == 2) "Address 2"       
		 else if (location_id == 3) "Address 3"       
		 else "Other Address"
	  }
	}


	object Demo1 extends App {
		 
	   var e1 = new Emp(100, "Raju", 1)
	   var e2 = new Emp(101, "Ravi", 2)   
			
	   println( Address.getAddress(e1.pLoc) )
	   println( Address.getAddress(e2.pLoc) )
	   println( Address.getAddress(5) )
	}



  Companion object
  ----------------
      -> Is used to add a singlton method for a class
      -> A companion object will have same name as the class and is defined in the same file


  Trait
  -----
      -> Similar to an "interface" in Java, but with some additional features.
      -> A trait can have both abstract and concrete methods.
      -> We can have fields inside a trait
      -> A class can extend any number of traits (but can extend only one class)


  Case class
  ----------
	-> Case Class is a special type of class
	-> It has all the features of a regular class but a few methods will be provided out of box.
	
		-> apply, unapply
		-> toString, hash, copy, equals

	-> Case classes are specifically used for pattern matching. 

 ================================================================================

   Spark
   -----
	-> Is an in-memory distributed computing framework for performing big data analytics
		in-memory -> ability to persist intermediate results in RAM (memory)

	-> Spark in written in SCALA
		
	-> Spark is a 'unified' analytics framework
	
	-> Spark is a polyglot
               Supports Scala, Java, Python and R

	-> Spark apps can run on multiple cluster managers or can run on local machine also
		Spark standalone, YARN, Mesos, Kubernetes
	

   Spark unified framework
   -----------------------
	-> Spark provides a consistent set of libraries for performing different analytics workloads
	   using the same execution engine and some well-defined data abstractions

	-> Spark APIs

		Spark Core    	=> Low-level API (RDD API)
		Spark SQL	=> For Batch Processing of structured data
		Spark Streaming	=> For Streaming Ananlytics
		Spark MLLib	=> For Predictive Analytics (Machine Learning)
		Spark GraphX	=> For Graph Parallel Computations


   Spark Architecture
   ------------------

    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, Standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks are reported to the driver. 
	

  
   Getting started with Spark
   --------------------------  	

    1. Use your lab
    2. Setup the IDE on your local machine
    3. Using Databricks cluster (use Databricks community edition)


   Creating a Spark project using Maven
   ------------------------------------

	Setting up a Maven Spark Application:

	  => Open Scala IDE
	  => File -> New -> Scala Project
		 -> Make sure jdk1.8 is selected
		 -> Cleck Finish
	  => (Optional) Change you Scala compiler version to 2.11
		 -> Right-click on you project
		 -> Scala -> Set Scala Installation -> Select 2.11.11
	  => Convert into Maven application 
		 -> Right-click on you project
		 -> Configure -> Conert to Maven -> Accept default -> Finish
	  => Open pom.xml file (close it and open it one more time if it is not showing pom.xml link)
		 -> Open pom.xml tab (towards the bottom)
		 -> add <dependencies></dependencies> XML tag
		 -> Add required dependencies between <dependencies> tag

	  => MVN Repository Link: https://mvnrepository.com/artifact/org.apache.spark


  RDD (Resilient Distributed Dataset)
  -----------------------------------

     -> RDD represents a collections of distributed in-memory partitions (that are executed in parallel)
        -> A partition is a collection of objects of some type

     -> RDDs are immutable

     -> RDDs are lazily evaluated
	-> transormations does not cause execution
	-> action commands trigger execution (and launch jobs on the cluster)


  Creating RDDs
  -------------
	Three ways:

	1. Create an RDD from external datasets

		val rddFile = sc.textFile(<dataPath>, 4)

	2. Create an RDD from programmatic data

		val rdd1 = sc.parallelize( List(3,2,1,4,2,3,5,7), 2 )

	3. By applying transformations on existing RDDs
	
		val rddWords = rddFile.flatMap(x => x.split(" "))


  RDD Operations
  --------------
	Two operation

	1. Transformations
		-> Does not cause execution
		-> Only create lineage DAGs

	2. Actions
		-> Trigger execution
		-> Produces output


  RDD Lineage DAG
  ---------------
  -> RDD Lineage DAg is a logical plan created when the RDD is created
  -> Maintained by the driver
  -> Contains a heirarchy of dependencies all the way from the very first RDD. 


	val rddFile = sc.textFile("E:\\Spark\\wodcount.txt", 4)

(4) E:\Spark\wordcount.txt MapPartitionsRDD[3] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[2] at textFile at <console>:24 []


	val rddWords = rddFile.flatMap(x => x.split(" "))

(4) MapPartitionsRDD[4] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[3] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[2] at textFile at <console>:24 []


	val rddPairs = rddWords.map( x => (x,1) )

(4) MapPartitionsRDD[5] at map at <console>:25 []
 |  MapPartitionsRDD[4] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[3] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[2] at textFile at <console>:24 []


	val rddWc = rddPairs.reduceByKey( (x,y) => x + y )

(4) ShuffledRDD[6] at reduceByKey at <console>:25 []
 +-(4) MapPartitionsRDD[5] at map at <console>:25 []
    |  MapPartitionsRDD[4] at flatMap at <console>:25 []
    |  E:\Spark\wordcount.txt MapPartitionsRDD[3] at textFile at <console>:24 []
    |  E:\Spark\wordcount.txt HadoopRDD[2] at textFile at <console>:24 []


	rddWc.collect()
	rddWc.saveAsTextFile(<Path>)


  Types of Transformations
  ------------------------
	
	1. Narrow Transformations
           -> Narrow transformations are those, where the computation of each partition depends ONLY
              on its input partition.
           -> There is no shuffling of data.
           -> Simple and efficient


      	2. Wide Transformations
           -> In wide transformations, the computation of a single partition depends on all/many
              partitions of its input RDD.
           -> Data shuffle across partitions will happen.
           -> Complex and expensive


  RDD Persistence
  ---------------
	rdd1 = sc.textFile(<dataPath>, 10)
	rdd2 = rdd1.t2(..)
	rdd3 = rdd1.t3(..)
	rdd4 = rdd3.t4(..)
	rdd5 = rdd3.t5(..)
	rdd6 = rdd5.t6(..)
	rdd6.persist()   ---> instruction to Spark to save rdd6 partitions
	rdd7 = rdd6.t7(..)

	rdd6.collect()
	lineage DAG => (10) rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		[textFile, t3, t5, t6] -> collected.

	rdd7.collect()
	lineage DAG => (10) rdd7 -> rdd6.t7
		[t7] -> collected.

	rdd6.unpersist()


	StorageLevels
        -------------
	MEMORY_ONLY		=> default, Memory Deserialized 1x Replicated
	MEMORY_AND_DISK		=> Disk Memory Deserialized 1x Replicated
	DISK_ONLY		=> Disk Serialized 1x Replicated
	MEMORY_ONLY_SER		=> Memory Serialized 1x Replicated
	MEMORY_AND_DISK_SER	=> Disk Memory Serialized 1x Replicated
	MEMORY_ONLY_2		=> Memory Deserialized 2x Replicated
	MEMORY_AND_DISK_2	=> Disk Memory Deserialized 2x Replicated
	
       Commands
       --------
	rdd1.cache()					=> in-memory persistence
	rdd1.persist()					=> in-memory persistence
	rdd1.persist(StorageLevel.MEMORY_AND_DISK)	=> with specific storage-level

	rdd1.unpersist()


  Spark Executor Memory Structure
  -------------------------------
     
      Let us say we are requesting an executor with 10 GB RAM. 
      The spark job will be allocated executors with 10.3 GB (10GB + 300MB) RAM. 
     
      
      1. Reserved Memory (300 MB)
           -> Spark's internal usage
      
      2. Spark Memory (spark.memory.fraction: 0.6)   => 6 GB (Unified Memory)
          
         2.1  Execution Memory
                 -> Used for RDD partition creating and transformations
                 -> Can forcibly evict RDD partitions from storage memory if it requires
                    additional memory upto the quote allocated for it.

         2.2  Storage Memory (spark.memory.storageFraction: 0.5) => 3 GB
                 -> The RDD partitions and broadcast variables are persisted
                    in this memory.

      3. User Memory  => 4 GB
         -> Running non-spark related code execution. 
         -> Related to running python methods, storing python collection.


  RDD Transformations
  --------------------
   
  A transformation applied on an RDD with the ouput RDD.

 
  1. map		P: U => V
			object to object transformation
			Transforms the input object by applying the function
			input RDD: N objects, output RDD: N objects

		val rdd2 = rdd1.map( x => x*10 )


  2. filter		P: U => Boolean
			Returns only those object for which the function returns True
			input RDD: N objects, output RDD: <= N objects
	
		rddWords.filter(x => x.length == 3).collect


  3. glom		P: None
			Returns one Array per partition with all the values of the partition

		rdd1		rdd2 = rdd1.glom()

		P0: 4,3,1,4,5 => glom => P0: Array(4,3,1,4,5)
		P1: 7,8,9,0,9 => glom => P1: Array(7,8,9,0,9)
		P2: 4,2,9,0,8 => glom => P2: Array(4,2,9,0,8)

		rdd1.glom.map(x => x.length).collect


  4. flatMap		P: U => TraversableOnce[V]
			flatMap flattens the iterables returned by the function.
			input RDD: N objects, output RDD: >= N objects
		
		val rddWords = rddFile.flatMap(x => x.split(" "))


  5. mapPartitions	P: Iterator[U] -> Iterator[V]
			partition to partition transformation


		val rdd2 = rdd1.mapPartitions( p => p.map(x => (x, x)) )
		val rdd2 = rdd1.mapPartitions( p => List(p.toList.sum).iterator ).collect


  6. mapPartitionsWithIndex    P: (Int, Iterator[U]) -> Iterator[V])
			Similar to mapPartitions, but gives you the partition-index as additional argument

		rdd1.mapPartitionsWithIndex( (i, p) => p.map(x => (i, x*10))).collect()
		rdd1.mapPartitionsWithIndex( (i, p) => List((i, p.sum)).iterator ).glom().collect()


  7. distinct		P: None, Optional: numPartitions
			Returns distinct objects of the RDD

		rddWords.distinct.collect

  Types of RDDs
  -------------

	1. Generic RDDs :  RDD[U]
	2. Pair RDDs	:  RDD[(K, V)]

 
  8. mapValues		P: U => V
			Applied only on Pair RDDs
			Transforms only the 'value' part of the (K,V) pairs

		val rdd3 = rdd2.mapValues(x => (x, 10))
		  -> where rdd3 is a pair-rdd


  9. sortBy		P: U => V, Optional: ascending (true/false), numPartitions
			Sort the objects of the RDD based on the function output.

		rdd1.sortBy(x => x%5).glom().collect()
		rdd1.sortBy(x => x%5, false).glom().collect()
		rdd1.sortBy(x => x%5, true, 2).glom().collect()


  10. groupBy		P: U => V, Optional: numPartition
			Returns a Pair RDD, where
				key: unique value of the function output
				value: CompactBuffer - all objects of the RDD that produced the key.

		    val rddWc = sc.textFile("./data/wordcount.txt", 4)
                  		.flatMap(x => x.split(" "))
                  		.groupBy(x => x)
                  		.mapValues(x => x.toList.length)
                  		.sortBy(x => x._1, true, 1)


  11. randomSplit	P: Array of weights (ex: Array(0.5, 0.5)), Optional: seed	
			Splits the RDD into multiple RDD in the specified weights

		val rddArr = rdd1.randomSplit(Array(1,1), 4234)
		val rddArr = rdd1.randomSplit(Array(1,1))			


  12. repartition	P: numPartitions
			Is used to increase or decrease the number of partitions of the output RDD
			Performs global shuffle


  13. coalesce		P: numPartitions
			Is used to only decrease the number of partitions of the output RDD
			Performs partition merging

	
		Recommendations
		---------------
		-> The size of the partition should be between 100 MB to 1 GB
		-> The ideal size of each partition if you are using hadoop is 128 MB
		-> The number of partitions should be a multiple of number of cores
		-> The number of cores per executor (in YARN) is 5


   ..ByKey transformations
   -----------------------
    	-> Are wide transformations
	-> Applied only on PairRDD	


  14. sortByKey		P: None, Optional: ascending (true/false), numPartitions
			Sorts the RDD based on the key
			 
		rddPairs.sortByKey().collect
		rddPairs.sortByKey(false).collect
		rddPairs.sortByKey(true, 1).collect


  15. groupByKey	P: None, Optional: numPartitions
			Returns a PairRDD where
				key: unique key
				value: CompactBuffer - grouped values

			Caution: Avoid groupByKey

    		val rddWc = sc.textFile("./data/wordcount.txt", 4)
                  		.flatMap(x => x.split(" "))
                  		.map(x => (x, 1))
                  		.groupByKey(1)
                  		.mapValues(x => x.sum)


  16. reduceByKey	P: (U, U) => U, Optional: numPartitions
			Reduces all the values of each unique key with in each partition in the first stage
			and across partitions in the next stage. 	

		val rddWc = sc.textFile("./data/wordcount.txt", 4)
                  		.flatMap(x => x.split(" "))
                  		.map(x => (x, 1))
                 	 	.reduceByKey( (x, y) => x + y )



  Use-Case
  --------
	
    dataset: https://github.com/ykanakaraju/sparkscala/blob/master/data/cars.tsv

    From cars.tsv dataset, get the average weight of all the models of each unique make from American origin cars
    -> Arrange the data in the DESC order of average-weight
    -> Save the output as a single text file
	
    => Try to do it yourself



  RDD Actions
  -----------

  1. collect    => returns an Array with all the objects of the RDD to the driver.

  2. count      => returns the count of objects in the RDD

  3. saveAsTextFile  => Save each partition of the RDD as a separate text-file in the specified non-existing directory.

  4. reduce	=> P: (U, U) => U
		   Reduces the entire RDD to one value of the same type.
		   Works at each partition at first stage and across partitions in the next stage.

			RDD[U].reduce( fn )  => U

		rdd1			
		P0: 1,4,2,4,6,5 -> reduce -> -20 -> reduce -> 17
		P1: 4,6,7,8,0,2 -> reduce -> -19
		P2: 5,7,6,9,0,1 -> reduce -> -18

 5. take(n)	=> Returns a Array object with first n objects of the RDD

		rddWc.take(5)

 6. takeOrdered(n, [Ordering])  => Returns a Array object with first n objects of the RDD in the sorted RDD

		rddWords.takeOrdered(20)
		rddWords.takeOrdered(20)(Ordering[String].reverse)

 7. takeSample	=> returns a Array of sampled objects using with/without replacement sampling 

		rdd1.takeSample(true, 10)	=> true: with-replacement sampling
		rdd1.takeSample(true, 10, 334)

		rdd1.takeSample(false, 10)	=> true: with-out-replacement sampling
		rdd1.takeSample(false, 10, 334)

 8. countByValue

		rddWords.countByValue
		res61: scala.collection.Map[String,Long] 
		  = Map(map -> 6, mapreduce -> 6, das -> 6, flatmap -> 12, sqoop -> 6, sadas -> 1, oozie -> 6, rdd -> 43, transformations -> 10, hadoop -> 25, asd -> 5, spark -> 40, hive -> 19, scala -> 28, actions -> 10, flume -> 6, groupby -> 6, hdfs -> 6, d -> 1)

 9. countByKey	
		rdd3.countByKey
		res65: scala.collection.Map[Int,Long] = Map(0 -> 8, 3 -> 6, 1 -> 10, 2 -> 6)


 10. foreach    => runs a function on all the objects of the RDD
		
		val x = rddWc.foreach( x => println(s"key: ${x._1}, value: ${x._2}") )

 11. saveAsSequenceFile 

		rddWc.saveAsSequenceFile("E:\\Spark\\output\\seq")

 12. saveAsObjectFile

		rddWc.saveAsObjectFile("E:\\Spark\\output\\obj")

		

  Shared Variables
  ----------------
	
   In spark, a closure constitute all the code (variables and methods) that must be visible inside an
   executor for the task to perform their computations on RDD partitions. 

   Spark serializes the closure and separate copy of it is sent to every executor. 

   Global counters can not be implemented using local variables because they have no shared state.
             

	Closure problem
        ----------------

	var counter = 0

	def isPrime(n: Int) : Boolean = {
	   var flag = true
	   for (i <- 2 to n-1) {
	      if ( n % i == 0 ) flag = false
	   }
	   flag 
	}
	
	def f1(n: Int) = {
	   if (isPrime(n)) counter += 1
	   n * 2
	}

    	val rdd1 = sc.parallelize( 1 to 4000, 4 )
	var rdd2 = rdd1.map( f1  )

	rdd2.collect()	    

	println(counter)    // 0


	Limitation of closure: Local variables can not be used to implement global counter. 
	Solution: Use an 'Accumulator' variable.


   Accumulator
   -----------

	Accumulator variables are maintained by the driver
	They are not part of the closure, hense they are not local copies.
	They are shared by all tasks.
	All tasks can add to these accumulators.
	Spark provides numeric accumulatos (Long & Double)
	Used for implementing global counter.


        var counter = sc.longAccumulator("c")

	def isPrime(n: Int) : Boolean = {
	   var flag = true
	   for (i <- 2 to n-1) {
	      if ( n % i == 0 ) flag = false
	   }
	   flag 
	}
	
	def f1(n: Int) = {
	   if (isPrime(n)) counter.add(1)
	   n * 2
	}

    	val rdd1 = sc.parallelize( 1 to 4000, 4 )
	var rdd2 = rdd1.map( f1  )

	rdd2.collect()	    

	println(counter.value)    // 130


   Broadcast Variable
   ------------------ 

	Allows you to broadcast a copy of an immutable object (of big size) 
        to every executor that can be shared by all the tasks in that executor.


	case class Emp(eid: Int) { .....}

     	val empMap: Map[Int, Emp] = Map( 0 -> Emp(0), 101 -> Emp(101), 102 -> Emp(102), 103 -> Emp(102), .... )  // 100 MB
	val empMapBroadcast = sc.broadcast( empMap )    

	def empLookup( eId: Int ) : Emp = {
	    empMapBroadcast.value(eId)
	}

	val empIds: RDD[Int] = sc.parallelize( List(101,102,103,.......), 4 )
    	val emp: RDD[Emp] = empIds.map( empLookup )

	emp.collect() 


 Spark Submit command
 --------------------

     Is a single command to submit any spark application (Scala, Java, Python, R) to any cluster manager
     (local, YARN, Mesos, Kubernetes)

	 spark-submit [options] <app jar | python file | R file> [app arguments]

	 spark-submit --master yarn
		--deploy-mode cluster
		--driver-memory 2G
		--driver-cores 2
		--executor-memory 5G
		--executor-cores 5
		--num-executors 10
		--class com.cts.mysparkapp.Wordcount
		<jar file path>
		[app arguments]




 =======================================================
    Spark SQL  (org.apache.spark.sql)
 =======================================================

    => Spark's structured data processing API

	  Structured data formats: Parquet (default), ORC, JSON, CSV (delimited text file), Text
	  JDBC format: RDBMS, NoSQL
	  Hive Format: To process Hive data

    => SparkSession
	
	 -> Starting point of execution. 	 
	 -> Introduced from Spark 2.0 onwards
	 
    -> SparkSession represents a user-session running inside an application.
	
	 -> We can have multiple sessions inside an application.

	   val spark = SparkSession
              .builder
              .master("local[2]")              
              .appName("DataSourceBasic")
              .getOrCreate()   

           import spark.implicit._ 


  Spark SQL Data Abstarctions:
  ----------------------------
	
	=> Dataset
		=> Collection of typed objects

        => DataFrame
		=> Collection of "Row" objects
		=> Alias of Dataset[Row]

	=> Dataset and DataFrame have two components:
		-> Data : Row objects (in the case of DFs), or any other object (in the case of Dataset)
		-> Schema : StructType object

		StructType(
			StructField(age,LongType,true), 
			StructField(gender,StringType,true), 
			StructField(id,LongType,true), 
			StructField(name,StringType,true)
		)


   Basic steps in working with Spark SQL
   -------------------------------------

    1. Creating DataFrame from some data source

		val df1 = spark.read.format("json").load(inputPath)
		val df1 = spark.read.json(inputPath)

    2. Apply transformations on the DataFrames

	-> Using DF Transformation Methods

     		   val df2 = df1.select("id", "name", "age", "gender")
                  		.where("age is not null")
                  		.orderBy("age", "name")
                  		.groupBy("age").count()
                  		.limit(5)
	-> Using SQL

		//import spark.{sql, table}

		df1.createOrReplaceTempView("people")     
     		spark.catalog.listTables().show()
     
     		val df3 = spark.sql("select * from people where age is not null")
     		df3.show()     
     
     		val df4 = spark.table("people")
     		df4.show()

    3. Save the DataFrame to a strcutured file format / database / hive etc. 

 		val outputPath = "output/json"
  
     		df2.write.format("json").save(outputPath)
		df2.write.json(outputPath)    

		df2.write.mode(SaveMode.Overwrite).json(outputPath)  
 

   Save Modes
   -----------
	1. ErrorIfExists (default)
	2. Ignore
	3. Append
	4. Overwrite

	df2.write.mode(SaveMode.Append).json(outputPath)   
	df2.write.mode(SaveMode.Overwrite).json(outputPath)   


  Local Temp Views
  -----------------
      df.createOrReplaceTempView("flights")
      spark.catalog.listTables().show()
     
      val qry = """select * from flights
                   where count > 100
                   order by count desc 
                   limit 30"""
     
      val df2 = spark.sql(qry)
      df2.show()

		
  Global Temp Views
  -----------------
     df.createGlobalTempView("g_flights")
     spark.catalog.listTables().show()
     
     val qry = """select * from global_temp.g_flights
                   where count > 100
                   order by count desc 
                   limit 30"""
     
     val df2 = spark.sql(qry)
     df2.show()     
     
     val spark2 = spark.newSession()
     spark2.catalog.listTables().show()
     
     val df3 = spark2.sql(qry)
     df3.show()


  show command
  ------------
 
      	df1.show()    		=> prints 20 rows with truncate = true
	df1.show(30)  		=> prints 30 rows with truncate = true
	df1.show(false)		=> prints 20 rows with truncate = false
	df1.show(30, false)	=> prints 30 rows with truncate = false
	df1.show(30, 30)	=> prints 30 rows with truncated to 30 chars.



  Working with different file formats
  -----------------------------------

   1. JSON

	Read
	----
	val df1 = spark.read.format("json").load("people.json")
	val df1 = spark.read.json(inputFile)

	Write
	-----
	df2.write.format("json").save(outputDir)
	df2.write.json(outputDir)
	df2.write.mode(SaveMode.Overwrite).format("json").save(outputDir)

   2. CSV (delimted text file)

	Read
	----
	val df1 = spark.read
			.format("csv")
			.option("header", true)
			.option("inferSchema", true)
			.load(inputFile)  

	val df1 = spark.read
			.option("header", true)
			.option("inferSchema", true)
			.csv(inputFile) 

	val df1 = spark.read
			.option("header", true)
			.option("inferSchema", true)
			.option("sep", "\t")
			.csv(inputFile) 
	Write
	-----
	df2.write.format("csv").save(outputDir)
	df2.write.mode(SaveMode.Overwrite).format("csv").save(outputDir)
	df2.write.mode(SaveMode.Overwrite).option("header", true).format("csv").save(outputDir)
	df2.write.option("header", true).option("sep", "\t").format("csv").save(outputDir)
 

   3. Parquet (default)

	Read
	----
	val df1 = spark.read.format("parquet").load(inputFile) 
	val df1 = spark.read.parquet(inputFile)  

	Write
	-----
	df2.write.save(outputDir)   // default format is parquet   
	df2.write.format("parquet").save(outputDir)
	df2.write.parquet(outputDir)


   4. ORC

	Read
	----
	val df1 = spark.read.format("orc").load(inputFile) 
	val df1 = spark.read.orc(inputFile)  

	Write
	-----
	df2.write.format("orc").save(outputDir)
	df2.write.orc(outputDir)


   5. Text
	Read
	----
	val df1 = spark.read.format("text").load(filePath)
	val df1 = spark.read.text(filePath)

	Write
	-----
	df2.write.format("text").save(filePath)
	df2.write.text(filePath)



   Create a Dataset
   ----------------
   
	import spark.implicits._
          
     	case class Person(name: String, age: Long)
    
     	val persons = Seq(Person("Raju", 43), 
                       Person("Ram", 33), 
                       Person("Rahim", 23))
     
     	val df1 = persons.toDS()

	df1.show()



  Create an RDD from a DataFrame/Dataset
  --------------------------------------

	val rdd1 = ds1.rdd   
   	rdd1.collect.foreach(println)
    
        -----------------------------

	val filePath = "data/flight-data/json/2015-summary.json"        
   	val df1 = spark.read.json(filePath)
   
   	val rdd1 = df1.rdd
   
   	rdd1.take(5).foreach(println)



  Create a DataFrame from programmatic collection
  -----------------------------------------------

        val listUsers = Seq((1, "Raju", 5),
			(2, "Ramesh", 15),
			(3, "Rajesh", 18),
			(4, "Raghu", 35),
			(5, "Ramya", 25),
			(6, "Radhika", 35),
			(7, "Ravi", 70))
					   
	val df1 = spark.createDataFrame(listUsers).toDF("id", "name", "age")
	
	df1.show()
	df1.printSchema()
       -------------------------------
       import spark.implicits._

       val df1 = listUsers.toDF("id", "name", "age")


  
  Create a DataFrame from RDD
  ----------------------------

	val rdd1 = spark.sparkContext.parallelize(listUsers)

	val rdd1 = spark.sparkContext.parallelize(users)
	val df1 = spark.createDataFrame(rdd1).toDF("id", "name", "age")

	-------------------------------

        import spark.implicits._    
     	val df1 = rdd1.toDF("id", "name", "age", "dept") 
     
     	df1.show()

  
  Applying programmatic/custom schema on a DataFrame
  --------------------------------------------------

	val filePath = "data/flight-data/json/2015-summary.json"     
   
   	val mySchema = new StructType(Array(
                StructField("ORIGIN_COUNTRY_NAME", StringType, true),
                StructField("DEST_COUNTRY_NAME", StringType, true),
                StructField("count", IntegerType, true)
              ))   
   
   	val df1 = spark.read.schema(mySchema).json(filePath)



   DataFrame Transformations
   -------------------------

   1. select

	Two Syntaxes:

		val df2 = df1.select( string objects )

		val df2 = df1.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME", "count")      


	Passing column objects

		val df2 = df1.select( columns objects )

		// diffrent way to get column objects
		val df2 = df1.select(df1.col("DEST_COUNTRY_NAME"),
                        col("DEST_COUNTRY_NAME"),
                        column("ORIGIN_COUNTRY_NAME"),
                        expr("count"),
                        $"ORIGIN_COUNTRY_NAME",
                        'ORIGIN_COUNTRY_NAME)

		val df2 = df1.select(col("DEST_COUNTRY_NAME") as "destination",
                        col("ORIGIN_COUNTRY_NAME") as "origin",
                        expr("count"),
                        expr("count + 10 as newCount"),
                        expr("count > 200 as highFrequency"),
                        expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic"))



   2. where / filter	

		val df3 = df2.where("domestic = false and origin = 'United States'")
		val df3 = df2.filter("domestic = false and origin = 'United States'")	
		val df3 = df2.where( col("count") > 1000 )


   3. orderBy / sort

		val df3 = df2.orderBy("count", "origin")
		val df3 = df2.sort("count", "origin")
		val df3 = df2.orderBy(desc("count"), asc("origin"))


   4. groupBy	=> returns RelationalGroupedDataset object

		val df3 = df2.groupBy("domestic", "highFrequency").count()
		val df3 = df2.groupBy("domestic", "highFrequency").sum("count")
		val df3 = df2.groupBy("domestic", "highFrequency").max("count")
		val df3 = df2.groupBy("domestic", "highFrequency").avg("count")	

		val df3 = df2.groupBy("domestic", "highFrequency") 
               		.agg( 	count("count") as "count",
                     		sum("count") as "sum",
                     		max("count") as  "max",
                     		avg("count") as "avg"
			 )	

   5. limit
		val df2 = df1.limit(10)

	      
   6. selectExpr

		val df2 = df1.selectExpr("DEST_COUNTRY_NAME as destination",
                        "ORIGIN_COUNTRY_NAME as origin",
                        "count",
                        "count + 10 as newCount",
                        "count > 200 as highFrequency",
                        "DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")


   7. withColumn & withColumnRenamed

   		val df3 = df1.withColumn("newCount", expr("count + 10"))
                	.withColumn("highFrequency", expr("count > 200"))
                	.withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME"))
                	.withColumnRenamed("DEST_COUNTRY_NAME", "destination")
                	.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")
                	.withColumn("country", lit("India"))


		when..otherwise
		----------------

		 val ageGroupDf = usersDf.withColumn("ageGroup", when(col("age") < 13, "child")
                                                    .when(col("age") < 20, "teenager")
                                                    .when(col("age") < 60, "adult")
                                                    .otherwise("senior"))

		case..when
		----------
		val ageGroup_expr = """case 
                               when age <= 12 then 'child'
	                             when age <= 19 then 'teenager' 
	                             when age <= 60 then 'adult'
	                             else 'senior' 
	                         end"""
    
    		val ageGroupDf = usersDf.withColumn("ageGroup", expr(ageGroup_expr))

	
  8. udf (user-defined-function)
	
		def get_age_group(age: Int) : String = {
			if (age <= 12) "child"
			else if (age <= 18) "teenager"
			else if (age <= 60) "adult"
			else "senior"            
		 }
			 
		val get_age_group_udf = udf(get_age_group(_ : Int))
		
		val ageGroupDf = usersDf.withColumn("ageGroup", get_age_group_udf(col("age")))
														
		ageGroupDf.show()

		-----------------------------------------
		
		val get_age_group = (age: Int) => {
			if (age <= 12) "child"
			else if (age <= 18) "teenager"
			else if (age <= 60) "adult"
			else "senior"            
		 }
			 
		val get_age_group_udf = udf(get_age_group)
		
		val ageGroupDf = usersDf.withColumn("ageGroup", get_age_group_udf(col("age")))
														
		ageGroupDf.show()

		------------------------------------------

		spark.udf.register("get_age_group_udf", get_age_group)   
    
    		spark.catalog.listFunctions().select("name").show(1000)
    
    		val qry = "select id, name, age, get_age_group_udf(age) as ageGroup from users"    
    		val df3 = spark.sql(qry)    
    		df3.show()


  9. drop	=> drop/exclude columns

		val df3 = df2.drop("newCount", "highFrequency")
   		df3.show(5)


  10. DataFrameNaFunctions => drop rows with NULL values in any or specified columns.

		
		val usersDf = spark.read.json("./data/users.json")
    		usersDf.show()   
    
		// drop rows with null values
    		usersDf.na.drop().show()
		usersDf.na.drop( Seq("phone", "age") ).show()

		// fill the columns with null values with some other value
		usersDf.na.fill("No Value").na.fill(0).show()
		usersDf.na.fill("NA", Seq("phone", "name")).show()


  11. dropDulplicates   => drop duplicate values from the DF


  12. distinct 


  13. union, intersect, subtract


  14. randomSplit


  15. repartition


  16. coalesce


  17. join


 







