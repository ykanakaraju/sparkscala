
  Agenda (Spark using Scala)
  --------------------------
   -> Scala Refresher
   -> Spark - Basics & Architecture
   -> Spark Core API
	-> RDD - Transformations and Actions
	-> Spark Shared Variable
        -> Spark-Submit
   -> Spark SQL
	-> DataFrame Operations
	-> Integration with MySQL & Hive
	-> SQL Optimizations & Tuning
   -> Spark Streaming
	-> DStreams API (introduction)
	-> Structured Streaming

  Materials
  ---------
	=> PDF Presentations
	=> Code Modules
	=> Class Notes
	=> Github: https://github.com/ykanakaraju/sparkscala

  Scala
  -----
	
   -> Scala is Java based compiler language
        -> Scala compiles to Java byte code
	-> Interoperable with Java language
	
    -> Scala is multi-paradigm programming language	
	 -> Pure OOP language
         -> Functional programming language

    -> Pure OOP language
	 -> Scala does not have primitive and operator

   -> Scala Variables: immutables -> val
		       mutable    -> var

   -> Scala infers the type automatically based on the assigned value.

   -> Scala infix notation : 
		
	val j = i.+(10)  can be written in scala as:
	val j = i + 10

   -> Scala is a statically typed language
	-> The type of every object/variable is know at compile time. 
	-> Once assigned a type can not be changed. 

  Blocks
  ------

	-> One or more statements enclosed in { .. }
	    -> If there is only one statement/expression then you may omit the { .. }
	-> A block returns an ouput
	    -> The output is the value of the last statement that is executed in the block.
	-. Scala does not use "return" keyword

  Unit
  ----
	-> Is a class that represents 'no value'
        -> Printed as "()"  	

  Flow Control Statements
  -----------------------

   if..else
   ----------

	if (<boolean>) { ... }
	else if (<boolean>) { ... }
	else { ...}

	=> If statement in Scala, retruns a value

   match..case
   -----------
	
	 output = i match {
     		case 10 => "ten"
     		case 20 => "twenty"
     		case _ if (i % 2 == 0) => "even number"
     		case _  => "default output" 
  	}
  

   Scala Class Hierarchy
   ---------------------

	Any => AnyVal => Int, Long, Float, Double, Boolean, Unit, Byte, Char
            => AnyRef => String, <all other classes>


   Range
   -----
	Is a collection which store the starting value, end value and a step.

	Range(1, 10)       => 1,2,3,4,5,6,7,8,9
	Range(1, 10, 2)    => 1,3,5,7,9
	Range(100, 0, -20) => 100,80,60,40,20
	Range(1, 10, -2)   => Empty 

	1 to 10 by 2       => 1, 3, 5, 7, 9
	100 to 0 by -20	   => 100,80,60,40,20,0   ('to' returns Range.Inclusive)
        100 until 0 by -20 => 100,80,60,40,20,0 
	     
   Loops
   -----
   1. while

	var i = 0
   
   	while( i < 10 ) {
     	   println(s"i = $i")     
     	  i = i + 1     
   	}

   2. do while

	 do {
           println(s"i = $i")     
          i = i + 1     
        } while( i < 10 )

   3. foreach

	<collection>.foreach( fn )

	function "fn" is executes on all the objects of the collection.

	def f1(i: Int) = { println(i) }    
    	List(1,2,3,4,5,6,7,8,9).foreach( f1 )

   4. for 

	for( <generator(s)> ) {
        }

	for( x <- 1 to 10 by 2 ){
      		println(x)
    	}

	for( x <- 1 to 10 by 2; y <- 1 to 20 by 4){
      	    println(x, y)
        }

        for( x <- 1 to 10 by 2 if (x != 5); y <- 1 to 20 by 4 if (y > x)){
           println(x, y)
        }

        for comprehension: Collects the data looped by for loop using "yield" function and returns a collection.  

		val v1 = for( x <- 1 to 10 by 2 if (x != 5); y <- 1 to 20 by 4 if (y > x)) yield( (x, y) )
		println( v1 )

		Output:
		Vector((1,5), (1,9), (1,13), (1,17), (3,5), (3,9), (3,13), (3,17), (7,9), (7,13), (7,17), (9,13), (9,17))



   Interpolators
   --------------
      => Interpolator evaluates a string.

	1. 's' interpolator
		s"x = $x, x+1 = ${x+1}, y = $y"

        2. 'f' interpolator  => s interpolator + fromatting chars
		f"x = $x%1.1f, x+1 = ${x+1}, y = $y"

        3. 'raw' interpolator => will escape the escape chars

		val filePath = raw"E:\Spark\new\wordcount.txt"

  Exception Handling
  ------------------
	try {
	    // write you that could throw an exception. 
	}
	catch {		
	   case e1: FileNotFoundException => { .... }
           case e2: ArrayIndexOutOfBoundsException => { .... }
	   case _:Exception => { .... }
	}
	finally {
	    // write code that is always executed.
	}
       

   Getting started with Scala
   --------------------------

     1. Working in your vLab
	   => Follow the instructions on the attached document that you receive by email.
           => You log into a Ubuntu/CentOS VM
	   => You can launch Scala Shell & can start Scala IDE

      2. Installing Scala IDE on your personal machine. 

	  2.1 Scala IDE for eclipse

	  	=> Make sure you have Java 8 (jdk 1.8 or up) installed.

	  	=> Download and extract Scala IDE for eclispe from the following url link:
			http://scala-ide.org/download/sdk.html
	     	=> Nivagate inside the extracted folder (such as scala-SDK-4.7.0-vfinal-2.12-win32.win32.x86_64)
			and click in 'scala ide' icon to launch the IDE.

	  2.2 IntelliJ

		Instruction on how to install and setup IntelliJ for Scala are described here:
		https://docs.scala-lang.org/getting-started/index.html (near to 'Open hello-world project')

      3. Signup to Databricks community account
		=> https://www.databricks.com/try-databricks

      4. Online Scala Compilers
		https://scastie.scala-lang.org/



   Tuple
   ------
     => Tuple is an object that can hold multiple objects of different type
	-> A tuple with two objects is called a Pair

	 val t1 = (10, 10.5, "Hello")
   	 -> t1: (Int, Double, String) = (10,10.5,Hello)

	print( t1._1, t1._2, t1._3 )


   Methods
   -------
    -> Callable/reusable code block


	def add(a: Int, b: Int, c: Int) : Int = {
		a + b + c
	}

        -> Methods can be called by position
		val s = add(10, 20, 30)

        -> Methods can be called using named arguments
		val s = add(b=10, c=20, a=30)
  
	-> Method arguments can have default values.	
	
		def add(a: Int, b: Int = 0, c: Int = 0) : Int = {
			a + b + c
		}
  		val s = add(10, 20, 30)
		val s = add(10, 20)
		val s = add(10)

	-> Methods can have variable-lenght arguments
		-> Only one variable-lengh arg. is allowed and it should be the last argument.
		-> If can ot have default values when using variable-lengh arguments

		def add(a: Int, b: Int*) : Int = {
       			var s = a
       			for (i <- b) s += i
       			s
    		}   
    
    		val s = add(10, 20, 30, 40, 50)

       -> Methods can be called recursivly

		def factorial(n : Int) : Int = {
       		    if (n == 1) 1
       		    else n * factorial(n-1)
    		}

       -> Nested-methods
	
		def factorial2(i: Int): Int = {
      		   def fact(i: Int, accumulator: Int): Int = {
         		if (i <= 1)
            			accumulator
         		else
            			fact(i - 1, i * accumulator)
      		   }
      		   fact(i, 1)
   		}

       -> Methods can have multiple parameter lists

		def add(a: Int, b: Int)(c: List[Int]) = {
        		a + b + c.sum
     		}
     
     		val s = add(10, 20)( List(10,20,30) )

   Procedure
   ---------
	=> A procedure always returns Unit. 

		def box(name: String) {
      		   val line = "-" * name.length + "----"       
      		   println( line + "\n| " + name.toUpperCase + " |\n" + line )
                }

   Functions
   ---------
	=> In FP languages, function is treated as a literal (just like 10, "Hello", true)
	=> A function, by nature, is anonymous

	=> A function can be assigned to a variable
               val f1 = (a: Int, b: Int) => a + b
	
	=> A function can be passed as a parameter to another method/function

		def m1(a: Int, b: Int, f: (Int, Int) => Int ) = { a + b + f(a, b) }     
   		val x = m1(10, 20, (a, b) => a + b )

	=> A function/method can return a function as the final value.

		def compute(op: String) : (Int, Int) => Int = {
         		op match {
           			case "+" => (a: Int, b: Int) => a + b
           			case "-" => (a: Int, b: Int) => a - b
           			case "*" => (a: Int, b: Int) => a * b
           			case "/" => (a: Int, b: Int) => a / b
           			case _ => (a: Int, b: Int) => a % b
         		}
      		}
      
      		val f1 = compute("blah")     
            
      		println( f1, f1(100, 15) )

	
	Function Literal				Function Type
	--------------------------------------------------------------------
	(n: String) => n.toUpperCase			String => String
	(i: Int, j: Int) => i + j			(Int, Int) => Int
	(s: String, i: Int) => s * i			(String, Int) => String
	(S: String) => print(s)				String => Unit
	() => "Windows 10"				() => String
	(p: (Int, Int)) => p._1 + p._2			((Int, Int)) => Int
	(l: List[Int]) => l.length			List[Int] => Int



   Higher Order Functions
   ----------------------
       => Are functions/methods that take a function as an argument (or return function as a return value)
       => Operated on some collections
               <collection>.higher-order-fn( fn ) => <modifed-collection>
		
	  
    1. map	  	P: U => V
			map transforms the input elements by applying the function.
			input: N objects, output: N objects

		List(1,2,1,6,3,7,8,5,7,6,9,0,7,4).map( x => (x, x*x) )

    2. filter		P: U => Boolean
			Only those elements from the input collection for which the function returns true
			will be there in the output collection.
			input: N objects, output: <= N objects

		l1.filter(x => x.length > 51)

    3. flatMap		P: U => GenTraversableOnce[V]
			flatMap flattens the collection objects returned by the function.
			input: N objects, output: >= N objects

    		val words = l1.flatMap(s => s.split(" "))

     4. reduce  (reduceLeft/reduce & reduceRight)
			P: (U, U) => U
			Will reduce the entire input collection to one object of the same type
			by iterativly applying the function. 

	 	l1 => List(2,1,3,2,4,5,7,4)   => 4
		l1.reduceRight( (a, b) => a - b )

     5. sortWith	P: Binary sorting function

		t1.sortWith( (a, b) => a._2 > b._2 )
		words.sortWith( (a, b) => a(a.length - 1) < b(b.length - 1))

     6. groupBy		P: U => V
			Objects of the input collection are grouped based on the function output
			Returns scala.collection.immutable.Map[V,List[U]] where:
				each unique value of the fn output is the key (K)
				all objects of the collection that produvced the key forms the value (V)

			List[U].groupBy( U => V ) => List[ (V, Iterable[U]) ]

			words.groupBy( x => x ).toList.map(t => (t._1, t._2.length))

     7. foldLeft & foldRight	=> reduces the entire collection to a type that is different that the type of objects

			Two parameters as param-list
			
			1. zero-value : starting value of the type of output you want as output
			2. Sequence Function : is a function that iterativly merges all the objects of the 
			   collection with the zero-value. 

			List[U].foldLeft( (V, U) => V )  => V
	
				
		l1.foldRight( (0,0) )( (v, z) => (z._1 + v, z._2 + 1) )   

   Wordcount Program
   ------------------
	val filePath = "E:\\Spark\\wordcount.txt"
     
     	val output = Source
                   .fromFile(filePath)
                   .getLines()
                   .toList
                   .flatMap(x => x.split(" "))
                   .groupBy( x => x )
                   .toList
                   .map( p => (p._1, p._2.length) ) 
                   .sortWith( (a, b) => a._2 > b._2 )
     
     	println( output )


  Collections
  -----------
    
   1. Array & ArrayBuffer => Both are mutable collections
	Array 		-> fixed length collection
	ArrayBuffer 	-> Variabl length collection

   2. Seq => An ordered collection
	
	2.1 IndexedSeq -> Optimal for random access of data
		=> Vector
		=> Range

	2.2 LinearSeq  -> Optimal for iteration of data
		=> List
		=> Queue
		=> Stream

   3. Map => A collection of (k, v) pairs

	val m1 = Map( 1 -> 10, 2 -> 20, 3 -> 30)
	val m1 = Map( (1,10), (2,20), (3,30) )

	m1(<key>)  
	m1.get(<key>)  -> Returns an Option object
	m1.getOrElse(<key>, <default-value>)

   4. Set => Is an unordered collection of unique elements
	

  Option
  -------
    => represents an object that may be there or may not be there. 
    
       Option[U] => Some[U]   if there is a value to return
		    None      if there is no value to return. 

     
 ==========================================
   Spark - Basics & Architecture
 ==========================================     
   
   Spark is a framework written in Scala

   Spark is a unified in-memory distributed computing framework for big-data analytics 
	=> Spark is 100x faster than MapReduce if you use 100% in-memory computations.
	=> Spark is 6 to 7 times fater than MapReduce even if you use disk-based computations.
   
   Spark is a polyglot
	=> Spark supports multiple programming language
	=> Scala, Java, Python, R (and SQL)

   Spark can run on multiple cluster managers
	=> local, Spark Standalone, YARN, Mesos, Kubernetes


   Unified Framework
   -----------------
	Spark provides a set of consistent APIs for processing different analytical workloads
	using the same execution engine.

         => Batch analytics of unstructured data	: Spark Core API (low-level)
	 => Batch analytics of structured data		: Spark SQL
	 => Stream analytics (real-time)		: Spark Streaming & Structured Streaming
	 => Predictive analytics (using ML)		: Spark MLlib
	 => Graph parallel computations			: Spark GraphX


   Getting started with Spark
   --------------------------	
	Popular Options : 
	
	=> Scale-IDE for Eclipse + Maven
		=> Add required dependencies from Central maven repo
		=> The required packges are download and added to your project.
	=> IntelliJ + SBT

        Central maven repository: https://mvnrepository.com/ (search for Spark)

	Downloading "winutils.exe" to your local directory
		=> Create the following folder: C:\hadoop\bin
		=> Download an appropriate version of winutils.exe from the following url;
			https://github.com/steveloughran/winutils

   Spark Architecture
   ------------------

    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.


   ===========================================
    Spark Core API (Low level API)
   ===========================================

    => Spark Core is used to analyse unstructured data and is a low-level API
    => Uses RDD as fundamental data abstraction

    RDD (Resilient Distributed Dataset)
    -----------------------------------

	-> RDD is a collection of distributed in-memory partitions
             -> A partition is a collection of objects of some type.

	-> RDDs are immutable

        -> RDDs are lazily evaluated
		=> Transformations does not cause execution
		=> Execution will be triggered only by action command

	-> RDDs are resilient 
		-> RDDs are resilient to missing in-memory partitions


   Creating RDDs
   -------------
	
    Three ways:

	1. Create an RDD from some external data source (such as a text-file)

		val rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)	

	2. Create an RDD from programmatic data

		val rdd1 = sc.parallelize( 1 to 100, 3 )

	3. By applying transformations on existing RDDs
		
		val rdd2 = rdd1.flatMap( x => x.split(" "))

   RDD Operations
   --------------

	To things:

	1. Transformations	
		-> Returns an RDD
		-> Does not cause execution
		-> Transformation only create RDD Lineage DAGs

        2. Actions
		-> Trigger execution of the RDD
		-> Produces output
		-> Converts thr logical Plan into a physical execution plan.


   RDD Lineage DAG
   ---------------
   -> RDD Lineage is alogical-plan, maintained by the driver
   -> RDD Lineage maintains the hierarchy of all parent RDDs that caused the creation of this RDD.
  
    	val rddFile = sc.textFile(filePath, 4)
	
(4) E:\Spark\wordcount.txt MapPartitionsRDD[1] at textFile at <console>:26 []
 |  E:\Spark\wordcount.txt HadoopRDD[0] at textFile at <console>:26 []

	val rddWords = rddFile.flatMap( x => x.split(" ") )
	
(4) MapPartitionsRDD[2] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[1] at textFile at <console>:26 []
 |  E:\Spark\wordcount.txt HadoopRDD[0] at textFile at <console>:26 []

	val rddPairs = rddWords.map( x => (x, 1) )

(4) MapPartitionsRDD[3] at map at <console>:25 []
 |  MapPartitionsRDD[2] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[1] at textFile at <console>:26 []
 |  E:\Spark\wordcount.txt HadoopRDD[0] at textFile at <console>:26 []

	val rddWc = rddPairs.reduceByKey( (a, b) => a + b )

(4) ShuffledRDD[4] at reduceByKey at <console>:25 []
 +-(4) MapPartitionsRDD[3] at map at <console>:25 []
    |  MapPartitionsRDD[2] at flatMap at <console>:25 []
    |  E:\Spark\wordcount.txt MapPartitionsRDD[1] at textFile at <console>:26 []
    |  E:\Spark\wordcount.txt HadoopRDD[0] at textFile at <console>:26 []


  RDD Transformations
  -------------------

   1. map		P: U => V
			Transforms each input object to the corresponding output object by applying the
			function. 
			Object to object transformation
			input RDD: N objects, output RDD: N objects

		rddFile.map(x => x.split(" "))

   2. filter		P: U => Boolean
			Filters the input objects based on the function.
			input RDD: N objects, output RDD: < N objects

		rddWords.filter(w => w(0) == 's').collect

   3. glom	


















