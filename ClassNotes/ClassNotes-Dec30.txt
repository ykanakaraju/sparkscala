
  Agenda
  -------

  Scala - refresher
     	Basics	
	Functional programming concepts
  Spark 
	Basics & Architecture
	Core API -> RDD (basics)
	Spark SQL (DataFrames)
	Spark Streaming
  Kafka
	Architecture
	Topic Operations
	Producer API
	Consumer API
  NiFi
	Basics Concepts
	3 or 4 different integrations.


  Materials
  ---------
	=> PDF Presentations
	=> Code Modules
	=> Class Notes
	=> Git Hub: https://github.com/ykanakaraju/sparkscala


  Using your lab
  ---------------

	-> Connect to the Windows server using the credentials got in the email
	-> Check out the word doc for other credentials (on the desktop)
	-> Click on the "Oracle VM Vitualbox" icon (desktop)
	-> Select the Vm ans click on the 'Start' button
	-> Enter your userid and password. 

	Spark Shell
        ----------
	-> Open the Terminal window
	-> Type "spark-shell"

	Launching Scala IDE
	-------------------
	-> Open the "ScalaIDE" folder on the desktop
	-> Open the "eclipse" folder
        -> Click on the 'eclipse' application icon (blue diamond)
	
 
  Scala
  -----
    
    Scala stands for SCAlable LAnguage
	
    Scala is a multi-paradigm programming language.
  
	-> Scala is a PURE OOP
		-> Scala has no primitives
		-> Scala has no operators

	-> Scala is a Functional programming language

    Scala immutables and mutables:

	 	val i : Int = 10  => immutable
		var i : Int = 10  => mutable

    Scala Type inference:

	  -> Scala can automatically infer the type based on the value assigned to it.

    Scala is statically typed language
	-> The type of every object is known at compile time.
	-> Once declared, the type of an object can not be changed.

    Blocks

	-> Set of expressions enclosed in { .. } is a block
  	-> Every block returns a value

    Unit
	Is a class that represents "no value"
	Printed as "()"

	
   Getting started with Scala
   ---------------------------
	1. Using you Lab

	2. Installing Scala IDE on your local environment
	      URL: http://scala-ide.org/download/sdk.html

	      => Ensure you are running JDK 1.8 or up (Java 8)
	      => Download the zip file from the above URL
              => Unzip/extract it, navigate to eclispe folder.
      
        3. Using Databricks community edition
	
		Sign-up: https://databricks.com/try-databricks
	  	Login: https://community.cloud.databricks.com/login.html

  String Interpolations 
  ----------------------

	s => Allows you to invoke variables using $ sybmol
		ex: val str = f"i = $i\nj = $j\nk = $k"

	f => s interpolator + can use formatting symbols
		ex: val str = f"i = $i%.2f\nj = $j%.2f\nk = $k"

        raw => s interpolator + escapes the escape chars
		val str = raw"i = $i%.2f\nj = $j%.2f\nk = $k"


  Control-Flow statements
  -----------------------

    if..else if..else
    ------------------

	-> if..else can return a value.

	val z = if (x > y) x - y else if (x < y) y - x else 0

    match..case
    ------------

    	val z = x match {
          case 10 => { "ten" }
          case 20 => { "twenty" }
          case a if (x % 2 == 0) => { s"even number ($a)" }
          case a if (x % 2 != 0) => { s"odd number ($a)" }
          case _ => { "none of the above" }
        }
 
  

  Scala Class Hierarchy
  ---------------------
	
	Any   	=> AnyVal   => Int, Long, Double, Float, Bool, Unit, Char, Byte ...
		=> AnyRef   => String, List, Seq, Map, .....

  Range
  -----  	
	Range(1, 10)        => 1,2,3,4,5,6,7,8,9
	Range(1, 10, 3)     => 1,4,7
	Range(100, 10, -20) => 100, 80, 60, 40, 20
	1 to 10             => 1,2,3,4,5,6,7,8,9,10   (Range.Inclusive)
	1 until 10          => 1,2,3,4,5,6,7,8,9
	0 to 10 by 2	    => 0,2,4,6,8,10
	0 until	10 by 2     => 0,2,4,6,8
	10 until 1          => Empty Range  (no elements)

  Loops
  -----

    => while loop

		while(i < x) {
      		    println(i)
      		    i += 1
    		}

    => do..while

		do {
      		   println(i)
      		   i += 1
    	        } while(i < x)


    => foreach

	   <Iterable>.foreach( function )

		(1 to 10).foreach( x => println(s"x=$x") )    
    		"SCALA LANGUAGE".foreach(println)

    => for

	for( <one or more generators> ) {
               <some code>
        }

	   for( x <- 0 until 10 by 2) {
         	println(s"x = $x")
     	   }

	   for( x <- 0 until 10 by 2; y <- "scala") {
         	println(s"x = $x, y = $y")
     	   }

	   for( x <- 0 until 10 by 2 if(x != 6); y <- 0 until 10 if( x > y )) {
         	println(s"x = $x, y = $y")
     	   }

	   for comprehension
	   -----------------
		 for producing a collection (Vector object) from a for loop:

		 val v = for( x <- 0 until 10 by 2 if(x != 6); y <- 0 until 10 if( x > y )) yield( (x, y) )
		 println( v )

		 Result: (2,0), (2,1), (4,0), (4,1), (4,2), (4,3), (8,0), (8,1), (8,2), (8,3), (8,4), (8,5), (8,6), (8,7)

    Scala Imports
    -------------

	import Source.io.File
	import Source.io._
	import java.io.{FileReader, FileNotFoundException, IOException}


   Exception handling
   -------------------
	try {
		some code that might through an exception
         }
         catch {
	     case e: ArrayIndexOutofBoundsException => {
	     }
             case f: FileNotFoundException => {
             }
	     case _ => {
             }
         }
         finally {
	     // code that executes after try/catch blocks
         }

      Example
      --------------

      try {
         println("Reading from a file ...")   
         val f = new FileReader("file1456.txt")            
      } 
      catch {
         case ex: FileNotFoundException => {
            println("Missing file exception")
            println( ex.getMessage() )
         }         
         case _: IOException => {
            println("IO Exception")
         }
         case _: Exception => {
            println("generic exception")
         }
      } 
      finally {
         println("Exiting finally...") 
      }  



   Methods
   -------
	-> Reusable code block that return some value
	-> Method will have a name and optionally some parameters/arguments

	-> NOTE: note that we are using an "=" symbol

	def sum(a: Int, b: Int) : Int = {
        	a + b
     	}
	
	-> methods can take 0 or more arguments                
	-> Methods can be called by positional parameters
	-> Methods can be called by named parameters

	val x = sum(10, 20)		// positional params
	val x = sum(b=10, a=20)   	// named params


	-> Method arguments can have default values	

	def sum(a: Int, b: Int, c: Int = 0) = {       
        	println( s"a = $a, b = $b, c = $c" )    
        	a + b + c
     	}

        -> Methods can have variable length arguments
	  
		 def sum(a: Int, i: Int*) = {       
       			var s = 0 
       
       			for (x <- i){
         			println(s"x = $x")
         			s += x
       			}
       
       			a + s
     		}
    
     		val x = sum(10, 20, 30, 40, 50)
	
		-> In this example, i represents [20, 30, 40, 50]

	-> methods can call themselves within the code. (recursive methods)

		def factorial(n: Int) : Int = {
        		if ( n == 1 ) n
        		else n * factorial(n - 1)
      		}

         -> methods can take multiple parameter lists

		def sum(a: Int, b: Int)(c: Int)(d: Int) = (a + b)*c*d    
    		val x = sum(10, 20)(5)(2)


   Procedures
   ----------
	-> Are like methods but does not return any value.
	-> A procedure always return "Unit" irrespective of the return type of the block
	-> Procedure does not have the = symbol in the defintion

	def box(name: String) {       
        	val line = "-" * name.length() + "----"
        	println( line + "\n| " + name.toUpperCase  + " |\n" + line)        
     	}
     
     	box("scala is a programming language")

  Functions
  ---------
       -> In Functional programming, a 'function' is like a literal.
       -> A function has a value and type of its own.
       -> Functions are anonymous by default
       -> A function can be assigned to a variable (named functions)
       -> A function can be passed as a parameter to a method/function
       -> A block can return a function as final value
       -> A method can return a function as return value

	def m1(a: Int, b: Int, c: String, f: (String, Int) => String ) = {
            f(c, b) + ":" + a
        }
  
        val n = m1(10, 15, "*", (s, n) => s * n)
     
        println( n )

   => A method/function can return a function as a return value.

	def compute(op: String) = {
        op match {
          case "+" => (x: Int, y: Int) => x + y
          case "-" => (x: Int, y: Int) => x - y
          case "*" => (x: Int, y: Int) => x * y
          case "/" => (x: Int, y: Int) => x / y
          case _ => (x: Int, y: Int) => x % y
        }
      }
      
      val f1 = compute("+")
      
      println( compute("+")(10, 20) )

        Function Literal			Type
        ---------------------------		-------------------
	(a : Int, b: Int) => a + b	        (Int, Int) => Int
	(n: Int) => n * 10			Int => Int
	(s: String) => s * 10			String => String
	(s: String, n: Int) => s * n		(String, Int) => String
	(l: List[Int]) => sum(l)		List[Int] => Int
	(t: (Int, Int)) => t._1 + t._2		(Int, Int) => Int
	() => "Windows 10"			() => String	
	(s: String) => println(s)		String => Unit

	
       //method returning a function
	def compute(op: String) = {
          (i: Int, j: Int) => {
             op match {
               case "+" => i + j
               case "-" => i - j
               case "*" => i * j
               case _ => i % j
             }
          }
        }      

       // method taking function as a parameter
       def calculate(a: Int, b: Int, f: (Int, Int) => Int) = {
         f(a, b)
       }
      
       val sum = compute("+")
       val diff = compute("-")
       val prod = compute("*")
       val mod = compute("blah")
      
      
       println ( calculate(10, 20, compute("+")) )
       println ( calculate(10, 20, sum) )

       println ( calculate(10, 20, (a: Int, b: Int) => a + b) )
       println ( calculate(10, 20, (a, b) => a + b) )
       println ( calculate(10, 20, _+_) )

	
   Higher Order Functions
   ----------------------
   1. map		P: U => V
			Performs object to object transformation
			input: N objects, output: N objects 

		s.map(x => x.split(" ").length)

   2. filter		P: U => Boolean
			The output collection will have only those elemnts for which the function
			returns true.
			input: N objects, output: < N objects 

		s.filter( a => a.split(" ").length > 8 )

   3. flatMap		P: U => GenTraversableOnce[V]
			flattens the iterables produced by the function
			input: N objects, output: > N objects 

		val words = lines.flatMap(x => x.split(" "))

   4. reduce (reduceLeft / reduceRight)
			P: (U, U) => U			
			The function is iterativly applied to reduce the entire collect to one final value
			of the same type.

   5. sortWith		P : Binary sorting function
			Sorts the elements of the elements based on the comparision statement in the binary sorting
                        function.

			words.sortWith( (x, y) => x.length < y.length )
			words.sortWith( (x, y) => x(x.length - 1) < y(y.length - 1))


   6. groupBy		P: U => V
			Returns a Map where the 'keys' are unique values of the function output and 'values'
			are grouped objects that produced the key

		words.groupBy( x => x ).toList.map(x => (x._1, x._2.length ) )

   7. mapValues		P: U -> V
			Operates in Map object and transforms only the value part of thr Map by applying the function.

		val m1 = Map( 1 -> 10, 2 -> 20, 3 -> 30, 4 -> 40)

		m1.mapValues(x => x + 1)
		res93: scala.collection.immutable.Map[Int,Int] = Map(1 -> 11, 2 -> 21, 3 -> 31, 4 -> 41)

		words.groupBy(x => x).mapValues(x => x.length).toList

   8. foldLeft & foldRight	
                 => reduces the entire collection to a type that is different that the type of objects

			Two parameters as param-list
			
			1. zero-value : starting value of the type of output you want as output
			2. Sequence Function : is a function that iterativly merges all the objects of the 
			   collection with the zero-value. 

			List[U].foldLeft( (V, U) => V )  => V					
			l1.foldRight( (0,0) )( (v, z) => (z._1 + v, z._2 + 1) ) 


   Use-Case: Writing a simple wordcount program
   -------------------------------------------
      val wordcounts = Source.fromFile("file1.txt")
                        .getLines
                        .toList
                        .flatMap(x => x.split(" "))
                        .groupBy(x => x)
                        .mapValues(l => l.length)
                        .toList
                        .sortWith( (x, y) => x._2 > y._2 )

   Reading from a file
   -------------------
      import scala.io.Source

      val lines = Source.fromFile("E:\\Spark\\wordcount.txt")


   Collections
   -----------
   1. Array  

	-> Mutable collection
	-> Fixed length

   2. ArrayBuffer

	-> Mutable collection
	-> Variable length

   3. Seq  => immutable
	      Ordered collection of same type
	
	  Indexed Sequences
		-> Optimized for random access of elements
		Vector, Range

	  Linear Sequences
		-> Optimized for iterating all the elements
		List, Queue, Stream


   4. Set    => unordered collection of unique elements


   5. Map    => Collection of (Key, Value) pairs
	
	val m1 = Map( 1 -> 10, 2 -> 20, 3 -> 30 )

	val m1 = Map( 1 -> "Sun", 2 -> "Mon", 3 -> "Tue", 4 -> "Wed" )
     
        println( m1.get(1), m1.get(5) )
     
        val v = m1.getOrElse(2, "No Value")
       
        println( v )


   Tuple
   -----
     => Is an object that holds multiple values of different types.


	val t1 = (10, 10.5, 10 > 20, 5)
	t1: (Int, Double, Boolean, Int) = (10,10.5,false,5)


	val t2 = (10, 20, List(1,2,3), Array(3,4,5))
	t2: (Int, Int, List[Int], Array[Int]) = (10,20,List(1, 2, 3),Array(3, 4, 5))

    => A tuple with only two elements is called a "Pair"

	
   Option
   ------
      Represents an entity that may or may not have a value. 
      Option class returns two objects
      val status: Option[U]  => Some[U],  None 

	
    Object Oriented Programming
    ---------------------------

      	-> class
		-> abstract class
		-> case class

	-> object   (singleton class)
		-> companion object
		-> case object

	-> trait


	class
	=====

	=> A single file can have any number of classes
	=> There is no relation between the file name and class name.

	-> classes, methods and variable are public by default.

	-> Scala implicitly provides getter and setter methods for class variables.
	    -> If the variable is "val", then scala provides only getter methods 
	    -> If the variable is "var", then scala provides both getter and setter methods 


        constructors
        ------------

	=> Two types of constructors:

		1. Primary Constructor (PC)
			-> Tied up with the class name itself (not user defined method)
			-> All the executable code present in the class is the code that
			   gets exeuted when you invoke PC.

		2. Auxiliary Constructor (AC)
			-> User defined method with the name "this"
			-> Every AC must call the PC or a previously defined AC as the first
			   statement.
			-> We may define as many ACs as we need.



	objects
   	=======
	-> Objects are singleton entities and only one instance of the object is created.
           The constructor od the object is executed only once when the object is first 
	   created.

	-> Created for utility methods


 =======================================
    Spark
 =======================================

  => Spark is written in Scala

  => Spark is a framework to perform big-data analytics
  => Spark is an in-memory distributed computing framework.
  => Spark is a unified framework
	
	Batch Analytics of Unstructured data  	=> Spark Core API (RDDs)
	Batch Analytics of Structured data	=> Spark SQL (DataFrames)
	Streaming Analytics (real-time)		=> Spark Streaming & Structured Streaming
	Predictive Analytics (ML)		=> Spark MLlib
	Graph parallel computations		=> Spark GraphX

   => Spark can run on the following Cluster managers
	 -> Spark Standalone
	 -> YARN (Hadoop)
	 -> Mesos
         -> Kubernetes

   => Spark is a polyglot. Spark supports multiple programming languages
	 -> Scala
	 -> Python (PySpark)
         -> Java
	 -> R


  Spark Architecture & Building Blocks
  ------------------------------------

    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.


   RDD ( Resilient Distributed Dataset ) 
   --------------------------------------

	=> Is a collection of distributed in-memory partitions 
		-> Partition is a collection of objects of any type. 
          
        => RDDs are immutable

        => RDDs are lazily evaluated.
		-> Transformations does not cause execution. 
		-> Actions trigger the execution.
  

  Creating RDDs
  -------------

      Three ways:

      1. Create an RDD from some external data source. 

		val filePath = "E:\\Spark\\wordcount.txt"
		val rddFile = sc.textFile( filePath, 4 )

      2. Create an RDD from programmatic data (such as a collection)

		val l1 = List(3,2,1,4,2,3,2,5,6,7,8,5,6,7,8,8,9,5,4,3,1,2,3,2,4,5)
		val rdd1 = sc.parallelize(l1, 3)

      3. By applying transformations on existings RDDs
	
		val rddWords = rddFile.flatMap( x => x.split(" ") )

  RDD Operations
  --------------
     Two things:

	1. Transformations
	    -> Transformations (or Data loading commands) does cause execution of the RDD
	    -> Transformations create the RDD's lineage DAG (logical plan) maintained by driver.	

	2. Actions
	    -> trigger execution of the RDDs
	    -> Converts the logical plan to physical plan (stages & tasks)

  RDD Lineage DAG
  ---------------
     rdd1.toDebugString => prints the lineage of rdd1.
    
     RDD Lineage is a logical plan that tracks the tasks to be performed to compute the RDD partitions
     It has the heirarchy of dependent RDD all the way upto the very first RDD.     


	val rddFile = sc.textFile( filePath, 4 )

(4) E:\Spark\wordcount.txt MapPartitionsRDD[10] at textFile at <console>:26 []
 |  E:\Spark\wordcount.txt HadoopRDD[9] at textFile at <console>:26 []

	val rddWords = rddFile.flatMap( x => x.split(" ") )

(4) MapPartitionsRDD[11] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[10] at textFile at <console>:26 []
 |  E:\Spark\wordcount.txt HadoopRDD[9] at textFile at <console>:26 []


	val rddPairs = rddWords.map( x => (x, 1) )

(4) MapPartitionsRDD[12] at map at <console>:25 []
 |  MapPartitionsRDD[11] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[10] at textFile at <console>:26 []
 |  E:\Spark\wordcount.txt HadoopRDD[9] at textFile at <console>:26 []


	val rddWordCount = rddPairs.reduceByKey( (x, y) => x + y )

(4) ShuffledRDD[13] at reduceByKey at <console>:25 []
 +-(4) MapPartitionsRDD[12] at map at <console>:25 []
    |  MapPartitionsRDD[11] at flatMap at <console>:25 []
    |  E:\Spark\wordcount.txt MapPartitionsRDD[10] at textFile at <console>:26 []
    |  E:\Spark\wordcount.txt HadoopRDD[9] at textFile at <console>:26 []



  Types of Transformations
  -------------------------

	Two types:

	1. Narrow transformations

	2. Wide transformations


  RDD Persistence
  ---------------

      	val rdd1 = sc.textFile(<dataset>, 4)
	val rdd2 = rdd1.t2(...)
	val rdd3 = rdd1.t3(...)
	val rdd4 = rdd3.t4(...)
	val rdd5 = rdd3.t5(...)
	val rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_ONLY )  -> instruction to spark to save the partitions
	val rdd7 = rdd6.t7(...)

	rdd6.collect()

	DAG: rdd6 => rdd5.t6 => rdd3.t5 => rdd1.t3 => sc.textFile
	(textFile,  t3,  t5,  t6) ==> collect

        rdd7.collect()

	DAG: rdd7 => rdd6.t7
	(t7) ==> collect

	rdd6.unpersist()

	
	Storage Levels
        --------------

	Types of persistence:	
	   -> Serialized persistence
	   -> Deserialized persistence

	1. MEMORY_ONLY		-> default, Memory Deserialized 1x Replicated
	
	2. MEMORY_AND_DISK	-> Disk Memory Deserialized 1x Replicated

	3. DISK_ONLY		-> Disk Serialized 1x Replicated

   	4. MEMORY_ONLY_SER	-> Memory Serialized 1x Replicated

	5. MEMORY_AND_DISK_SER	-> Disk Memory Serialized 1x Replicated	

	6. MEMORY_ONLY_2	-> Memory Deserialized 2x Replicated

	7. MEMORY_AND_DISK_2	-> Disk Memory Deserialized 2x Replicated


	Commands
	---------
	rdd1.cache()
	rdd1.persist()
	rdd1.persist(StorageLevel.DISK_ONLY)

	rdd1.unpersist()

	
  RDD Transformations
  -------------------

  1. map			P: U => V
				Object to object transformation
				input: N objects, output: N objects

		val rdd2 = rddFile.map(x => x.split(" "))

  2. filter			P: U => Boolean
				Only those objects for which the function returns true will be in the output.
				input: N objects, output: <= N objects

		rdd2.filter(x => (x._1 + x._2) > 6).collect

  3. glom			P: None
				Return one Array object per partition

		rdd1			rdd2 = rdd1.glom()
	
		P0: 3,1,2,4,2,5 -> glom -> P0: Array(3,1,2,4,2,5)
		P1: 4,4,2,5,7,9 -> glom -> P1: Array(4,4,2,5,7,9)
		P2: 6,3,4,6,9,0 -> glom -> P2: Array(6,3,4,6,9,0)

		rdd1.count = 18 Int	   rdd2.count = 3 Array[Int]


   4. flatMap			P: U => TraversableOnce[V]
				Flattens the iterables produced by the function.
				input: N objects, output: >= N objects

		val rddWords = rddFile.flatMap(x => x.split(" "))

   5. mapPartitions		P: Iterator[U] => Iterator[V]
				Transforms an entire partition to the corresponding output partitions

		rdd1		rdd2 = rdd1.mapPartitions( p => List(p.sum).iterator )
	
		P0: 3,1,2,4,2,5 -> mapPartitions -> P0: 18
		P1: 4,4,2,5,7,9 -> mapPartitions -> P1: 37
		P2: 6,3,4,6,9,0 -> mapPartitions -> P2: 28

		rdd1.mapPartitions( p => List(p.sum).iterator ).collect

   6. mapPartitionsWithIndex	P: (Int, Iterator[U]) => Iterator[V]
   				Transforms an entire partition to the corresponding output partitions
				We get the partition-index also as an additional function parameter.

		rdd1.mapPartitionsWithIndex( (i,p) => List((i, p.sum)).iterator ).collect
		rdd1.mapPartitionsWithIndex( (i,p) => p.map( x => (i,x) ) ).filter(x => x._1 == 1).map(x => x._2).collect

   7. distinct			P: None, Optional: numPartitions
				Returns distinct objects of the RDD

		rddWords.distinct.glom.collect
		rddWords.distinct(1).glom.collect

   8. sortBy			P: U => V, Optional: ascending (true/false), numPartitions
				Sorts the elements based on the function output.

		rdd1.sortBy(x => x%3).glom.collect
		rdd1.sortBy(x => x%3, false).glom.collect
		rdd1.sortBy(x => x%3, false, 2).glom.collect

  
	




			










