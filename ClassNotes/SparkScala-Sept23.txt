
  Agenda (Spark using Scala)
  --------------------------
   -> Scala Refresher
   -> Spark - Basics & Architecture
   -> Spark Core API
	-> RDD - Transformations and Actions
	-> Spark Shared Variable
        -> Spark-Submit
   -> Spark SQL
	-> DataFrame Operations
	-> Integration with MySQL & Hive
	-> SQL Optimizations & Tuning
   -> Spark Streaming
	-> DStreams API (introduction)
	-> Structured Streaming

  Materials
  ---------
	=> PDF Presentations
	=> Code Modules
	=> Class Notes
	=> Github: https://github.com/ykanakaraju/sparkscala

  Scala
  -----
	
   -> Scala is Java based compiler language
        -> Scala compiles to Java byte code
	-> Interoperable with Java language
	
    -> Scala is multi-paradigm programming language	
	 -> Pure OOP language
         -> Functional programming language

    -> Pure OOP language
	 -> Scala does not have primitive and operator

   -> Scala Variables: immutables -> val
		       mutable    -> var

   -> Scala infers the type automatically based on the assigned value.

   -> Scala infix notation : 
		
	val j = i.+(10)  can be written in scala as:
	val j = i + 10

   -> Scala is a statically typed language
	-> The type of every object/variable is know at compile time. 
	-> Once assigned a type can not be changed. 

  Blocks
  ------

	-> One or more statements enclosed in { .. }
	    -> If there is only one statement/expression then you may omit the { .. }
	-> A block returns an ouput
	    -> The output is the value of the last statement that is executed in the block.
	-. Scala does not use "return" keyword

  Unit
  ----
	-> Is a class that represents 'no value'
        -> Printed as "()"  	

  Flow Control Statements
  -----------------------

   if..else
   ----------

	if (<boolean>) { ... }
	else if (<boolean>) { ... }
	else { ...}

	=> If statement in Scala, retruns a value

   match..case
   -----------
	
	 output = i match {
     		case 10 => "ten"
     		case 20 => "twenty"
     		case _ if (i % 2 == 0) => "even number"
     		case _  => "default output" 
  	}
  

   Scala Class Hierarchy
   ---------------------

	Any => AnyVal => Int, Long, Float, Double, Boolean, Unit, Byte, Char
            => AnyRef => String, <all other classes>


   Range
   -----
	Is a collection which store the starting value, end value and a step.

	Range(1, 10)       => 1,2,3,4,5,6,7,8,9
	Range(1, 10, 2)    => 1,3,5,7,9
	Range(100, 0, -20) => 100,80,60,40,20
	Range(1, 10, -2)   => Empty 

	1 to 10 by 2       => 1, 3, 5, 7, 9
	100 to 0 by -20	   => 100,80,60,40,20,0   ('to' returns Range.Inclusive)
        100 until 0 by -20 => 100,80,60,40,20,0 
	     
   Loops
   -----
   1. while

	var i = 0
   
   	while( i < 10 ) {
     	   println(s"i = $i")     
     	  i = i + 1     
   	}

   2. do while

	 do {
           println(s"i = $i")     
          i = i + 1     
        } while( i < 10 )

   3. foreach

	<collection>.foreach( fn )

	function "fn" is executes on all the objects of the collection.

	def f1(i: Int) = { println(i) }    
    	List(1,2,3,4,5,6,7,8,9).foreach( f1 )

   4. for 

	for( <generator(s)> ) {
        }

	for( x <- 1 to 10 by 2 ){
      		println(x)
    	}

	for( x <- 1 to 10 by 2; y <- 1 to 20 by 4){
      	    println(x, y)
        }

        for( x <- 1 to 10 by 2 if (x != 5); y <- 1 to 20 by 4 if (y > x)){
           println(x, y)
        }

        for comprehension: Collects the data looped by for loop using "yield" function and returns a collection.  

		val v1 = for( x <- 1 to 10 by 2 if (x != 5); y <- 1 to 20 by 4 if (y > x)) yield( (x, y) )
		println( v1 )

		Output:
		Vector((1,5), (1,9), (1,13), (1,17), (3,5), (3,9), (3,13), (3,17), (7,9), (7,13), (7,17), (9,13), (9,17))



   Interpolators
   --------------
      => Interpolator evaluates a string.

	1. 's' interpolator
		s"x = $x, x+1 = ${x+1}, y = $y"

        2. 'f' interpolator  => s interpolator + fromatting chars
		f"x = $x%1.1f, x+1 = ${x+1}, y = $y"

        3. 'raw' interpolator => will escape the escape chars

		val filePath = raw"E:\Spark\new\wordcount.txt"

  Exception Handling
  ------------------
	try {
	    // write you that could throw an exception. 
	}
	catch {		
	   case e1: FileNotFoundException => { .... }
           case e2: ArrayIndexOutOfBoundsException => { .... }
	   case _:Exception => { .... }
	}
	finally {
	    // write code that is always executed.
	}
       

   Getting started with Scala
   --------------------------

     1. Working in your vLab
	   => Follow the instructions on the attached document that you receive by email.
           => You log into a Ubuntu/CentOS VM
	   => You can launch Scala Shell & can start Scala IDE

      2. Installing Scala IDE on your personal machine. 

	  2.1 Scala IDE for eclipse

	  	=> Make sure you have Java 8 (jdk 1.8 or up) installed.

	  	=> Download and extract Scala IDE for eclispe from the following url link:
			http://scala-ide.org/download/sdk.html
	     	=> Nivagate inside the extracted folder (such as scala-SDK-4.7.0-vfinal-2.12-win32.win32.x86_64)
			and click in 'scala ide' icon to launch the IDE.

	  2.2 IntelliJ

		Instruction on how to install and setup IntelliJ for Scala are described here:
		https://docs.scala-lang.org/getting-started/index.html (near to 'Open hello-world project')

      3. Signup to Databricks community account
		=> https://www.databricks.com/try-databricks

      4. Online Scala Compilers
		https://scastie.scala-lang.org/



   Tuple
   ------
     => Tuple is an object that can hold multiple objects of different type
	-> A tuple with two objects is called a Pair

	 val t1 = (10, 10.5, "Hello")
   	 -> t1: (Int, Double, String) = (10,10.5,Hello)

	print( t1._1, t1._2, t1._3 )


   Methods
   -------
    -> Callable/reusable code block


	def add(a: Int, b: Int, c: Int) : Int = {
		a + b + c
	}

        -> Methods can be called by position
		val s = add(10, 20, 30)

        -> Methods can be called using named arguments
		val s = add(b=10, c=20, a=30)
  
	-> Method arguments can have default values.	
	
		def add(a: Int, b: Int = 0, c: Int = 0) : Int = {
			a + b + c
		}
  		val s = add(10, 20, 30)
		val s = add(10, 20)
		val s = add(10)

	-> Methods can have variable-lenght arguments
		-> Only one variable-lengh arg. is allowed and it should be the last argument.
		-> If can ot have default values when using variable-lengh arguments

		def add(a: Int, b: Int*) : Int = {
       			var s = a
       			for (i <- b) s += i
       			s
    		}   
    
    		val s = add(10, 20, 30, 40, 50)

       -> Methods can be called recursivly

		def factorial(n : Int) : Int = {
       		    if (n == 1) 1
       		    else n * factorial(n-1)
    		}

       -> Nested-methods
	
		def factorial2(i: Int): Int = {
      		   def fact(i: Int, accumulator: Int): Int = {
         		if (i <= 1)
            			accumulator
         		else
            			fact(i - 1, i * accumulator)
      		   }
      		   fact(i, 1)
   		}

       -> Methods can have multiple parameter lists

		def add(a: Int, b: Int)(c: List[Int]) = {
        		a + b + c.sum
     		}
     
     		val s = add(10, 20)( List(10,20,30) )

   Procedure
   ---------
	=> A procedure always returns Unit. 

		def box(name: String) {
      		   val line = "-" * name.length + "----"       
      		   println( line + "\n| " + name.toUpperCase + " |\n" + line )
                }

   Functions
   ---------
	=> In FP languages, function is treated as a literal (just like 10, "Hello", true)
	=> A function, by nature, is anonymous

	=> A function can be assigned to a variable
               val f1 = (a: Int, b: Int) => a + b
	
	=> A function can be passed as a parameter to another method/function

		def m1(a: Int, b: Int, f: (Int, Int) => Int ) = { a + b + f(a, b) }     
   		val x = m1(10, 20, (a, b) => a + b )

	=> A function/method can return a function as the final value.

		def compute(op: String) : (Int, Int) => Int = {
         		op match {
           			case "+" => (a: Int, b: Int) => a + b
           			case "-" => (a: Int, b: Int) => a - b
           			case "*" => (a: Int, b: Int) => a * b
           			case "/" => (a: Int, b: Int) => a / b
           			case _ => (a: Int, b: Int) => a % b
         		}
      		}
      
      		val f1 = compute("blah")     
            
      		println( f1, f1(100, 15) )

	
	Function Literal				Function Type
	--------------------------------------------------------------------
	(n: String) => n.toUpperCase			String => String
	(i: Int, j: Int) => i + j			(Int, Int) => Int
	(s: String, i: Int) => s * i			(String, Int) => String
	(S: String) => print(s)				String => Unit
	() => "Windows 10"				() => String
	(p: (Int, Int)) => p._1 + p._2			((Int, Int)) => Int
	(l: List[Int]) => l.length			List[Int] => Int



   Higher Order Functions
   ----------------------
       => Are functions/methods that take a function as an argument (or return function as a return value)
       => Operated on some collections
               <collection>.higher-order-fn( fn ) => <modifed-collection>
		
	  
    1. map	  	P: U => V
			map transforms the input elements by applying the function.
			input: N objects, output: N objects

		List(1,2,1,6,3,7,8,5,7,6,9,0,7,4).map( x => (x, x*x) )

    2. filter		P: U => Boolean
			Only those elements from the input collection for which the function returns true
			will be there in the output collection.
			input: N objects, output: <= N objects

		l1.filter(x => x.length > 51)

    3. flatMap		P: U => GenTraversableOnce[V]
			flatMap flattens the collection objects returned by the function.
			input: N objects, output: >= N objects

    		val words = l1.flatMap(s => s.split(" "))

     4. reduce  (reduceLeft/reduce & reduceRight)
			P: (U, U) => U
			Will reduce the entire input collection to one object of the same type
			by iterativly applying the function. 

	 	l1 => List(2,1,3,2,4,5,7,4)   => 4
		l1.reduceRight( (a, b) => a - b )

     5. sortWith	P: Binary sorting function

		t1.sortWith( (a, b) => a._2 > b._2 )
		words.sortWith( (a, b) => a(a.length - 1) < b(b.length - 1))

     6. groupBy		P: U => V
			Objects of the input collection are grouped based on the function output
			Returns scala.collection.immutable.Map[V,List[U]] where:
				each unique value of the fn output is the key (K)
				all objects of the collection that produvced the key forms the value (V)

			List[U].groupBy( U => V ) => List[ (V, Iterable[U]) ]

			words.groupBy( x => x ).toList.map(t => (t._1, t._2.length))

     7. foldLeft & foldRight	=> reduces the entire collection to a type that is different that the type of objects

			Two parameters as param-list
			
			1. zero-value : starting value of the type of output you want as output
			2. Sequence Function : is a function that iterativly merges all the objects of the 
			   collection with the zero-value. 

			List[U].foldLeft( (V, U) => V )  => V
	
				
		l1.foldRight( (0,0) )( (v, z) => (z._1 + v, z._2 + 1) )   

   Wordcount Program
   ------------------
	val filePath = "E:\\Spark\\wordcount.txt"
     
     	val output = Source
                   .fromFile(filePath)
                   .getLines()
                   .toList
                   .flatMap(x => x.split(" "))
                   .groupBy( x => x )
                   .toList
                   .map( p => (p._1, p._2.length) ) 
                   .sortWith( (a, b) => a._2 > b._2 )
     
     	println( output )


  Collections
  -----------
    
   1. Array & ArrayBuffer => Both are mutable collections
	Array 		-> fixed length collection
	ArrayBuffer 	-> Variabl length collection

   2. Seq => An ordered collection
	
	2.1 IndexedSeq -> Optimal for random access of data
		=> Vector
		=> Range

	2.2 LinearSeq  -> Optimal for iteration of data
		=> List
		=> Queue
		=> Stream

   3. Map => A collection of (k, v) pairs

	val m1 = Map( 1 -> 10, 2 -> 20, 3 -> 30)
	val m1 = Map( (1,10), (2,20), (3,30) )

	m1(<key>)  
	m1.get(<key>)  -> Returns an Option object
	m1.getOrElse(<key>, <default-value>)

   4. Set => Is an unordered collection of unique elements
	

  Option
  -------
    => represents an object that may be there or may not be there. 
    
       Option[U] => Some[U]   if there is a value to return
		    None      if there is no value to return. 

     
 ==========================================
   Spark - Basics & Architecture
 ==========================================     
   
   Spark is a framework written in Scala

   Spark is a unified in-memory distributed computing framework for big-data analytics 
	=> Spark is 100x faster than MapReduce if you use 100% in-memory computations.
	=> Spark is 6 to 7 times fater than MapReduce even if you use disk-based computations.
   
   Spark is a polyglot
	=> Spark supports multiple programming language
	=> Scala, Java, Python, R (and SQL)

   Spark can run on multiple cluster managers
	=> local, Spark Standalone, YARN, Mesos, Kubernetes


   Unified Framework
   -----------------
	Spark provides a set of consistent APIs for processing different analytical workloads
	using the same execution engine.

         => Batch analytics of unstructured data	: Spark Core API (low-level)
	 => Batch analytics of structured data		: Spark SQL
	 => Stream analytics (real-time)		: Spark Streaming & Structured Streaming
	 => Predictive analytics (using ML)		: Spark MLlib
	 => Graph parallel computations			: Spark GraphX


   Getting started with Spark
   --------------------------	
	Popular Options : 
	
	=> Scale-IDE for Eclipse + Maven
		=> Add required dependencies from Central maven repo
		=> The required packges are download and added to your project.
	=> IntelliJ + SBT

        Central maven repository: https://mvnrepository.com/ (search for Spark)

	Downloading "winutils.exe" to your local directory
		=> Create the following folder: C:\hadoop\bin
		=> Download an appropriate version of winutils.exe from the following url;
			https://github.com/steveloughran/winutils

   Spark Architecture
   ------------------

    	1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.


   ===========================================
    Spark Core API (Low level API)
   ===========================================

    => Spark Core is used to analyse unstructured data and is a low-level API
    => Uses RDD as fundamental data abstraction

    RDD (Resilient Distributed Dataset)
    -----------------------------------

	-> RDD is a collection of distributed in-memory partitions
             -> A partition is a collection of objects of some type.

	-> RDDs are immutable

        -> RDDs are lazily evaluated
		=> Transformations does not cause execution
		=> Execution will be triggered only by action command

	-> RDDs are resilient 
		-> RDDs are resilient to missing in-memory partitions


   Creating RDDs
   -------------
	
    Three ways:

	1. Create an RDD from some external data source (such as a text-file)

		val rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)	

	2. Create an RDD from programmatic data

		val rdd1 = sc.parallelize( 1 to 100, 3 )

	3. By applying transformations on existing RDDs
		
		val rdd2 = rdd1.flatMap( x => x.split(" "))

   RDD Operations
   --------------

	To things:

	1. Transformations	
		-> Returns an RDD
		-> Does not cause execution
		-> Transformation only create RDD Lineage DAGs

        2. Actions
		-> Trigger execution of the RDD
		-> Produces output
		-> Converts thr logical Plan into a physical execution plan.


   RDD Lineage DAG
   ---------------
   -> RDD Lineage is alogical-plan, maintained by the driver
   -> RDD Lineage maintains the hierarchy of all parent RDDs that caused the creation of this RDD.
  
    	val rddFile = sc.textFile(filePath, 4)
	
(4) E:\Spark\wordcount.txt MapPartitionsRDD[1] at textFile at <console>:26 []
 |  E:\Spark\wordcount.txt HadoopRDD[0] at textFile at <console>:26 []

	val rddWords = rddFile.flatMap( x => x.split(" ") )
	
(4) MapPartitionsRDD[2] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[1] at textFile at <console>:26 []
 |  E:\Spark\wordcount.txt HadoopRDD[0] at textFile at <console>:26 []

	val rddPairs = rddWords.map( x => (x, 1) )

(4) MapPartitionsRDD[3] at map at <console>:25 []
 |  MapPartitionsRDD[2] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[1] at textFile at <console>:26 []
 |  E:\Spark\wordcount.txt HadoopRDD[0] at textFile at <console>:26 []

	val rddWc = rddPairs.reduceByKey( (a, b) => a + b )

(4) ShuffledRDD[4] at reduceByKey at <console>:25 []
 +-(4) MapPartitionsRDD[3] at map at <console>:25 []
    |  MapPartitionsRDD[2] at flatMap at <console>:25 []
    |  E:\Spark\wordcount.txt MapPartitionsRDD[1] at textFile at <console>:26 []
    |  E:\Spark\wordcount.txt HadoopRDD[0] at textFile at <console>:26 []


   Getting started with Spark
   --------------------------	
	Popular Options : 
	
	=> Scale-IDE for Eclipse + Maven
		=> Add required dependencies from Central maven repo
		=> The required packges are download and added to your project.
	=> IntelliJ + SBT

        Central maven repository: https://mvnrepository.com/ (search for Spark)

	Downloading "winutils.exe" to your local directory
		=> Create the following folder: C:\hadoop\bin
		=> Download an appropriate version of winutils.exe from the following url;
			https://github.com/steveloughran/winutils

  
  RDD Persistence
  ---------------
     	val rdd1 = sc.textFile( <file> , 3 )
	val rdd2 = rdd1.t2(...)
	val rdd3 = rdd1.t3(...)
	val rdd4 = rdd3.t4(...)
	val rdd5 = rdd3.t5(...)
	val rdd6 = rdd5.t6(...)
	rdd6.persist( StorageLevel.MEMORY_ONLY )  ===> instruction to spark to save rdd6 partitions
	val rdd7 = rdd6.t7(...)

	rdd6.collect()	
	Lineage of rdd6 => rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
	Tasks: [sc.textFile, t3, t5, t6] -> collect

	rdd7.collect()
	Lineage of rdd7 => rdd6.t7
	Tasks: [t7] -> collect

	rdd6.unpersist()
	
	StorageLevels
        -------------
	1. MEMORY_ONLY		=> default, in-memory & deserialized
	2. MEMORY_ONLY_SER	=> in-memory & serialized
	3. MEMORY_AND_DISK	=> in-memory or on-disk, deserialized
	4. MEMORY_AND_DISK_SER	=> in-memory or on-disk, serialized
	5. DISK_ONLY		=> on-disk, serialized
	6. MEMORY_ONLY_2	=> 2 coies on different executors are stored
	7. MEMORY_AND_DISK_2	=> 2 coies on different executors are stored

       Commands
       ---------
	  => <rdd>.cache()  // can not define storage-level. It is MEMORY_ONLY 
	     <rdd>.persist()
	     <rdd>.persist( StorageLevel.DISK_ONLY )

	     <rdd>.unpersist()

	     <rdd>.toDebugString  => to check the lineage DAG of the rdd.
	     

   Executor memory structure
   ==========================

   	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


   Types of Transformations
   =========================

	-> Narrow Transformations
		-> is a transformation that does not cause data shuffling
		-> partition to partition transformations
		-> number of partitions of the output rdd is equal to that of input rdd

	-> Wide Transformations
		-> cause shuffling of the data across various input partitions
		-> number of partitions of the output rdd can be different than that of input rdd

  RDD Transformations
  ====================

   1. map		P: U => V
			Transforms each input object to the corresponding output object by applying the
			function. 
			Object to object transformation
			input RDD: N objects, output RDD: N objects

		rddFile.map(x => x.split(" "))

   2. filter		P: U => Boolean
			Filters the input objects based on the function.
			input RDD: N objects, output RDD: < N objects

		rddWords.filter(w => w(0) == 's').collect

   3. glom		P: None
			Create one Array object per partition with all the elements of the partition.

			rdd1			val rdd2 = rdd1.glom()

			P0: 2,1,3,2,4,5 -> glom -> P0: Array(2,1,3,2,4,5)
			P1: 4,6,4,6,5,7 -> glom -> P1: Array(4,6,4,6,5,7)
			P2: 8,0,7,9,5,6 -> glom -> P2: Array(8,0,7,9,5,6)

			rdd1.count : 18	(Int)    rdd2.count : 3  (Array[Int])

		rdd1.glom().map( x => x.sum ).collect()

    4. flatMap		P: U -> TraversableOnce[V] 
			flattens the elements of the iterables produced by the function

		val rddWords = rddFile.flatMap(x => x.split(" "))

    5. mapPartitions		P: Iterable[U] => Iterator[V]
				partition to partition transformation
				Takes entire partition as function input and transforms it to another iterable

		rdd1.mapPartitions( p => p.map(x => (x, x>5))).glom.collect
		rdd1.mapPartitions( p => List(p.sum).iterator ).glom.collect

    6. mapPartitionsWithIndex	P: (Int, Iterable[U]) => Iterator[V]
				Similar to mapPartitions but we get partition-index as additional function
				parameter.
			
		rdd1.mapPartitionsWithIndex( (i, p) => List( (i, p.sum) ).iterator ).collect

    7. distinct		P: None, Optional: numPartitions
			Returns an RDD with distinct elements

		rdd1.distinct().collect

    8. sortBy		P: U => V, Optional: ascending (true/false), numPartitions
			Elements of the RDD are sorted based on the function output

		rddWords.sortBy(w => w(0)).collect
		rddWords.sortBy(w => w(0), false).collect
		rddWords.sortBy(w => w(0), true, 5).collect

    Types of RDDs
    -------------
	-> Generic RDD: RDD[U]
	-> Pair RDD:	RDD[(K, V)]


    9. groupBy		P: U => V, Optional: numPartitions
			Returns a Pair RDD, where
			   key: Unique-value of the function output
			   value: CompactBuffer containing all the elements that returned the key


		rddWords.groupBy(x => x.length).map(p => (p._1, p._2.toList.length)).collect
		rddWords.groupBy(x => x).map(p => (p._1, p._2.toList.length)).collect


		val rddFile = sc.textFile("data/wordcount.txt", 4)
                      .flatMap(x => x.split(" "))
                      .groupBy(x => x)
                      .map(x => (x._1, x._2.toList.length))
                      .sortBy(x => x._2, false, 1)

   10. mapValues 	P: U => V
			Applied to Pair RDDs only
			Transforms thh 'value' part of the (K, V) bu applying the function.

		val rdd2 = rdd1.map(x => (x, x+10))
		Note: In the above function 'x' is the 'value' part of the (K,V) pairs.

   11. randomSplit	P: Array of weights
			Returns an array of RDDs split randomly approximatly in the weights specified.

		val rddArr = rdd1.randomSplit( Array(0.3, 0.2, 0.5) )
		val rddArr = rdd1.randomSplit( Array(0.3, 0.2, 0.5), 546435 )

   12. repartition	P: numPartitions
			Is used to increase or decrease the number of partitions of the output RDD. 
			Cause global shuffle.

   13. coalesce		P: numPartitions
			Is used to only decrease the number of partitions of the output RDD. 
			Cause partition merging

	Recommendations for better performance from RDDs
        ------------------------------------------------
	-> The size of each partitions should be around 128 MB (between 100MB to 1000MB)
	-> The number of partitions should be a multiple of number of cores
	-> If the number of partitions is less than but close to 2000, bump it up to 2000 partitions
	-> The number of cores in each executor should be 5 (4 to 6)

		
   14. partitionBy	P: partitioner
			Applied only to Pair RDDs
			Is used to control which (K,V)s go to which partition. 

		1. Range Partitioner

			val rangePartitioner = new RangePartitioner(5, rdd)    
    			val partitionedRdd = rdd.partitionBy(rangePartitioner)

		2. Hash Partitioner

			val hashPartitioner = new HashPartitioner(3)    
    			val partitionedRdd = rdd.partitionBy(hashPartitioner)

		3. Custom partitioner
			=> User can define custom partitioning logic.

   15. union, intersection, subtract & cartesian
			=> They are applied on two RDDs
			=> ex: rdd1.union(rdd2)

	  Let us say rdd1 has M partitions & rdd2 has N partitions, then

	  command			partitions
          ----------------------------------------
	  rdd1.union(rdd2)		M + N, narrow
	  rdd1.intersection(rdd2)	Bigger of M & N, wide
	  rdd1.subtract(rdd2)		M, wide
	  rdd1.cartesian(rdd2)		M * N


   ..ByKey Transformations
   ------------------------
	=> Wide transformations
	=> Applied only on Pair RDDs
	=> Does some computation based on the key

    16. sortByKey		P: None, Optional: ascending (true/false), numPartitions
				Sorts the objects of the RDD by key

			rddPairs.sortByKey().collect()
			rddPairs.sortByKey(false).collect()
			rddPairs.sortByKey(true, 6).collect()

    17. groupByKey		P: None, Optional: numPartitions
				Returns a pair RDD where
				  key: Each unique key
				  value: CompactBuffer containg all values.

				WARNING: Avoid "groupByKey"

			rddPairs.groupByKey().collect

		val rddFile = sc.textFile("data/wordcount.txt", 4)
                      .flatMap(x => x.split(" "))
                      .map(x => (x, 1))
                      .groupByKey()
                      .mapValues(x => x.sum)
                      .sortBy(x => x._2, false, 1)

     18. reduceByKey		P: (U, U) -> U, Optional: numPartitions
				Reduces all the values of each unique key by iterativly applying the function
				on all partitions (narrow op) and then across the output generated at each 
				partition (shuffle op).
       
		val rddFile = sc.textFile("data/wordcount.txt", 4)
                      .flatMap(x => x.split(" "))
                      .map(x => (x, 1))
                      .reduceByKey((a,b) => a + b)
                      .sortBy(x => x._2, false, 1)


     19. aggregateByKey	  => Reduces the values of each unique key to a value of type zero-value. 

					Three parameters:

					1. zero-value:  the final value of each unique-key is if type zero-value
					2. sequence function
					3. combine function
		
		val rdd_students = sc.parallelize(students_list, 3)
                            .map(x => (x._1, x._3))
                            .aggregateByKey( (0,0) )(seq_fun, comb_fun)
                            .mapValues(x => x._1.toDouble/x._2)

	val students_list = List(
  ("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  ("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  ("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  ("Keerti", "Maths", 87), ("Keerti", "Physics", 93), ("Keerti", "Chemistry", 91), ("Keerti", "English", 74), 
  ("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  ("Anitha", "Maths", 86), ("Anitha", "Physics", 62), ("Anitha", "Chemistry", 75), ("Anitha", "English", 83), 
  ("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60))
  
    val student_rdd = sc.parallelize(students_list , 3)
    
    val seqFn = (z:(Int, Int), v: Int) => (z._1 + v, z._2 + 1)
    val combFn = (a: (Int, Int), b: (Int, Int)) => (a._1 + b._1, a._2 + b._2)
    
    val avg_rdd = student_rdd.map( x => (x._1, x._3) )
                     .aggregateByKey((0,0))(seqFn, combFn)
                     .mapValues(p => p._1.toDouble/p._2)
    
    avg_rdd.collect.foreach(println)



	20. joins  => to be discussed..

        21. cogroup	


  Action Commands
  ---------------

   1. collect

   2. count

   3. saveAsTextFile

   4. reduce		P: (U, U) => U
			Reduces the entire RDD to one final value of the same type as the RDD.
			Reduces each partition in the first stage and reduces the outputs of each partition further
			to produce the final output in the second stage

		P0: 2, 1, 3, 2, 4, 5, 6, 6	-> reduce -> 29 -> reduce => 116
		P1: 4, 5, 7, 8, 9, 0, 9, 7, 5   -> reduce -> 54
		P2: 7, 1, 2, 3, 2, 3, 5, 4, 6	-> reduce -> 33

		rdd1.reduce( (a,b) => a + b )


   5. aggregate  	   -> Reduces the entire RDD to a type different than the type of elements using
			      a zero-value. The final output is of the type of zero-value (not of the type 
			      of elements)

			      Three parameters:  RDD[U]

			      1. zero-value : Z (type of zero-value)
			      2. seq-operation:  Operates on each partition and folds the elements with the 
						 zero-value.
					         (Z, U) => Z     (similar to scala 'fold' HOF)
			      3. combine operation: Reduces all the values of each partition produced by 
						    seq-operation using a reduce function.
						  (Z, Z) => Z						
			rdd1: 
			P0: 8, 3, 8, 9, 8, 3   => (39, 6)  => (100, 18)
			P1: 4, 2, 1, 4, 6, 7   => (24, 6)
			P2: 8, 9, 8, 5, 6, 1   => (37, 6)

			
			rdd1[U].aggregate(zv: Z)(seq-fn: (Z, U) => Z, comb-fn: (Z, Z) => Z)

			rdd1[U].aggregate( (0,0) )( (z, v) => (z._1 + v, z._2 + 1) , 
						    (a, b) => (a._1 + b._1, a._2 + b._2) )
	


















   


