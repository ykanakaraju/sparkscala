  
  Curriculum (52 hours)
  ---------------------------
   Scala
     	-> Basics		
	-> Functional programming
	-> Object oriented programming
	-> Pattern matching 
   Spark Core API Basics
	-> RDD - Transformations & Actions
	-> Spark shared variables
   Spark Submit command
   Spark SQL
	-> DataFrame Operations
	-> Integrations - RDBMS & Hive
   Spark Streaming
	-> Structured Streaming
   Spark Performance Tuning


   Materials        
   ---------
	-> PDF Presentations
	-> Code Modules - Scala & Spark
	-> Class Notes
	-> Databricks Notebooks (DBC file)
	-> Github: https://github.com/ykanakaraju/sparkscala

  
  Getting started with Scala
  --------------------------
   
      1. vLab - Lab allocated to you. 
	
	 -> Follow the instructions given in the attached document.
	 -> You will be loging into a Window Server
	     -> Here you find a document on the desktop with useris and password. 
	 -> Click on the "Oracle VM Virtualbox" and connect to Ubuntu lab. 

	 => here you can open a terminal and connect to Spark shell (type "spark-shell")
	 => You can also launch "Eclipse" 
   
  
     2. Setting up your own environemnt on your personal machine. 

	   Pre-requisite: Java 8
	   => Open a terminal and type "java -version" (it has to be 1.8.xxx or up)
		
	   1. Scala IDE (version of Eclipse)
		URL: http://scala-ide.org/download/sdk.html

		Download Scala IDE for your OS and unzip it to a suitable location
		Navigate into the unzipped folder and click on "Eclipse" application icon to launch Scala IDE.

	   2. IntelliJ
		Follow the instructions @ https://docs.scala-lang.org/getting-started/index.html
		
	        Two build tools for Scala:
		-> Maven (Scala IDE + Maven)  
		-> SBT  (IntelliJ + SBT)


     3. Signup to "Databricks Community" Edition Free account.

		URL to Signup : https://databricks.com/try-databricks
		URL to Login: https://community.cloud.databricks.com/login.html
	

  Scala
  -----
     
     -> SCAable LAnguage -> SCALA
      -> Scala is a JVM based Language
	
      -> Scala is multi-paradigm programming language.
	
	-> Objected Oriented Programming
	-> Functional Programming

	-> Scala is BOTH object-oriented & functional programming.

      -> Scala is a statically (strongly) typed language
           -> The data type of every variable is fixed and known at compile time

      => Scala "Pure Object Oriented" programming language.
	   -> Every data point is a "object" in scala
	   -> There are no premitives in Scala

		val i = 10
		val j = 20
		val k = i + j
                    => In the above statement i & J are Int objects
		    =>  '+' is a method			

      Scala Variables and Values
		val -> immutable values
	       	once a value is assigned, you can not change it.

		var -> mutable variable
	       	the value can be changed after assignment

      Type Declaration:   
		-> val i : Int = 10


      Scala Type Inference
         	-> Scala can implicitly infer the types based on the assigned value.
	 	-> We do not have to explicitly declare data types.

      Scala is PURE object oriented language
      --------------------------------------	
      -> Scala does not have primitives or operators.	
      -> In scala, all data is objects and operations are method invocations.

	  val i = 10.*(40)  
   
             -> 10 is an Int object
	     -> * is a method invoked on 10 (Int object)
	     -> 40 is an Int object passed as a parameter
	     -> i is an Int object returned by the * method.
		
      -> <obj>.method<param1> => <obj> method<param1> => <obj> method param1


      Scala Blocks
      ------------
	 => A block any code enclosed in  { }
	 => A block is scala has a return value.
	 => The block returns the value of the last expression that is executed.

         Scala Unit => In Scala "Unit" is an object that represents "no value"
		    prined as "()"


      Scala Class Hierarchy 
      ---------------------
	   Any 	=> AnyVal  => Int, Long, Boolean, Unit, Byte, Char, ...
		=> AnyRef  => String, List, Map, ....


	=> 'Unit' represents no value.
	   prints as ()


      String Interpolations
      ---------------------
	s => Allows you to invoke variables using $ sybmol
		ex: val str = f"i = $i\nj = $j\nk = $k"

	f => s interpolator + can use formatting symbols
		ex: val str = f"i = $i%.2f\nj = $j%.2f\nk = $k"

        raw => s interpolator + escapes the escape chars
		val str = raw"i = $i%.2f\nj = $j%.2f\nk = $k"

	
	printing output
        ----------------
	val name = "Raju"
    	val age = 48
    	val height = 5.9
    
	print(name)
	println(name)
    	println(name, age, height)
	println("name: %f, age: %f".format(name, age))


      String Interpolations
      ---------------------
	s => Allows you to invoke variables using $ sybmol
		ex: val str = f"i = $i\nj = $j\nk = $k"

	f => s interpolator + can use formatting symbols
		ex: val str = f"i = $i%.2f\nj = $j%.2f\nk = $k"

        raw => s interpolator + escapes the escape chars
		val str = raw"i = $i%.2f\nj = $j%.2f\nk = $k"


      Reading Data from StdIn
      -----------------------

      val name = StdIn.readLine("Enter your name: ")
     
      println("Enter your age: ")  
      val age = StdIn.readInt() 
    
      println("Enter your height: ")  
      val ht = StdIn.readDouble() 
      
      println(s"Name: $name,  Age: $age, Height: $ht") 


     Flow Control Constructs
      -----------------------
	1. if .. else if .. else

		In scala, if condition returns a value
		The type of the value returned is the common-denomitor class of various branches if if condition.

        2. match .. case

		   val k = j match {     
     				case 10 => { "Ten" }
     				case 20 => { "Twenty" }
     				case _ if (j % 2 == 0) => { "Even Number" }
     				case _ if ( j > 100 ) => { "Greater than 100" }
     				case _ => { "Nothing matches" }
   			  }
    Loop
    ----
	=> while
	=> do .. while
	=> foreach : take a function as a parameter and applies that on all elements of a collection object
		List(1,2,3,4,7,5,6,7).foreach( x => println(s"x = $x") )

	=> for loop
	
		for(<generator>, <generator>, ...) { .. }

		for( i <-  1 to 10 by 2){
        	    println(s"i = $i")
     		}

		for( i <-  1 to 10 by 2; j <- 1 to 20 by 4 ){
        	    println(s"i = $i, j = $j")
     		}

		for( i <-  1 to 10 by 2 if (i != 3); j <- 1 to 20 by 4 if (j > i) ){
       		    println(s"i = $i, j = $j")
     		}

		for comprehension
		-----------------
		val l1 = for(i <- 1 to 100 by 2) yield(i)

    
      Range Object
      -------------
		Range(0, 100, 10)  => 0: start, 100: end (excluded), 10: step
		Range(0, 10)  => step is one if not specified
		Range(10, 1, -1) =>  10, 9, 8, 7, 6, 5, 4, 3, 2
		Range(100, 0, -10) => 100, 90, 80, 70, 60, 50, 40, 30, 20, 10
		Range(0, 100, -2) => Empty
		
		0 until 10 by 2 => Range(0, 10, 2)
		0 to 10 by 2    => Range(0, 11, 2)


     Exception Handling 
     ------------------
	
         try {
		some code that might through an exception
         }
         catch {
	     case e: ArrayIndexOutofBoundsException => {
	     }
             case f: FileNotFoundException => {
             }
	     case _ => {
             }
         }
         finally {
	     // code that executes after try/catch blocks
         }


      Example
      -------

      try {
         println("Reading from a file ...")   
         val f = new FileReader("file1456.txt")            
      } 
      catch {
         case ex: FileNotFoundException => {
            println("Missing file exception")
            println( ex.getMessage() )
         }         
         case _: IOException => {
            println("IO Exception")
         }
         case _: Exception => {
            println("generic exception")
         }
      } 
      finally {
         println("Exiting finally...") 
      }  


    Collections
    -----------

     => Array: mutable & fixed length
     => ArrayBuffer: mutable with variable length

     Immutable Collections
    
        -> Seq  (Sequences)
	     -> Ordered collections and elements can be accessed using an index.

	     -> Indexed Sequences
		 -> Vector
		 -> Range

		 => Optimized for fast random-acccess		

	     -> Linear Sequences
		-> List
		-> Queue
		-> Stream 

		=> Optimized for visiting the elements linearly (i.e in a loop)
		=> They are organized as linked lists
		
		List(1,2,3,4,5,6) => List(1, List(2,3,4,5,6))   
			// here 1 is head, List(2,3,4,5,6) is tail
		        => List(1, List(2, List(3, List(4, List(5, List(6, List())))))) 

		list => List(head, tail)

	-> Set
	     -> Is a unordered collection of unique values.
	     -> We can NOT access the elements using an index.
	     -> SortedSet, BitSet

	-> Map
	     -> A collection of (K, V) pairs

		val m1 = Map( (1, 'A'), (2, 'B'), (3, 'C') )
		val m1 = Map( 1 -> "A", 2 -> "B", 3 -> "C" )

                m1(1) -> "A"
		m1(10) -> raise java.util.NoSuchElementException	
	
     Range Object
     -------------
	=> exclusive range (the final value is excluded)

	Range(1, 10)        => 1,2,3,4,5,6,7,8,9
	Range(1, 10, 2)     => 1,3,5,7,9
	Range(100, 0, -20)  => 100, 80, 60, 40, 20
	Range(1, 20, -1)    => Empty Range() object

	1.to(10) or 1 to 10 => Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
				-> Inclusive Range

	0 to 10 by 2	  => Range(0, 2, 4, 6, 8, 10)    // inclusive
	0 until 10 by 2	  => Range(0, 2, 4, 6, 8)	 // exclusive	
	
	100 until 0 by -25  -> Range(100, 75, 50, 25)

	
     Tuple
     -----
	-> Is an object which can hold elements of multiple data types
	-> A Tuple with only two elements is generally called a "pair"
  
  	 val t1 = (10, 10.5, 10>5, "Hello", (10, 20))

	 val t2 = (1,2.5,5,false)

	 t1._2     // 10.5
	 t1._5     // (10, 20)
	 t1._5._2  // 20

	
    Option[U]
    ---------

      -> Represents an data type that represents a value that may or may not exist.
      -> Some[U] & None  --> are the values of Option Type

	Example
	--------
	val m1 = Map( 1 -> 100, 2 -> 200, 3 -> 300 )

	m1.get(<key>) returns an Option[Int] object which have a value of 'Some[Int]' if
	the key is present in the map, else it return 'None'

	m1.get(1)  => Some(100)
	m1.get(10) => None



  Scala Functional Programming
  ============================

   Methods
   -------
	-> Reusable code block that return some value
	-> Method will have a name and optionally some parameters/arguments

	-> NOTE: note that we are using an "=" symbol

	def sum(a: Int, b: Int) : Int = {
        	a + b
     	}
	
	-> methods can take 0 or more arguments                
	-> Methods can be called by positional parameters
	-> Methods can be called by named parameters

	val x = sum(10, 20)		// positional params
	val x = sum(b=10, a=20)   	// named params


	-> Method arguments can have default values	

	def sum(a: Int, b: Int, c: Int = 0) = {       
        	println( s"a = $a, b = $b, c = $c" )    
        	a + b + c
     	}

        -> Methods can have variable length arguments
	  
		 def sum(a: Int, i: Int*) = {       
       			var s = 0 
       
       			for (x <- i){
         			println(s"x = $x")
         			s += x
       			}
       
       			a + s
     		}
    
     		val x = sum(10, 20, 30, 40, 50)
	
		-> In this example, i represents [20, 30, 40, 50]

	-> methods can call themselves within the code. (recursive methods)

		def factorial(n: Int) : Int = {
        		if ( n == 1 ) n
        		else n * factorial(n - 1)
      		}

         -> methods can take multiple parameter lists

		def sum(a: Int, b: Int)(c: Int)(d: Int) = (a + b)*c*d    
    		val x = sum(10, 20)(5)(2)


   Procedures
   ----------
	-> Are like methods but does not return any value.
	-> A procedure always return "Unit" irrespective of the return type of the block
	-> Procedure does not have the = symbol in the defintion

	def box(name: String) {       
        	val line = "-" * name.length() + "----"
        	println( line + "\n| " + name.toUpperCase  + " |\n" + line)        
     	}
     
     	box("scala is a programming language")
      

   Functions
   ---------

       -> In Functional programming, a 'function' is like a literal.
       -> A function has a value and type of its own.
       -> Functions are anonymous by default
       -> A function can be assigned to a variable (named functions)
       -> A function can be passed as a parameter to a method/function
       -> A block can return a function as final value
       -> A method can return a function as return value
  
         function literal		 	type
	----------------------------------------------------------
	(a: Int, b: Int) => a + b		(Int, Int) => Int
	(a: Int) => a * a			Int => Int
	(a : String) => a.length		String => Int
	() => "Windows 10"			() => String
	(name: String) => print(name)		String => Unit	
	(a: Int, b: Int) => List(a, b)		(Int, Int) => List[Int]
	(a: Int, b: Int) => (a, b)		(Int, Int) => (Int, Int)
	(a: (Int, Int), b: (Int, Int)) => (a._1 + b._1, a._2 + b._2)
					((Int, Int), (Int, Int)) => (Int, Int)

        //method returning a function
	def compute(op: String) = {
          (i: Int, j: Int) => {
             op match {
               case "+" => i + j
               case "-" => i - j
               case "*" => i * j
               case _ => i % j
             }
          }
        }      

        // method taking function as a parameter
       def calculate(a: Int, b: Int, f: (Int, Int) => Int) = {
         f(a, b)
       }
      
       val sum = compute("+")
       val diff = compute("-")
       val prod = compute("*")
       val mod = compute("blah")      
      
       println ( calculate(10, 20, compute("+")) )
       println ( calculate(10, 20, sum) )

       println ( calculate(10, 20, (a: Int, b: Int) => a + b) )
       println ( calculate(10, 20, (a, b) => a + b) )
       println ( calculate(10, 20, _+_) )




   Scala Higher Order Functions (HOF)
   ----------------------------------

	=> A HOF is a method/function that takes a function as a paramer or return a function
	   as a return value. 
	=> They usually operate on collections and apply the function to the elements of the colelction.

     1. map		P: U => V
			Element to element transformation
			Returns a Collection that transformed by applying the function on all the elements of the
			collection. 

		 lines.map( a => a.split(" ") )
		 l1.map(x => x > 5)

		 List[String].map( String => Array[String] ) => List[Array[String]]
		 List[U].map(U -> V) => List[V]

    2. filter		  P: U -> Boolean
			  Only those elements for which the function returns True will be there in the output
			  collection.

			  List[U].filter => List[U]

    3. reduceLeft (reduce), reduceRight  P: (U, U) => U
			  The function is iterativly applied on all elements of the collections and reduces the
			  entire collection to one value of the same type. 

			  List[U].reduce( (U, U) => U ) => onr value of type "U"

		List(1, 2, 1, 3, 2, 5, 7, 4, 6, 7, 8, 9, 0, 9).reduce( (x, y) => x + y ) = 64

    4. flatMap		  P: U -> scala.collection.GenTraversableOnce[V]
			  flatMap flattens all the elements of the collection

			   List[String].flatMap( String => Array[String] ) => List[String]  
			   Collection[U].flatMap( U -> Iterable[V]) => List[V]

    5. sortWith		  P: (U, U) => Binary Sorting function
			  Based on the sorting function the elements of the output list are sorted. 
		
			  words.sortWith( (x, y) => x < y )
			  words.sortWith( (x, y) => x.length < y.length )

    6. groupBy		  P: U => V
			  Returns a Map objects, where
				key: Each unique value of the function output
				value: List object containing all the objects that produced the key.

				List[U].groupBy( U => V ) => Map[ V, List[U] ]

    7. mapValues	  P: U -> V
			  Applies only on Map objects
			  Transforms the value part of the map by applying the function.


    Scala Wordcount Program
    -----------------------

	val wordcount = Source.fromFile("file1.txt")
                    .getLines()
                    .toList
                    .flatMap(x => x.split(" "))
                    .groupBy(x => x)
                    .mapValues( x => x.length )
                    .toSeq
                    .sortWith( (a,b) => (a._2 > b._2))
                    
    	wordcount.foreach(println)


  ===================
     Spark
  ===================

     => Spark is a unified in-memory distributed computing framework

     => Spark is used for performing Big Data Analytics

     => Spark framework is written using "Scala" programming. 
	
     => Spark is a polyglot
	 => Support Scala, Java, Python, R 

     => Spark Unified Framework

	-> Spark provides consistent set of APIs for performing different analytics workloads
	   using same execution engine. 

		=> Batch Processing of Unstructured data  :  Spark Core
		=> Batch Processing of Structured data	  :  Spark SQL
		=> Stream Processing (Real time)	  :  DStreams API, Structured Streaming
		=> Predictive Analytics (ML)	  	  :  Spark MLLib
		=> Graph Parallel Computations		  :  Spark GraphX


    Getting started with Spark using Scala
    --------------------------------------

        => Two package managers (& build tools) are popular with Spark (Scala or Java)
		=> Maven
		=> SBT (Scala Build Tool / Simple Build Tool)



   Spark Architecture & Building blocks
   ------------------------------------
    
	1. Cluster Manager (CM)
		-> Applications are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, spark standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the 'SparkContext' object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		-> Where to run the driver process
		1. Client : default, driver runs on the client. 
		2. Cluster: driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> Receives the tasks from the Driver
		-> All tasks run the same code but on different partitions of the data
		-> The status of tasks are reported to the driver. 


  Spark Core API
   ---------------
	=> Is the low-level API
	=> This is responsible for performing all low level operations such as task-schedules,
           memory-management, fault-recovery etc.


   RDD (Resilient Distributed Dataset)
   -----------------------------------
	
     => Is the fundamental in-memory data abstraction of Spark core API

     => RDD is a collection of distributed in-memory partitions
	   -> Partition is a collection of objects (of any type)

     => RDD partitions are immutable
	  -> Can apply transformation to create new RDDs

     => RDDs are lazily evaluated
	-> Transformations does not execution
	-> Only Action commands trigger execution.

     => RDDs are resilient to missing in-memory partitions
	  -> RDDs can recreate missing partitions at run time.


   Creating RDDs
   -------------
	Three ways:

	1. Creating RDD from external data file
	
		val rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

		val rddFile = sc.textFile("E:\\Spark\\wordcount.txt")
			-> The number of partitions of rddFile is determined by "sc.defaultMinPartitions"
	
	2. Creating RDD from programmatic data
	
		val rdd1 = sc.parallelize( 1 to 100 by 5, 3 )	

		val rdd1 = sc.parallelize( 1 to 100 by 5 )
			-> The number of partitions of rdd1 is determined by "sc.defaultParallelism"	

	3. Create an RDD by applying transformations on existing RDD

		val rddWords = rddFile.flatMap(x => x.split(" "))

   RDD Operations
   --------------
	Two operations:

	1. Transformations
		-> Transformations return an RDD
		-> Transformations only create the Lineage DAG (logical plan) 
		-> Transformations does not cause execution

        2. Actions
		-> Trigger the execution of the RDD
		-> Causes the DAG to be converted into a physical plan. 


   RDD Lineage DAG
   ---------------	

	val rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

(4) E:\Spark\wordcount.txt MapPartitionsRDD[1] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[0] at textFile at <console>:24 []

	val rddWords = rddFile.flatMap(x => x.split(" "))

(4) MapPartitionsRDD[2] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[1] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[0] at textFile at <console>:24 []

	val rddPairs = rddWords.map(x => (x, 1))

(4) MapPartitionsRDD[3] at map at <console>:25 []
 |  MapPartitionsRDD[2] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[1] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[0] at textFile at <console>:24 []

	val rddWc = rddPairs.reduceByKey( (x,y) => x + y )

(4) ShuffledRDD[4] at reduceByKey at <console>:25 []
 +-(4) MapPartitionsRDD[3] at map at <console>:25 []
    |  MapPartitionsRDD[2] at flatMap at <console>:25 []
    |  E:\Spark\wordcount.txt MapPartitionsRDD[1] at textFile at <console>:24 []
    |  E:\Spark\wordcount.txt HadoopRDD[0] at textFile at <console>:24 []


   RDD Transformations 
   ===================

      => to be discussed ..























