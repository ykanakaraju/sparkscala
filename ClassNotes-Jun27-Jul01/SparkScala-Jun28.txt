
  Agenda (Spark using Scala)
  --------------------------
   -> Scala Refresher
   -> Spark - Basics & Architecture
   -> Spark Core API
	-> RDD Transformations and Actions
	-> Spark Shared Variable
        -> Spark-Submit
   -> Spark SQL
	-> DataFrame Operations
	-> Integration with MySQL & Hive
	-> SQL Optimizations & Tuning
   -> Spark Streaming
	-> DStreams API (introduction)
	-> Structured Streaming


  Materials
  ---------
	=> PDF Presentations
	=> Code Modules
	=> Class Notes
	=> Github: https://github.com/ykanakaraju/sparkscala

 
  Scala
  ===== 
     	=> SCAlable LAnguage
	
	=> Statically Typed Language
		-> Data type os a variable is fixed at compile time.
		-> Can not change the data type. 
		
	=> Scala is comiler-based language

   	=> Scala is a multi-paradigm Programming Lang.
		-> Scala is an OOP Lang
		-> Scala is Functional Programming

	=> Scala has type inference

	=> Scala is a Pure object oriented language. 
               -> Scala does not have primitives and operators

        => Scala mutables and immutables
		var => Multables: Can change  
		val => Immutable: Can not change once assigned.

		Scala prefers "Immutables"

		val i = 10
		i = 20  // Invalid
	
		var i = 10
		i = 20  // Valid
			

     Scala Blocks
     ------------
	=> A block is a set of statements and expressions enclosed in  { }
	=> A block has a return value 
		-> The return value of the block is the value of the last statement/expression that
		   is executed in that block.
	
		
     Scala "Unit" => is a Class whose object represent "no value".
		 => Printed as ()


     Scala Flow Control Constructs
     -----------------------------

	=> if .. else if .. else
        => match .. case

	
	=> if .. else consutruct returns a value
	   The return value is the value if the last executed statement of the executed block. 

		val x = 100
    		val y = 150
    		var z = 0
    
    		z = if ( x > y )  x - y  else if (x < y) y - x else x   

        => match..case consutruct returns a value
	   The return value is the value if the last executed statement of the executed block. 

		x match {
        		case 10 => z = 10
        		case 20 => z = 20
        		case a if (a % 2 == 0) => { z = 10 }
        		case _ => { z = -1 }        
    		} 

		z = x match {
       			case 10 => 10
        		case 20 => 20
        		case a if (a % 2 == 0) => {
          			val i = 10
          			val j = 20
          			i + j + a
        		}
        		case _ => -1        
    		}

    Loop Constructs
    ---------------

	-> while
	-> do .. while
	-> foreach

		<Iterable>.foreach(<function>)

		"scala".foreach( x => println("x = " + x) ) 
	-> for

	for loop: 
	---------

		for ( i <- 1 to 10 ) {
         		println(s"i: $i, j: $j")
      		}

		for ( i <- 1 to 10; j <- 1 to 10 by 2) {
         		println(s"i: $i, j: $j")
      		}

		for ( i <- 1 to 10 if (i%2 != 0); j <- 1 to 10 by 2 ) {
         		println(s"i: $i, j: $j")
      		}

		for ( i <- 1 to 10 if (i%2 != 0); j <- 1 to 10 by 2 if (i != j) ) {
         		println(s"i: $i, j: $j")
      		}

		for comprehension
		-----------------
		val v1 = for ( i <- 1 to 10 if (i%2 != 0)) yield(i*10)      
     		println( v1 )   // Returns Vector(10, 30, 50, 70, 90)


    Range class
    -----------
	Range(start, end, step) => generates Int values from start to (end-1) with a step

	Range(1, 10, 2) => 1,3,5,7,9
	Range(20, 0, -3) => 20, 17, 14, 11, 8, 5, 2
	Range(1, 10, -1) => empty
	Range(1, 10) => 1,2,3,4,5,6,,7,8,9 (default step is 1)

	1 to 10 	 => 1,2,3,4,5,6,7,8,9,10  (Range.inclusive)
	1 until 10 	 => 1,2,3,4,5,6,7,8,9     (Range)

	0 to 10 by 2	 => 0,2,4,6,8,10
	0 until 10 by 2  => 0,2,4,6,8

 
   Interpolators
   -------------

    s interpolator : s"x = $x, y = ${y + 10}"

    f interpolator : s interpolator + interpolate formatting characters 
		     f"x = $x%2.2f, y = $y%1.3f"

    raw interpolator : s interpolator + escape the escape characters
		  raw"x = $x%2.2f\ny = $y%1.3f"



    Exception Handling
    -------------------

	try {
		// some code that throws an exception
	}
	catch {
	    case e: FileNotFoundException => {
		   println( e.getMessage ) 
		}

	    case e: ArrayIndexOutOfBounds => { ... }
            case e: Exception => { ... }
	    case _ => { ... }
	}
	finally {
	     // some code that is always executed
	}


   Getting started with Scala
   ==========================

     1. Using your vLab

	  -> You connect to Windows server
	  -> Double Click on the Oracle VM Virtualbox icon
		-> Launch the Ubuntu VM
		   -> Open a terminal
		   -> Type "spark-shell"
		   -> Launch "Scala-IDE" for eclipse

     2. Setting up Scala development environment on your personal machine.

	 2.1 Scala IDE

	  	-> Make sure you have Java 8 (JDK 1.8.x)
		  (run "java -version" command at command prompt)
          	-> Download Scala IDE for Eclispe from http://scala-ide.org/download/sdk.html

         2.2 IntelliJ
		-> https://docs.scala-lang.org/getting-started/index.html

		=> Community Edition: https://www.jetbrains.com/idea/download/#section=windows
		=> Installing Scala Plug-in: https://www.jetbrains.com/help/idea/managing-plugins.html

		=> Working with Scala on IntelliJ:
		   -> https://docs.scala-lang.org/getting-started/intellij-track/building-a-scala-project-with-intellij-and-sbt.html


     3. Using Databricks Community Edition Free account
 
		Sign-up: https://databricks.com/try-databricks
	  	Login: https://community.cloud.databricks.com/login.html



     4. Using Online Scala Editors: https://scastie.scala-lang.org/pEBYc5VMT02wAGaDrfLnyw



    Scala Class Heirarchy
    ----------------------

       Any   => AnyVal => Int, Long, Double, Boolean, CHar, Unit
	     => AnyRef => String, List, Tuple, All other classes..


    Collections
    -----------
	=> Array	  -> Mutable & Fixed Length
	   ArrayBuffer	  -> Mutable & Variable Length

	=> Seq  : Ordered Collections
		  Elements can be invoked using an index.

		 => IndexedSeq : Optimized for random-access of data
			-> Range, Vector

		 => LinearSeq : Optimized for sequential-access (loops)
				These are linked lists
			-> List, Queue, Stream

	=> Set  : Unordered collection of unique objects

	=> Map : Is a collection of (Key, Value) pair


   Reading from File
   -----------------

	val lines = scala.io.Source.fromFile( <filePath> ).getLines.toSeq


    Option[U]
    ---------

	=> Represents a object which may or may not have a value.
	=> Returns Some[U] if value is present
		   None if value is not present
	

    Methods
    --------
	=> A method is a executable code block

	=> Methods can be called using positional parameter
	=> Methods can be called using named parameter
	=> methods can have multiple parameter lists
	=> Methods parameter can have one variable-lengh arguments and it has to be last argument.
	=> Method parameters can have default values.
	=> Methods can be called recusivly (recursive method)


    Procedures
    ----------
	=> Are like mathods, but they always return Unit


	def box(s: String) {
       	     val border = "-" * s.length() + "----"       
       	     println( border + "\n| " + s + " |\n" + border )
    	}

    Functions
    ---------
	=> Function are treated as literals
		Function literal: (a: Int, b: Int) => { a + b }
	=> Functions are anonymous by nature
	=> Function can be assigned to a variable

        Function literal				Type
	----------------------------------------------------------------
	(a: Int, b: Int) => a + b			(Int, Int) => Int
	(s: String) => s.toUpperCase			String => String
	(a: Int, b: String) => b * a			(Int, String) => String
	() =>  "Windows 10"				() => String
	(s: String) => print(s)				String => Unit
	(a: (Int, Int), b: Int) => (a._1+b, a._2+b)     ((Int, Int), Int) => (Int, Int)

	=> A function can be passed as a parameter to a method or function
	=> A block can return a function as a final value.
	=> A method / function can return a function as an output.

	def compute(op: String) = {
            op match {
         	case "+" => (a : Int, b: Int) => a + b
          	case "-" => (a : Int, b: Int) => a - b
          	case "*" => (a : Int, b: Int) => a * b
          	case _ => (a : Int, b: Int) => a % b
             }
     	}


   Higher Order Functions (HOF)
   ----------------------
    A HOF is a method or function that takes a function as a parameter or returns a function as a return value.

    1. map			P: U => V
   				Transforms each object of the input collection to a different object by appying 
				the function
				input: N object, output: N object 

		 l1.map(x => x > 8)
		 lines.map( s => s.split(" ") )

    2. filter			P: U => Boolean 
				Only those objects for which the function returns true will be in the output. 
				input: N object, output: <= N object 

		lines.filter(x => x.split(" ").length > 8)

    3. reduceLeft, reduceRight	P: (U, U) => U 
				Reduces the entire collection to one final value of the same time by iterativly
				applying the function.

		List(3,2,4,3,5,6).reduceLeft( (x, y) => x - y )
    		3,2,4,3,5,6 => 1,4,3,5,6 => -3,3,5,6 => -6,5,6 => -11,6 => -17

		List(3,2,4,3,5,6).reduceRight( (x, y) => x - y )
		3,2,4,3,5,6 => 3,2,4,3,-1 => 3,2,4,4 => 3,2,0 => 3,2 => 1		

		List[U].reduceLeft( (U,U) => U ) => U

		l2.reduceLeft( (x, y) => ( ((if (x._1 > y._1) x._1 else y._1), (if (x._2 > y._2) x._2 else y._2)) ))


    4. flatMap			P: U => GenTraversableOnce[V]  (fn output should be a collection object)
				flatMap flattens the function output to constituent elements.
				input: N object, output: >= N object 

				List[U].flatMap( U -> List[V] ) => List[V]

				val words = lines.flatMap(x => x.split(" "))


   5. sortWith			P: binary sorting function
				Elements of th collection are sorted based on the binary sorting function.  
				input: N object, output: N object 

			words.sortWith( (x, y) => x(0) < y(0) )

   6. groupBy			P: U => V
				Elements of the input collection are grouped based on the function output.
				Returns a Map object where:
					key : unique function output
					value: Collection containing elements that produced the key

			l2.groupBy(x => x._2)
			words.groupBy(x => x).toList.map(x => (x._1, x._2.length)).sortWith((x, y) => x._1 < y._1)


   7. foldLeft & foldRight	=> reduces the entire collection to a type that is different that the type of objects

			Two parameters as param-list
			
			1. zero-value : starting value of the type of output you want as output
			2. Sequence Function : is a function that iterativly merges all the objects of the 
			   collection with the zero-value. 

			List[U].foldLeft( (V, U) => V )  => V	
				
		l1.foldRight( (0,0) )( (v, z) => (z._1 + v, z._2 + 1) )


  8. mapValues		 	=> P: U => V
				   Operates only on Map objects
				   Will transform only the 'value' part of the key-value pairs.

		 m1.mapValues( x => x.length )


  Assignments 
  ===========
	=> Write a method to print the first N numbers in the fibonnaci series
	   function: printFib	
		printFib(7) => 1, 1, 2, 3, 5, 8, 13

        => Write a method to print the first N prime numbers		
	   function: printPrimes
		printPrimes(7) => 2, 3, 5, 7, 11, 13, 17

  Use-Case
  ========
   WordCount: Fetch the frequency of each unique word in a given text file.

   val output = Source.fromFile("E:\\Spark\\wordcount.txt")
                      .getLines()
                      .toList
                      .flatMap(x => x.split(" "))
                      .groupBy(x => x)
                      .mapValues(x => x.length)
                      .toSeq
                      .sortWith((a, b) => a._2 > b._2)

  ========================================================
       Spark
  ========================================================

    -> Spark is written in 'Scala'
  
    -> Spark is an in-memory distributed computing framework for big data analytics
       using well-defined programming constructs. 

    -> Spark is a Unifed Framework

         Spark provides a consistent set of APIs for processing different analytics workloads
	 based on the same execution engine.

	   => Batch Processing of unstructured data	: Spark Core API
	   => Batch Processing of structured data	: Spark SQL
	   => Stream processing (real-time)		: Structured Streaming, DStreams API
	   => Predictive analytics (using ML)		: Spark MLlib
           => Graph parallel computations		: Spark GraphX

     -> Spark is a polyglot
	  -> Spark apps can be written in : Scala, Java, Python, R (and SQL)
	

    Spark Architecture and Building Blocks
    --------------------------------------    
       1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.


   Getting started with Spark
   --------------------------	
	Popular Options : 
	
	=> Scale-IDE for Eclipse + Maven
		=> Add required dependencies from Central maven repo
		=> The required packges are download and added to your project.
	=> IntelliJ + SBT

        Central maven repository: https://mvnrepository.com/ (search for Spark)

	Downloading "winutils.exe" to your local directory
		=> Create the following folder: C:\hadoop\bin
		=> Download an appropriate version of winutils.exe from the following url;
			https://github.com/steveloughran/winutils


   Spark Core API
   --------------
	=> Is Spark's low-level API
		-> reponsible for memort-management, job-scheduling, fault-tolerence  etc.

        => The data is processed as RDDs.
	=> Used to process unstructured data.

     
   RDD (Resilient Distributed Data)
   ---------------------------------

	=> Is a collection of distributed in-memory partitions
		-> Each partition is a collection of objects of some type. 

	=> RDDs are immutable
		-> Once created, data of the partitions can not be changed.

	=> RDDs are lazily evaluated.
		-> Transformations does not cause execution. 
		-> Execution is triggered by action command.

        => RDDs can recreate missing in-memory partitions on the fly.


   Creating RDDs
   -------------
	Three ways:

	1. Create an RDD from some external data source (such as a text file)

		val rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

		val rddFile = sc.textFile("E:\\Spark\\wordcount.txt")
		=> The number of partitions is given by the value of "sc.defaultMinPartitions"



	2. Create an RDD from programmatic data (such as a Scala collection)

		val rdd1 = sc.parallelize( List(4,2,1,6,4,5,7,8,9,0,4,3,6,5,7,8,9,0,7,5,6,7,4), 3 )

		val rdd1 = sc.parallelize( List(4,2,1,6,4,5,7,8,9,0,4,3,6,5,7,8,9,0,7,5,6,7,4), 3 )
		=> The number of partitions is given by the value of "sc.defaultParallelism"
		   (equal to number of cores allocated)


	3. By applying transformations on existing RDDs.

	NOTE: rdd1.getNumPartitions give the number of partitions of rdd1.


   RDD Operations
   ---------------
    
	Two operations:

	1. Transformations	
		-> Returns an RDD
		-> Does not cause execution
		-> Transformation only create RDD Lineage DAGs

        2. Actions
		-> Trigger execution of the RDD
		-> Produces output
		-> Converts thr logical Plan into a physical execution plan.

   
   RDD Lineage DAG
   ---------------

     rdd1.toDebugString => prints the lineage of rdd1.
    
     RDD Lineage is a logical plan that tracks the tasks to be performed to compute the RDD partitions
     It has the heirarchy of dependent RDD all the way upto the very first RDD. 

	
	 val rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

(4) E:\Spark\wordcount.txt MapPartitionsRDD[5] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[4] at textFile at <console>:24 []

	val rddWords = rddFile.flatMap(x => x.split(" "))

(4) MapPartitionsRDD[9] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[5] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[4] at textFile at <console>:24 []

	val rddPairs = rddWords.map(x => (x, 1))

(4) MapPartitionsRDD[10] at map at <console>:25 []
 |  MapPartitionsRDD[9] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[5] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[4] at textFile at <console>:24 []


   	val rddWc = rddPairs.reduceByKey((a, b)=>a+b)

(4) ShuffledRDD[11] at reduceByKey at <console>:25 []
 +-(4) MapPartitionsRDD[10] at map at <console>:25 []
    |  MapPartitionsRDD[9] at flatMap at <console>:25 []
    |  E:\Spark\wordcount.txt MapPartitionsRDD[5] at textFile at <console>:24 []
    |  E:\Spark\wordcount.txt HadoopRDD[4] at textFile at <console>:24 []


  Types of Transformations
  -------------------------
	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD 


  RDD Persistence
  ---------------
	
	val rdd1 = sc.textFile(.....)
	val rdd2 = rdd1.t2(..)
	val rdd3 = rdd1.t3(..)
	val rdd4 = rdd3.t4(..)
	val rdd5 = rdd3.t5(..)
	val rdd6 = rdd5.t6(..)
	rdd6.persist(StorageLevel.MEMORY_AND_DISK)   ===> instruction to Spark to save RDD6 partitions.
	val rdd7 = rdd6.t7(..)

	rdd6.collect()
	
	Lineage of rdd6:  rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		(textFile, t3, t5, t6) => collect

	rdd7.collect()
	
	Lineage of rdd7:  rdd7 -> rdd6.t7
		(t7) => collect

        rdd6.unpersist()


	Storage Levels
	--------------
		Storage Formats => Serialized Format    (byte-streaming)
				   Deserialized Format  (object-format)

	   1. MEMORY_ONLY	  : (default) Memory Deserialized 1x Replicated
	   2. MEMORY_AND_DISK	  : Disk Memory Deserialized 1x Replicated
	   3. DISK_ONLY		  : Disk Serialized 1x Replicated
	   4. MEMORY_ONLY_SER     : Memory Serialized 1x Replicated
	   5. MEMORY_AND_DISK_SER : Disk Memory Serialized 1x Replicated
	   6. MEMORY_ONLY_2	  : Memory Deserialized 2x Replicated
	   7. MEMORY_AND_DISK_2	  : Disk Memory Deserialized 2x Replicated
	
				   
       Commands
       --------
	   => rdd1.cache()    => in-memory persistence  (MEMORY_ONLY)
	   => rdd1.persist()  => in-memory persistence  (MEMORY_ONLY)
	   => rdd1.persist(StorageLevel.MEMORY_AND_DISK)

	   => rdd1.unpersist()


   Execution memory structure
   --------------------------

	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


  RDD Transformations
  -------------------

   1. map			P: U => V
				Object to Object transformation
				input RDD: N objects, Output RDD: N objects

		rddWords.map( _.length ).collect
		rddFile.map(x => x.split(" ")).collect
		rddArr.map( a => a.map( _.toUpperCase ) ).collect()

  2. filter			P: U => Boolean
				Only those object for which the function returns true will be there in the output.
				input RDD: N objects, Output RDD: <= N objects

		rddWords.filter( x => x(0) == 'r' ).collect
 
  3. glom			P: None
				Return one Array object with all the elements of each partition.
				input RDD: N objects, Output RDD: = number of partitions

		rdd1			     rdd2 = rdd1.glom()
		P0: 2,4,1,4,5,6,7 -> glom -> P0: Array(2,4,1,4,5,6,7)
		P1: 9,0,5,7,8,1,6 -> glom -> P1: Array(9,0,5,7,8,1,6)
		P2: 6,5,8,9,0,5,0 -> glom -> P2: Array(6,5,8,9,0,5,0)

		rdd1.count = 21 (Int)		rdd2.count = 3 (Array[Int])

   4. flatMap			P: U => TraversableOnce[V]
				flatMap flattens the iterable produced by function.
				input RDD: N objects, Output RDD: >= N object

		rddFile.flatMap(x => x.split(" "))

   5. mapPartitions		P: Iterator[U] => Iterator[V]
				partition to partition transformation

		rdd1		rdd1.mapPartitions( p => List(p.max).iterator )
		P0: 2,4,1,4,5,6,7 -> mapPartitions -> P0: 7
		P1: 9,0,5,7,8,1,6 -> mapPartitions -> P1: 9
		P2: 6,5,8,9,0,5,0 -> mapPartitions -> P2: 9

		rdd1.mapPartitions( p => p.map(x => x%2 == 0) ).glom.collect


   6. mapPartitionsWithIndex	P: ( Int, Iterator[U] ) => Iterator[V]
				Similar to mapPartitions, but you get partition-index as an additional 
				function parameter.

		rdd1.mapPartitionsWithIndex( (i, p) => if (i == 1) p else List().iterator ).collect

   7. distinct			P: None, Optional: numPartitions
				Returns distinct elements of the input RDD into the output RDD.

		rddWords.flatMap(x => x).distinct.collect

   Types of RDDs
   -------------
	1. Generic RDDs : RDD[U]
	2. Pair RDDs    : RDD[(U, V)]


   8. mapValues			P: U => V
				Applied only on pair RDDs
				Transforms the value part of the pair RDD by applying the function

		rdd2.mapValues( x => (x,x) ).collect

   9. sortBy			P: U => V,   Optional: ascending (true/false), numPartitions
				The objects (of the output RDD) are sorted based on the function output

		rddWc.sortBy( x => x._2).collect
		rddWc.sortBy( x => x._2, false ).collect
		rdd1.sortBy(x => x%4, true, 2).glom.collect
 

      



   