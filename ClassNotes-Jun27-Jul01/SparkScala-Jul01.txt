
  Agenda (Spark using Scala)
  --------------------------
   -> Scala Refresher
   -> Spark - Basics & Architecture
   -> Spark Core API
	-> RDD Transformations and Actions
	-> Spark Shared Variable
        -> Spark-Submit
   -> Spark SQL
	-> DataFrame Operations
	-> Integration with MySQL & Hive
	-> SQL Optimizations & Tuning
   -> Spark Streaming
	-> DStreams API (introduction)
	-> Structured Streaming


  Materials
  ---------
	=> PDF Presentations
	=> Code Modules
	=> Class Notes
	=> Github: https://github.com/ykanakaraju/sparkscala

 
  Scala
  ===== 
     	=> SCAlable LAnguage
	
	=> Statically Typed Language
		-> Data type os a variable is fixed at compile time.
		-> Can not change the data type. 
		
	=> Scala is comiler-based language

   	=> Scala is a multi-paradigm Programming Lang.
		-> Scala is an OOP Lang
		-> Scala is Functional Programming

	=> Scala has type inference

	=> Scala is a Pure object oriented language. 
               -> Scala does not have primitives and operators

        => Scala mutables and immutables
		var => Multables: Can change  
		val => Immutable: Can not change once assigned.

		Scala prefers "Immutables"

		val i = 10
		i = 20  // Invalid
	
		var i = 10
		i = 20  // Valid
			

     Scala Blocks
     ------------
	=> A block is a set of statements and expressions enclosed in  { }
	=> A block has a return value 
		-> The return value of the block is the value of the last statement/expression that
		   is executed in that block.
	
		
     Scala "Unit" => is a Class whose object represent "no value".
		 => Printed as ()


     Scala Flow Control Constructs
     -----------------------------

	=> if .. else if .. else
        => match .. case

	
	=> if .. else consutruct returns a value
	   The return value is the value if the last executed statement of the executed block. 

		val x = 100
    		val y = 150
    		var z = 0
    
    		z = if ( x > y )  x - y  else if (x < y) y - x else x   

        => match..case consutruct returns a value
	   The return value is the value if the last executed statement of the executed block. 

		x match {
        		case 10 => z = 10
        		case 20 => z = 20
        		case a if (a % 2 == 0) => { z = 10 }
        		case _ => { z = -1 }        
    		} 

		z = x match {
       			case 10 => 10
        		case 20 => 20
        		case a if (a % 2 == 0) => {
          			val i = 10
          			val j = 20
          			i + j + a
        		}
        		case _ => -1        
    		}

    Loop Constructs
    ---------------

	-> while
	-> do .. while
	-> foreach

		<Iterable>.foreach(<function>)

		"scala".foreach( x => println("x = " + x) ) 
	-> for

	for loop: 
	---------

		for ( i <- 1 to 10 ) {
         		println(s"i: $i, j: $j")
      		}

		for ( i <- 1 to 10; j <- 1 to 10 by 2) {
         		println(s"i: $i, j: $j")
      		}

		for ( i <- 1 to 10 if (i%2 != 0); j <- 1 to 10 by 2 ) {
         		println(s"i: $i, j: $j")
      		}

		for ( i <- 1 to 10 if (i%2 != 0); j <- 1 to 10 by 2 if (i != j) ) {
         		println(s"i: $i, j: $j")
      		}

		for comprehension
		-----------------
		val v1 = for ( i <- 1 to 10 if (i%2 != 0)) yield(i*10)      
     		println( v1 )   // Returns Vector(10, 30, 50, 70, 90)


    Range class
    -----------
	Range(start, end, step) => generates Int values from start to (end-1) with a step

	Range(1, 10, 2) => 1,3,5,7,9
	Range(20, 0, -3) => 20, 17, 14, 11, 8, 5, 2
	Range(1, 10, -1) => empty
	Range(1, 10) => 1,2,3,4,5,6,,7,8,9 (default step is 1)

	1 to 10 	 => 1,2,3,4,5,6,7,8,9,10  (Range.inclusive)
	1 until 10 	 => 1,2,3,4,5,6,7,8,9     (Range)

	0 to 10 by 2	 => 0,2,4,6,8,10
	0 until 10 by 2  => 0,2,4,6,8

 
   Interpolators
   -------------

    s interpolator : s"x = $x, y = ${y + 10}"

    f interpolator : s interpolator + interpolate formatting characters 
		     f"x = $x%2.2f, y = $y%1.3f"

    raw interpolator : s interpolator + escape the escape characters
		  raw"x = $x%2.2f\ny = $y%1.3f"



    Exception Handling
    -------------------

	try {
		// some code that throws an exception
	}
	catch {
	    case e: FileNotFoundException => {
		   println( e.getMessage ) 
		}

	    case e: ArrayIndexOutOfBounds => { ... }
            case e: Exception => { ... }
	    case _ => { ... }
	}
	finally {
	     // some code that is always executed
	}


   Getting started with Scala
   ==========================

     1. Using your vLab

	  -> You connect to Windows server
	  -> Double Click on the Oracle VM Virtualbox icon
		-> Launch the Ubuntu VM
		   -> Open a terminal
		   -> Type "spark-shell"
		   -> Launch "Scala-IDE" for eclipse

     2. Setting up Scala development environment on your personal machine.

	 2.1 Scala IDE

	  	-> Make sure you have Java 8 (JDK 1.8.x)
		  (run "java -version" command at command prompt)
          	-> Download Scala IDE for Eclispe from http://scala-ide.org/download/sdk.html

         2.2 IntelliJ
		-> https://docs.scala-lang.org/getting-started/index.html

		=> Community Edition: https://www.jetbrains.com/idea/download/#section=windows
		=> Installing Scala Plug-in: https://www.jetbrains.com/help/idea/managing-plugins.html

		=> Working with Scala on IntelliJ:
		   -> https://docs.scala-lang.org/getting-started/intellij-track/building-a-scala-project-with-intellij-and-sbt.html


     3. Using Databricks Community Edition Free account
 
		Sign-up: https://databricks.com/try-databricks
	  	Login: https://community.cloud.databricks.com/login.html



     4. Using Online Scala Editors: https://scastie.scala-lang.org/pEBYc5VMT02wAGaDrfLnyw



    Scala Class Heirarchy
    ----------------------

       Any   => AnyVal => Int, Long, Double, Boolean, CHar, Unit
	     => AnyRef => String, List, Tuple, All other classes..


    Collections
    -----------
	=> Array	  -> Mutable & Fixed Length
	   ArrayBuffer	  -> Mutable & Variable Length

	=> Seq  : Ordered Collections
		  Elements can be invoked using an index.

		 => IndexedSeq : Optimized for random-access of data
			-> Range, Vector

		 => LinearSeq : Optimized for sequential-access (loops)
				These are linked lists
			-> List, Queue, Stream

	=> Set  : Unordered collection of unique objects

	=> Map : Is a collection of (Key, Value) pair


   Reading from File
   -----------------

	val lines = scala.io.Source.fromFile( <filePath> ).getLines.toSeq


    Option[U]
    ---------

	=> Represents a object which may or may not have a value.
	=> Returns Some[U] if value is present
		   None if value is not present
	

    Methods
    --------
	=> A method is a executable code block

	=> Methods can be called using positional parameter
	=> Methods can be called using named parameter
	=> methods can have multiple parameter lists
	=> Methods parameter can have one variable-lengh arguments and it has to be last argument.
	=> Method parameters can have default values.
	=> Methods can be called recusivly (recursive method)


    Procedures
    ----------
	=> Are like mathods, but they always return Unit


	def box(s: String) {
       	     val border = "-" * s.length() + "----"       
       	     println( border + "\n| " + s + " |\n" + border )
    	}

    Functions
    ---------
	=> Function are treated as literals
		Function literal: (a: Int, b: Int) => { a + b }
	=> Functions are anonymous by nature
	=> Function can be assigned to a variable

        Function literal				Type
	----------------------------------------------------------------
	(a: Int, b: Int) => a + b			(Int, Int) => Int
	(s: String) => s.toUpperCase			String => String
	(a: Int, b: String) => b * a			(Int, String) => String
	() =>  "Windows 10"				() => String
	(s: String) => print(s)				String => Unit
	(a: (Int, Int), b: Int) => (a._1+b, a._2+b)     ((Int, Int), Int) => (Int, Int)

	=> A function can be passed as a parameter to a method or function
	=> A block can return a function as a final value.
	=> A method / function can return a function as an output.

	def compute(op: String) = {
            op match {
         	case "+" => (a : Int, b: Int) => a + b
          	case "-" => (a : Int, b: Int) => a - b
          	case "*" => (a : Int, b: Int) => a * b
          	case _ => (a : Int, b: Int) => a % b
             }
     	}


   Higher Order Functions (HOF)
   ----------------------
    A HOF is a method or function that takes a function as a parameter or returns a function as a return value.

    1. map			P: U => V
   				Transforms each object of the input collection to a different object by appying 
				the function
				input: N object, output: N object 

		 l1.map(x => x > 8)
		 lines.map( s => s.split(" ") )

    2. filter			P: U => Boolean 
				Only those objects for which the function returns true will be in the output. 
				input: N object, output: <= N object 

		lines.filter(x => x.split(" ").length > 8)

    3. reduceLeft, reduceRight	P: (U, U) => U 
				Reduces the entire collection to one final value of the same time by iterativly
				applying the function.

		List(3,2,4,3,5,6).reduceLeft( (x, y) => x - y )
    		3,2,4,3,5,6 => 1,4,3,5,6 => -3,3,5,6 => -6,5,6 => -11,6 => -17

		List(3,2,4,3,5,6).reduceRight( (x, y) => x - y )
		3,2,4,3,5,6 => 3,2,4,3,-1 => 3,2,4,4 => 3,2,0 => 3,2 => 1		

		List[U].reduceLeft( (U,U) => U ) => U

		l2.reduceLeft( (x, y) => ( ((if (x._1 > y._1) x._1 else y._1), (if (x._2 > y._2) x._2 else y._2)) ))


    4. flatMap			P: U => GenTraversableOnce[V]  (fn output should be a collection object)
				flatMap flattens the function output to constituent elements.
				input: N object, output: >= N object 

				List[U].flatMap( U -> List[V] ) => List[V]

				val words = lines.flatMap(x => x.split(" "))


   5. sortWith			P: binary sorting function
				Elements of th collection are sorted based on the binary sorting function.  
				input: N object, output: N object 

			words.sortWith( (x, y) => x(0) < y(0) )

   6. groupBy			P: U => V
				Elements of the input collection are grouped based on the function output.
				Returns a Map object where:
					key : unique function output
					value: Collection containing elements that produced the key

			l2.groupBy(x => x._2)
			words.groupBy(x => x).toList.map(x => (x._1, x._2.length)).sortWith((x, y) => x._1 < y._1)


   7. foldLeft & foldRight	=> reduces the entire collection to a type that is different that the type of objects

			Two parameters as param-list
			
			1. zero-value : starting value of the type of output you want as output
			2. Sequence Function : is a function that iterativly merges all the objects of the 
			   collection with the zero-value. 

			List[U].foldLeft( (V, U) => V )  => V	
				
		l1.foldRight( (0,0) )( (v, z) => (z._1 + v, z._2 + 1) )


  8. mapValues		 	=> P: U => V
				   Operates only on Map objects
				   Will transform only the 'value' part of the key-value pairs.

		 m1.mapValues( x => x.length )


  Assignments 
  ===========
	=> Write a method to print the first N numbers in the fibonnaci series
	   function: printFib	
		printFib(7) => 1, 1, 2, 3, 5, 8, 13

        => Write a method to print the first N prime numbers		
	   function: printPrimes
		printPrimes(7) => 2, 3, 5, 7, 11, 13, 17

  Use-Case
  ========
   WordCount: Fetch the frequency of each unique word in a given text file.

   val output = Source.fromFile("E:\\Spark\\wordcount.txt")
                      .getLines()
                      .toList
                      .flatMap(x => x.split(" "))
                      .groupBy(x => x)
                      .mapValues(x => x.length)
                      .toSeq
                      .sortWith((a, b) => a._2 > b._2)

  ========================================================
       Spark
  ========================================================

    -> Spark is written in 'Scala'
  
    -> Spark is an in-memory distributed computing framework for big data analytics
       using well-defined programming constructs. 

    -> Spark is a Unifed Framework

         Spark provides a consistent set of APIs for processing different analytics workloads
	 based on the same execution engine.

	   => Batch Processing of unstructured data	: Spark Core API
	   => Batch Processing of structured data	: Spark SQL
	   => Stream processing (real-time)		: Structured Streaming, DStreams API
	   => Predictive analytics (using ML)		: Spark MLlib
           => Graph parallel computations		: Spark GraphX

     -> Spark is a polyglot
	  -> Spark apps can be written in : Scala, Java, Python, R (and SQL)
	

    Spark Architecture and Building Blocks
    --------------------------------------    
       1. Cluster Manager
		-> Jobs are submitted to CMs
		-> Allocates containers for lauching driver and executors on the cluster. 
		-> Spark supports multiple CMs
			-> local, standalone, YARN, Mesos, Kubernetes		

	2. Driver
		-> Master process
		-> Runs the SparkContext object
		-> Manages the user code and sends the tasks to the executors as per the code. 

		Deploy Modes:
		1. Client : default, driver runs on the client. 
		2. Cluster : driver runs on one of the nodes on the cluster.

	3. SparkContext
		-> Is an application context
		-> Starting point of execution
		-> Is the link between the driver and tasks running on the executors

	4. Executors
		-> receives the tasks from the Driver
		-> all tasks runs the same execution code but on different partitions of the data
		-> the status of tasks os reported to the driver.


   Getting started with Spark
   --------------------------	
	Popular Options : 
	
	=> Scale-IDE for Eclipse + Maven
		=> Add required dependencies from Central maven repo
		=> The required packges are download and added to your project.
	=> IntelliJ + SBT

        Central maven repository: https://mvnrepository.com/ (search for Spark)

	Downloading "winutils.exe" to your local directory
		=> Create the following folder: C:\hadoop\bin
		=> Download an appropriate version of winutils.exe from the following url;
			https://github.com/steveloughran/winutils


   Spark Core API
   --------------
	=> Is Spark's low-level API
		-> reponsible for memort-management, job-scheduling, fault-tolerence  etc.

        => The data is processed as RDDs.
	=> Used to process unstructured data.

     
   RDD (Resilient Distributed Data)
   ---------------------------------

	=> Is a collection of distributed in-memory partitions
		-> Each partition is a collection of objects of some type. 

	=> RDDs are immutable
		-> Once created, data of the partitions can not be changed.

	=> RDDs are lazily evaluated.
		-> Transformations does not cause execution. 
		-> Execution is triggered by action command.

        => RDDs can recreate missing in-memory partitions on the fly.


   Creating RDDs
   -------------
	Three ways:

	1. Create an RDD from some external data source (such as a text file)

		val rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

		val rddFile = sc.textFile("E:\\Spark\\wordcount.txt")
		=> The number of partitions is given by the value of "sc.defaultMinPartitions"

	2. Create an RDD from programmatic data (such as a Scala collection)

		val rdd1 = sc.parallelize( List(4,2,1,6,4,5,7,8,9,0,4,3,6,5,7,8,9,0,7,5,6,7,4), 3 )

		val rdd1 = sc.parallelize( List(4,2,1,6,4,5,7,8,9,0,4,3,6,5,7,8,9,0,7,5,6,7,4), 3 )
		=> The number of partitions is given by the value of "sc.defaultParallelism"
		   (equal to number of cores allocated)


	3. By applying transformations on existing RDDs.

	NOTE: rdd1.getNumPartitions give the number of partitions of rdd1.


   RDD Operations
   ---------------
    
	Two operations:

	1. Transformations	
		-> Returns an RDD
		-> Does not cause execution
		-> Transformation only create RDD Lineage DAGs

        2. Actions
		-> Trigger execution of the RDD
		-> Produces output
		-> Converts thr logical Plan into a physical execution plan.

   
   RDD Lineage DAG
   ---------------

     rdd1.toDebugString => prints the lineage of rdd1.
    
     RDD Lineage is a logical plan that tracks the tasks to be performed to compute the RDD partitions
     It has the heirarchy of dependent RDD all the way upto the very first RDD. 

	
	 val rddFile = sc.textFile("E:\\Spark\\wordcount.txt", 4)

(4) E:\Spark\wordcount.txt MapPartitionsRDD[5] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[4] at textFile at <console>:24 []

	val rddWords = rddFile.flatMap(x => x.split(" "))

(4) MapPartitionsRDD[9] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[5] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[4] at textFile at <console>:24 []

	val rddPairs = rddWords.map(x => (x, 1))

(4) MapPartitionsRDD[10] at map at <console>:25 []
 |  MapPartitionsRDD[9] at flatMap at <console>:25 []
 |  E:\Spark\wordcount.txt MapPartitionsRDD[5] at textFile at <console>:24 []
 |  E:\Spark\wordcount.txt HadoopRDD[4] at textFile at <console>:24 []


   	val rddWc = rddPairs.reduceByKey((a, b)=>a+b)

(4) ShuffledRDD[11] at reduceByKey at <console>:25 []
 +-(4) MapPartitionsRDD[10] at map at <console>:25 []
    |  MapPartitionsRDD[9] at flatMap at <console>:25 []
    |  E:\Spark\wordcount.txt MapPartitionsRDD[5] at textFile at <console>:24 []
    |  E:\Spark\wordcount.txt HadoopRDD[4] at textFile at <console>:24 []


  Types of Transformations
  -------------------------
	1. Narrow Transformations
	    -> Does not cause shuffling of the data from one partition to other partitions
	    -> Partition to partition transformations
	    -> The output RDD will have the same number of partitions as input RDD

	2. Wide Transformation
	    -> Causes shuffling of the data
	    -> One output partition may need data from multiple input partitions
	    -> The output RDD may have different number of partitions than input RDD 


  RDD Persistence
  ---------------	
	val rdd1 = sc.textFile(.....)
	val rdd2 = rdd1.t2(..)
	val rdd3 = rdd1.t3(..)
	val rdd4 = rdd3.t4(..)
	val rdd5 = rdd3.t5(..)
	val rdd6 = rdd5.t6(..)
	rdd6.persist(StorageLevel.MEMORY_AND_DISK)   ===> instruction to Spark to save RDD6 partitions.
	val rdd7 = rdd6.t7(..)

	rdd6.collect()
	
	Lineage of rdd6:  rdd6 -> rdd5.t6 -> rdd3.t5 -> rdd1.t3 -> sc.textFile
		(textFile, t3, t5, t6) => collect

	rdd7.collect()
	
	Lineage of rdd7:  rdd7 -> rdd6.t7
		(t7) => collect

        rdd6.unpersist()


	Storage Levels
	--------------
	   Storage Formats => Serialized Format    (byte-streaming)
			      Deserialized Format  (object-format)

	   1. MEMORY_ONLY	  : (default) Memory Deserialized 1x Replicated
	   2. MEMORY_AND_DISK	  : Disk Memory Deserialized 1x Replicated
	   3. DISK_ONLY		  : Disk Serialized 1x Replicated
	   4. MEMORY_ONLY_SER     : Memory Serialized 1x Replicated
	   5. MEMORY_AND_DISK_SER : Disk Memory Serialized 1x Replicated
	   6. MEMORY_ONLY_2	  : Memory Deserialized 2x Replicated
	   7. MEMORY_AND_DISK_2	  : Disk Memory Deserialized 2x Replicated
	
				   
       Commands
       --------
	   => rdd1.cache()    => in-memory persistence  (MEMORY_ONLY)
	   => rdd1.persist()  => in-memory persistence  (MEMORY_ONLY)
	   => rdd1.persist(StorageLevel.MEMORY_AND_DISK)

	   => rdd1.unpersist()


   Execution memory structure
   --------------------------

	Reference URL: https://spark.apache.org/docs/latest/configuration.html

	Let us assume, we request executors with 10 GB RAM.
	
	-> Cluster Manager allocates exectors with 10.3 GB RAM

	1. Reserved Memory  : 300 MB
		-> Spark's internal usage. 

	2. Spark Memory (spark.memory.fraction: 0.6) => 6 GB
		-> Used for RDD execution and storage

		2.1 Execution Memory
			-> Used for execution of RDD tasks and creating RDD partitions. 

		2.2 Storage Memory (spark.memory.storageFraction = 0.5)  => 3 GB
			-> Used for RDD persistence and storing broadcast variables.

            -> Storage memory can not evict execution memory even if execution memory is
               using more than its 3 GB limit. It has to wait until more memory becomes 
	       available.

	    -> Execution memory can evict some partitions from storage, if it requires more
	       memory. But, it can evict only excess portion that is used by storage beyond its
 	       3 GB limit. 

	3. User Memory (1 - spark.memory.fraction = 0.4) => 4 GB
		-> Used for user code (Python/Scala/Java etc)


  RDD Transformations
  -------------------

   1. map			P: U => V
				Object to Object transformation
				input RDD: N objects, Output RDD: N objects

		rddWords.map( _.length ).collect
		rddFile.map(x => x.split(" ")).collect
		rddArr.map( a => a.map( _.toUpperCase ) ).collect()

  2. filter			P: U => Boolean
				Only those object for which the function returns true will be there in the output.
				input RDD: N objects, Output RDD: <= N objects

		rddWords.filter( x => x(0) == 'r' ).collect
 
  3. glom			P: None
				Return one Array object with all the elements of each partition.
				input RDD: N objects, Output RDD: = number of partitions

		rdd1			     rdd2 = rdd1.glom()
		P0: 2,4,1,4,5,6,7 -> glom -> P0: Array(2,4,1,4,5,6,7)
		P1: 9,0,5,7,8,1,6 -> glom -> P1: Array(9,0,5,7,8,1,6)
		P2: 6,5,8,9,0,5,0 -> glom -> P2: Array(6,5,8,9,0,5,0)

		rdd1.count = 21 (Int)		rdd2.count = 3 (Array[Int])

   4. flatMap			P: U => TraversableOnce[V]
				flatMap flattens the iterable produced by function.
				input RDD: N objects, Output RDD: >= N object

		rddFile.flatMap(x => x.split(" "))

   5. mapPartitions		P: Iterator[U] => Iterator[V]
				partition to partition transformation

		rdd1		rdd1.mapPartitions( p => List(p.max).iterator )
		P0: 2,4,1,4,5,6,7 -> mapPartitions -> P0: 7
		P1: 9,0,5,7,8,1,6 -> mapPartitions -> P1: 9
		P2: 6,5,8,9,0,5,0 -> mapPartitions -> P2: 9

		rdd1.mapPartitions( p => p.map(x => x%2 == 0) ).glom.collect


   6. mapPartitionsWithIndex	P: ( Int, Iterator[U] ) => Iterator[V]
				Similar to mapPartitions, but you get partition-index as an additional 
				function parameter.

		rdd1.mapPartitionsWithIndex( (i, p) => if (i == 1) p else List().iterator ).collect

   7. distinct			P: None, Optional: numPartitions
				Returns distinct elements of the input RDD into the output RDD.

		rddWords.flatMap(x => x).distinct.collect

   Types of RDDs
   -------------
	1. Generic RDDs : RDD[U]
	2. Pair RDDs    : RDD[(U, V)]


   8. mapValues			P: U => V
				Applied only on pair RDDs
				Transforms the value part of the pair RDD by applying the function

		rdd2.mapValues( x => (x,x) ).collect

   9. sortBy			P: U => V,   Optional: ascending (true/false), numPartitions
				The objects (of the output RDD) are sorted based on the function output

		rddWc.sortBy(x => x._2).collect
		rddWc.sortBy(x => x._2, false).collect
		rdd1.sortBy(x => x%4, true, 2).glom.collect

  10. groupBy			P: U => V, Optional: numPartitions
				Returns a Pair RDD, where 
				    key: Each unique value of the function output
				    value: CompactBuffer with the objects that produced the key

				RDD[U].groupBy(U => V) => RDD[(V, CompactBuffer[U])]
  
 		rdd1.groupBy(x => x%5, 2).glom.collect

		 val rdd1 = sc.textFile("E:\\Spark\\wordcount.txt")
                   	.flatMap(x => x.split(" "))
                   	.groupBy( x => x )
                  	.mapValues( x => x.toList.length )


  11. randomSplit		P: Array of weights (e.g: Array(0.6, 0.4)), Optional: seed
				Returns an array of RDDs split randomly in the specified weights.

		val rddArr = rdd1.randomSplit( Array(0.5, 0.5))
		val rddArr = rdd1.randomSplit( Array(0.5, 0.5), 53345 )  //53345 is a seed

  12. repartition		P: numPartition	
				Is used to increase or decrease the number of output partitions
				Performs global shuffle

  13. coalesce 			P: numPartition	
				Is only decrease the number of output partitions
				Performs partition merging

	Recommandations (Databricks)
	-----------------------------
	1. The size of the partition should be around 128 MB
	2. The number of partitions should be a multiple of number of CPU cores allocated
	3. If the number of partitions is less than but close to 2000, bump it up to 2000
	4. The number of cores in each executor should be 5


  14. union, intersection, subtract, cartesian
				
		Let us say, rdd1 has M partitions and rdd2 has N partitions

		command					number of output partitions
                --------------------------------------------------------------------
		rdd1.union(rdd2)			M + N, narrow
		rdd1.intersection(rdd2)			bigger of M & N, wide
		rdd1.subtract(rdd2)			M (equal to input RDDs partitions), wide
		rdd1.cartesian(rdd2)			M * N	


  15. partitionBy		P: partitioner
				Applied only to pair RDDs
				Partitioning happens based on the 'key'
				Is used to control which data goes to which partition.

		Built-in partitioners:

		1. Range Partitioner

			val rangePartitioner = new RangePartitioner(5, rdd)    
    			val partitionedRdd = rdd.partitionBy(rangePartitioner)

		2. Hash Partitioner

			val hashPartitioner = new HashPartitioner(3)    
    			val partitionedRdd = rdd.partitionBy(hashPartitioner)

		3. Custom partitioner
			=> User can define custom partitioning logic.

  ..ByKey Transformation
  ----------------------
	=> Wide transformations
	=> Applied only to Pair RDDs

  16. sortByKey			P: None, Optional: ascending (true/false), numPartitions
				Sorts the objects of the RDD based on the key.

		rddPairs.sortByKey().collect()
		rddPairs.sortByKey(false).collect()
		rddPairs.sortByKey(false, 4).collect()

  17. groupByKey		P; None, Optional: numPartitions
				Returns a Pair where with unique-keys and grouped values.
				
                *** NOTE: AVOID groupByKey, if you can ***

 		val rdd1 = sc.textFile("E:\\Spark\\wordcount.txt")
                   	.flatMap(x => x.split(" "))
                   	.map(x => (x, 1) )
                   	.groupByKey()
                   	.mapValues(x => x.sum)
                   	.sortByKey()

  18. reduceByKey		P: (U, U) -> U
				Reduce all the values of each unique key to one value of the same type

		val rdd1 = sc.textFile("E:\\Spark\\wordcount.txt", 4)
                   	.flatMap(x => x.split(" "))
                   	.map(x => (x, 1) )
                   	.reduceByKey( (a, b) => a + b )
                   	.sortByKey(true, 1)

  19. aggregateByKey	       => Used to reduce all the values of each unique key to a type which is different
				  than the type of the value part of  K, v) pairs

			 RDD[U] => aggregate => V

			 Three parameters (mandatory):

			 1. zero-value : starting value of the type of the output you want to produce
			 2. Sequence Function: merges all the objects of the RDD in each partition with the zero
				   -> produces one output (of the type of ZV) per partition
			 3. Combine Function: reduces the outputs each partition to one final value.

			 4. (optional) : numPartitions

	val students_list = List(
  		("Aditya", "Maths", 83), ("Aditya", "Physics", 74), ("Aditya", "Chemistry", 91), ("Aditya", "English", 82), 
  		("Amrita", "Maths", 69), ("Amrita", "Physics", 62), ("Amrita", "Chemistry", 97), ("Amrita", "English", 80), 
  		("Pranav", "Maths", 78), ("Pranav", "Physics", 73), ("Pranav", "Chemistry", 68), ("Pranav", "English", 87), 
  		("Keerti", "Maths", 87), ("Keerti", "Physics", 93), ("Keerti", "Chemistry", 91), ("Keerti", "English", 74), 
  		("Harsha", "Maths", 56), ("Harsha", "Physics", 65), ("Harsha", "Chemistry", 71), ("Harsha", "English", 68), 
  		("Anitha", "Maths", 86), ("Anitha", "Physics", 62), ("Anitha", "Chemistry", 75), ("Anitha", "English", 83), 
  		("Komala", "Maths", 63), ("Komala", "Physics", 69), ("Komala", "Chemistry", 64), ("Komala", "English", 60))
  
      def seqFn(z: (Int, Int), v: Int) =  (z._1 + v, z._2 + 1) 
      def combFn(a: (Int, Int), b: (Int, Int)) = (a._1 + b._1, a._2 + b._2)           
           
       val rdd1 = sc.parallelize(students_list, 3)
                    .map( t => (t._1, t._3) )
                    .aggregateByKey((0,0))(seqFn, combFn)
                    .mapValues( x => x._1.toDouble / x._2 ) 
                    
                    
       rdd1.collect().foreach( println )


    20. joins => join, leftOuterJoin, rightOuterJoin, fullOuterJoin
		Joins two piar RDDs with same type of key.

		RDD[(K, V)].join( RDD[(K, W)] ) => RDD[ (K, (V,W)) ]


		RDD[(String, Int)].join( RDD[(String, String)] ) => RDD[ (String, (Int,String)) ]
		RDD[(String, Int)].leftOuterjoin( RDD[(String, String)] ) => RDD[ (String, (Int, Option[String])) ]

    21. cogroup			=> groupByKey -> fullOuterJoin

			rdd1:  (key1,1), (key2,2), (key3,2), (key2,2), (key1,3), (key5,5)


			rdd2:  (key1,5), (key2,4),(key4,4),(key2,4),(key3,8)

			rdd1.cogroup(rdd2)
				(key4,(CompactBuffer(),CompactBuffer(4)))
				(key5,(CompactBuffer(5),CompactBuffer()))
				(key2,(CompactBuffer(2, 2),CompactBuffer(4, 4)))
				(key3,(CompactBuffer(2),CompactBuffer(8)))
				(key1,(CompactBuffer(1, 3),CompactBuffer(5)))

 RDD Actions 
 -----------

   1. collect

   2. count

   3. saveAsTextFile

   4. reduce		  P: (U, U) => U
			  Reduces an entire RDD into one fincal of the "same type" by iterativly
			  running the reduce function.

			 RDD[U] => reduce => U
		rdd1
		P0: 7, 3, 4, 2, 1, 4, 4	    -> reduce -> 25 -> reduce => 102
		P1: 6, 7, 8, 9, 0, 8, 9	    -> reduce -> 47
		P2: 0, 5, 6, 7, 5, 4, 3, 0  -> reduce -> 30

   5. aggregate		 Used to reduce an entire RDD to a type which is different then type of the RDD

			 RDD[U] => aggregate => V

			 Three parameter:

			 1. zero-value : starting value of the type of the output you want to produce
			 2. Sequence Function: merges all the objects of the RDD in each partition with the zero
				   -> produces one output (of the type of ZV) per partition
			 3. Combine Function: reduces the outputs each partition to one final value.					

   6. first

   7. take

   8. takeOrdered
		rdd1.takeOrdered(10)
		rdd1.takeOrdered(10)(Ordering[Int].reverse)

   9. takeSample
		rdd1.takeSample(true, 10)   	 // with replacement sampling
		rdd1.takeSample(true, 10, 466)   // 466 is a seed

		dd1.takeSample(false, 10)   	 // with out replacement sampling
		rdd1.takeSample(false, 10, 466)   // 466 is a seed

   10. countByValue

   11. countByKey

   12. forEach	P: some function
		Runs the function on all objects of the RDD.

 		rdd10.foreach(x => println(s"key: ${x._1}, value: ${x._2}"))

   13. saveAsSequenceFile

   14. saveAsObjectFile



   Closure
   =======
      A closure in Spark constitues all the variables and methods that must be visible inside a executor
      for the tasks to perform their operation on RDDs. 
   
      The driver serializes this code and separate copy of closure is sent to every executor.

             
              var c = 0     // counter

	      def isPrime(n: Int) : Boolean = (2 until n).forall(x => n % x != 0)

	      def f1(n: Int) = {
		   if ( isPrime(n) ) c += 1
                   n * 2
	      }

              val rdd1 = sc.parallelize( 1 to 4000, 4 )
	      val rdd2 = rdd1.map( f1 )

	      rdd2.collect()

              println(s"Number of primes: $c")    // c = 0


          Limitation: Local variables can not be used to implement global counter.
	  Solution:  Use "Accumulator" variable
      

   Shared Variables
   ================

	Two Shared Variables:

	1. Accumulator Variable
	
	    -> is a shared variable
	    -> is not part of Closure, hence not a local copy at task level
	    -> All the tasks can add to this one copy.
	    -> Maintained by the driver
	    -> Used to implement global counters. 

	      var c = sc.longAccumulator("counter")

	      def isPrime(n: Int) : Boolean = (2 until n).forall(x => n % x != 0)

	      def f1(n: Int) = {
		   if ( isPrime(n) ) c.add(1)
                   n * 2
	      }

              val rdd1 = sc.parallelize( 1 to 4000, 4 )
	      val rdd2 = rdd1.map( f1 )

	      rdd2.collect()

              println(s"Number of primes: ${c.value}")    // c = 0


	2. Broadcast Variable

		=> A large immutable collection (such as a lookup table) can be broadcasted to every executor
		   node instead of having that as a closure variable. 

		=> This saves a lot of execution memory as we have only one copy per node (instead of per task)

	
		case class Emp(id: Int) { ... }	

		val m1 : Map[Int, Emp] = Map( 1 -> Emp(1), 2 -> Emp(2), 3 -> Emp(3), ... )    // 100 MB

		val bcM1 = sc.broadcast( m1 )

		def getEmp( id: Int ) : Emp => bcM1.value( id )

		val rdd1 : Int = sc.parallelize( List(1,2,3,4,5,6,7,8, ....), 2 )

		val rdd2 : Emp = rdd1.map( getEmp )

		rdd2.collect()


    Use-case
    --------
    Solve the following using RDD API

    Dataset: https://github.com/ykanakaraju/sparkscala/blob/master/SparkCore/data/cars.tsv
    
    From cars.tsv file find out the average weight of all models of each make of American origin cars.
    -> Arrange the data in the DESC order of average weight
    -> Save the output as a single text file.

	=> Try it yourself

     
   ================================================
       Spark-Submit command
   ================================================
   		
	=> Is a single command that is used to submit any spark application (Scala, Java, Python, R)
           to submit to any cluster manager (local, standalone scheduler, YARN, Mesos, Kubernetes)

	   spark-submit --master yarn
		--deploy-mode cluster
		--driver-memory 2G
		--driver-cores 2
		--executor-memory 5G
		--executor-cores 5
		--num-executors 10
	        --class <qualified class name>
		<jar-file-path> [app-args]



    ===============================================
	Spark SQL  (org.apache.spark.sql)
    ===============================================

    => Is a high-level API built on top of Spark Core
  
    => Spark's structured data processing API

		Structured fle formats: Parquet (default), ORC, JSON, CSV (delimited text file)
			   JDBC format: RDBMS, NoSQL databases
			   Hive Format: Hive warehouse

    => SparkSession
	   -> Starting point of execution for SPark SQL apps. 
	   -> Represents a user-session running inside an application (sparkContext)
	   -> We can have multiple SparkSession objects inside an application

	   val spark = SparkSession
              		.builder
              		.master("local[2]")              
              		.appName("DataSourceBasic")
              		.getOrCreate()  

     => Spark SQL Data Abstarctions:

   	=> Dataset
		=> Collection of typed objects

        => DataFrame
		=> Collection of "Row" objects
		=> Alias of Dataset[Row]

	=> Dataset and DataFrame have two components:
		-> Data : Row objects (in the case of DFs), or any other object (in the case of Dataset)
		-> Schema : StructType object

		StructType(
			StructField(age,LongType,true), 
			StructField(gender,StringType,true), 
			StructField(id,LongType,true), 
			StructField(name,StringType,true)
		)	


   Basic Steps in a Spael SQL Applications
   ---------------------------------------

     1. Creating a DataFrame from some source.

		val df1 = spark.read.format("json").load(inputPath)
		val df1 = spark.read.json(inputPath)

     2. Apply transformations on the DF using DF Transformation methods or using SQL

		Using DF Transformation methods
		-------------------------------

			val df2 = df1.select("id", "name", "age")
                 			.where("age is not null")
                 			.orderBy("age", "name")
                 			.groupBy("age").count()
                 			.orderBy("age")
                 			.limit(5)
		Using SQL
		---------
			df1.createOrReplaceTempView("people")
     			spark.catalog .listTables().show()                    
    
     			val qry = """select age, count(*) as count
                  			from people
                  			where age is not null
                  			group by age
                  			order by age
                  			limit 5"""
     
     			val df3 = spark.sql(qry)

			val peopleDf = spark.table("people")  // select all the content of people table


     3. Save the DF to some file/DB etc.

		 df3.write.format("json").save(outputPath)
		 df3.write.json(outputPath)
		 df3.write.mode( SaveMode.Overwrite ).json(outputPath)

   Save Modes
   ----------
	1. ErrorifExists (default)
	2. Ignore
	3. Append
	4. Overwrite

	df3.write.mode( SaveMode.Overwrite ).format("json").save(outputPath)

   
     Local Temp Views
     -----------------
      df.createOrReplaceTempView("flights")
      spark.catalog.listTables().show()
     
      val qry = """select * from flights
                   where count > 100
                   order by count desc 
                   limit 30"""
     
      val df2 = spark.sql(qry)
      df2.show()

		
     Global temp Views
     -----------------
     df.createGlobalTempView("g_flights")
     spark.catalog.listTables().show()
     
     val qry = """select * from global_temp.g_flights
                   where count > 100
                   order by count desc 
                   limit 30"""
     
     val df2 = spark.sql(qry)
     df2.show()     
     
     val spark2 = spark.newSession()
     spark2.catalog.listTables().show()
     
     val df3 = spark2.sql(qry)
     df3.show()

   
   DataFrame Transformations
   -------------------------
   1. select

	    	val df2 = df1.select("ORIGIN_COUNTRY_NAME", "DEST_COUNTRY_NAME", "count")
     		df2.show(5)

		val df2 = df1.select( column("DEST_COUNTRY_NAME"),
                           col("DEST_COUNTRY_NAME"),
                           expr("DEST_COUNTRY_NAME"),
                           $"DEST_COUNTRY_NAME",
                           'DEST_COUNTRY_NAME
                         )

		val df2 = df1.select( column("DEST_COUNTRY_NAME") as "destination",
                           col("ORIGIN_COUNTRY_NAME") as "origin",
                           expr("count"),
                           expr("count + 10 as newCount"),
                           expr("count > 200 as highFrequncy"),
                           expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic"))

   2. where / filter

		val df3 = df2.where("domestic = false and count > 1000")
		val df3 = df2.filter("domestic = false and count > 1000")

		val df3 = df2.where( col("count") > 1000 )
		val df3 = df2.filter( col("count") > 1000 )

   3. orderBy / sort

		val df3 = df2.orderBy("count", "destination")
		val df3 = df2.sort("count", "destination")

		val df3 = df2.orderBy(col("count").desc, col("destination").asc)
		val df3 = df2.orderBy(desc("count"), asc("destination"))

		val df3 = df2.sort(col("count").desc, col("destination").asc)
		val df3 = df2.sort(desc("count"), asc("destination"))


   4. groupBy  => returns RelationalGroupedDataset
		  you have to apply an aggregation method to return a DF

		val df3 = df2.groupBy("domestic", "highFrequncy").count()
		val df3 = df2.groupBy("domestic", "highFrequncy").sum("count")
		val df3 = df2.groupBy("domestic", "highFrequncy").max("count")
		val df3 = df2.groupBy("domestic", "highFrequncy").avg("count")

		 val df3 = df2.groupBy("domestic", "highFrequncy")
                  		.agg( count("count") as "count",
                        		sum("count") as "sum",
                        		max("count") as "max",
                        		avg("count") as "avg" )

   5. limit
		val df2 = df1.limit(10)

   6. selectExpr

		val df2 = df1.selectExpr( "DEST_COUNTRY_NAME as destination",
                           "ORIGIN_COUNTRY_NAME as origin",
                           "count",
                           "count + 10 as newCount",
                           "count > 200 as highFrequncy",
                           "DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME as domestic")

   7. withColumn & withColumnRenamed

 		val df3 = df1.withColumn("newCount", col("count") + 10 )
                  	.withColumn("highFrequncy", expr("count > 200") )
                  	.withColumn("domestic", expr("DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME") )
                  	.withColumnRenamed("DEST_COUNTRY_NAME", "destination")
                  	.withColumnRenamed("ORIGIN_COUNTRY_NAME", "origin")

		------------------------------------------

		val df3 = usersDf.withColumn("ageGroup", when(col("age") <= 12, "child")
                                              		.when(col("age") <= 19, "teenager")
                                              		.when(col("age") < 60, "adult")
                                              		.otherwise("senior") )

		-----------------------------------------
		val case_when = """case 
                           when age <= 12 then 'child'
	                         when age <= 19 then 'teenager' 
	                         when age < 60 then 'adult'
	                         else 'senior' 
	                      end"""
   
     		val df3 = usersDf.withColumn("ageGroup", expr(case_when))

    8. udf

		val get_age_group = (age: Int) => {
        		if (age <= 12) "child"
        		else if (age <= 19) "teenager"
        		else if (age < 60) "adult"
        		else "senior"            
     		}
    
     		val get_age_group_udf = udf(get_age_group)     
     		val df3 = usersDf.withColumn("ageGroup", get_age_group_udf(col("age")) )

                --------------------------------------------

		usersDf.createOrReplaceTempView("users")     
     		val df4 = spark.table("users")     
     		df4.show()
     
     		spark.udf.register("getAgeGroup", get_age_group) 
     		spark.sql("select id, name, age, getAgeGroup(age) as ageGroup from users").show()

   9. drop	=> excludes columns in the output dataframe.

		val df3 = df2.drop("newCount", "highFrequncy")     
     		df3.show(5)

   10. na  (DataFrameNaFunctions) => delete the rows with null values/fill null values with a given values

		val users4Df = spark.read.json("E:\\PySpark\\data\\users.json")
     		users4Df.show()
     
     		users4Df.na.drop( Seq("phone", "age") ).show()    
     		users4Df.na.fill("No Value").na.fill(0).show()

   11. dropDulicates

		val users3 = Seq((1, "Raju", 5),
                     (1, "Raju", 5),
                     (3, "Raju", 5),
                     (4, "Raghu", 35),
                     (4, "Raghu", 35),
                     (6, "Raghu", 35),
                     (7, "Ravi", 70))

    		val users3Df = spark.createDataFrame(users3).toDF("id", "name", "age") 
    		users3Df.show()

		users3Df.dropDuplicates().show()
    		users3Df.dropDuplicates("name", "age").show()

   12. distinct

		users3Df.distinct().show()

   13. randomSplit

		val dfArr = df1.randomSplit( Array(0.5, 0.5), 46456 )
		println( dfArr(0).count, dfArr(1).count )

   14. sample

		val df2 = df1.sample(true, 0.5)         // withReplacement sampling
		val df2 = df1.sample(true, 0.5, 456)	// 456 is a seed
		val df2 = df1.sample(true, 2.5, 456)    // fraction > 1 is allowed

		val df2 = df1.sample(false, 0.6, 456)	// withOutReplacement sampling
		val df2 = df1.sample(false, 1.6, 456)	// ERROR - fraction > 1 NOT allowed

   15, union, intersect

		val df2 = df1.where("count > 1000")
     		df2.show()
     		println("count > 1000 => " + df2.count())   // 14
     
     		val df3 = df1.where("ORIGIN_COUNTRY_NAME = 'India'")
     		df3.show()
     		println("ORIGIN_COUNTRY_NAME = 'India' => " + df3.count())   // 1
     
     		println( df2.rdd.getNumPartitions, df3.rdd.getNumPartitions )
     
     
     		val df4 = df2.union(df3)     
     		println( df4.rdd.getNumPartitions, df4.count )
     		df4.show()
     
     		val df5 = df4.intersect(df3)
     		println( df5.rdd.getNumPartitions, df5.count )
     		df5.show()     

   16. repartition

		val df2 = df1.repartition(4)
     		println( df2.rdd.getNumPartitions )
     
     		val df3 = df2.repartition(3)
     		println( df3.rdd.getNumPartitions )
     
     		val df4 = df1.repartition(5, col("ORIGIN_COUNTRY_NAME"))
     		println( df4.rdd.getNumPartitions )
     
     		val df5 = df1.repartition(col("ORIGIN_COUNTRY_NAME"))
     		println( df5.rdd.getNumPartitions )

		NOTE: You can set the default shuffle partition count using the following config:
		=> spark.conf.set("spark.sql.shuffle.partitions", "10")

   17. coalesce
		val df3 = df2.coalesce(2)

   18. join => discussed as a separate topics


   Working with different file formats 
   -----------------------------------
	1. JSON

		Read
		----
		val df1 = spark.read.format("json").load("people.json")
		val df1 = spark.read.json(inputFile)

		Write
		-----
		df2.write.format("json").save(outputDir)
		df2.write.json(outputDir)
		df2.write.mode(SaveMode.Overwrite).format("json").save(outputDir)

	2. CSV

		Read
		----
		val df1 = spark.read.format("csv").option("header", true).option("inferSchema", true).load(inputFile)  

		Write
		-----
		df2.write.format("csv").save(outputDir)
		df2.write.mode(SaveMode.Overwrite).format("csv").save(outputDir)
		df2.write.mode(SaveMode.Overwrite).option("header", true).format("csv").save(outputDir)
		df2.write.option("header", true).option("sep", "\t").format("csv").save(outputDir)
     

	3. Parquet (default)

		Read
		----
		val df1 = spark.read.format("parquet").load(inputFile) 
		val df1 = spark.read.parquet(inputFile)  

		Write
		-----
		df2.write.save(outputDir)   // default format is parquet   
		df2.write.format("parquet").save(outputDir)
		df2.write.parquet(outputDir)


	4. ORC

		Read
		----
		val df1 = spark.read.format("orc").load(inputFile) 
		val df1 = spark.read.orc(inputFile)  

		Write
		-----
		df2.write.format("orc").save(outputDir)
		df2.write.orc(outputDir)

   Creating a RDD from a DataFrame 
   --------------------------------
	val rdd1 = df1.rdd     
     	rdd1.take(5).foreach(println)


   Creating a DataFrame from programmatic data
   -------------------------------------------

	 val listUsers = Seq((1, "Raju", 5),
                       (2, "Ramesh", 15),
                       (3, "Rajesh", 18),
                       (4, "Raghu", 35),
                       (5, "Ramya", 25),
                       (6, "Radhika", 35),
                       (7, "Ravi", 70))
                       
     	 val df2 = spark.createDataFrame(listUsers).toDF("id", "name", "age") 

       ---------------------------------------------------------------------

	 import spark.implicits._
	 val df2 = listUsers.toDF("id", "name", "age") 


   Creating a DataFrame from an RDD
   --------------------------------
	val rdd1 = spark.sparkContext.parallelize(listUsers)    
     	val df2 = rdd1.toDF("id", "name", "age")

    
   Creating a DataFrame using Programmatic schema
   -----------------------------------------------
	val mySchema = StructType( Array(
                       StructField("ORIGIN_COUNTRY_NAME", StringType, true),
                       StructField("DEST_COUNTRY_NAME", StringType, true),
                       StructField("count", IntegerType, true)
                     ) )      
       
     	val df1 = spark.read.schema(mySchema).json(inputPath)   


   Joins
   ======

     Supported Joins => inner, left/left_outer, right/right_outer, full/full_outer, left_semi, left_anti

	
      left_semi join
      ---------------
	Like inner join, but only left table's data will be there in the output
	
	Equivalent to the following sub-query:

		=> select * from emp where deptid IN (select id from dept)

      left_anti join
      ---------------
	Equivalent to the following sub-query:

		=> select * from emp where deptid NOT IN (select id from dept)

     ----------------------
	val employee = Seq(
        	(1, "Raju", 25, 101),
        	(2, "Ramesh", 26, 101),
        	(3, "Amrita", 30, 102),
        	(4, "Madhu", 32, 102),
        	(5, "Aditya", 28, 102),
        	(6, "Aditya", 28, 10000)
       	).toDF("id", "name", "age", "deptid")
    
    	val department = Seq(
        	(101, "IT", 1),
        	(102, "Opearation", 1),
        	(103, "HRD", 2)).toDF("id", "deptname", "locationid")
        
    	employee.show()
    	department.show()
    
    	val joinCol = employee.col("deptid") === department.col("id")
    
    	val joinedDf = employee.join(department, joinCol, "left_anti")
    ----------------------------------


    => Join Strategies:
	   -> Shuffle Join (Big Table to Big Table)
		-> Shuffle-Hash Joins (HASH)
		-> Sort-Merge Join    (MERGE)

	   -> Broadcast Joins (Big Table to Small Table)

	   -> Small Table is defined as a table/DF that is smaller than the value
	      defined by autoBroadcastJoinThreshold parameter (def: 10 MB)
     

     	    => Explicit Broadcast Join:
		    employee.join(broadcast(department), joinEmpDept, "inner")


   Optimizations & Performance Tuning
   ----------------------------------

   1. Caching the data (or dataframes) in memory.
				
	dataframe:  employee.cache()
	      employee.unpersist()

	temp-view: spark.catalog.cacheTable("emp")
	      spark.catalog.uncacheTable("emp")

       	spark.sql.inMemoryColumnarStorage.compressed (default: true)
	spark.sql.inMemoryColumnarStorage.batchSize (default: 10000)

   2. SQL Query Join Strategy Hints
	
	  => BROADCAST, MERGE (sort-merge), SHUFFLE_HASH
	
		df1.join( df2.hint("broadcast"), joinCol, joinType )

		df1.join( broadcast(df2), joinCol, joinType )

	  Documentation: https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-hints.html


   3. Adaptive Query Execution	

	=> Introduced in Spark SQl 3.0 onwards
	=> diasbled by default for < Spark SQL 3.2.0, and enabled by default from 3.2.0 onwards			
		spark.conf.set("spark.sql.adaptive.enabled", "true")

	Optimizations:

	=> Coalesceing Shuffle Partitions
        => Choosing the optimized join strategy at run time
        => Optimizing Skew Joins.

  
    Use-Case
    ========
	
     datasets: https://github.com/ykanakaraju/sparkscala/tree/master/data/movielens

     From movies.csv and ratings.csv datasets, find out the top 10 movies with highest averageUserRating
	=> Consider only those movies that are rated by atleast 30 users.
	=> Data: movieId, title, ratingCount, averageUserRating
	=> Arrange the data in the descending order of averageUserRating
	=> Save the output as a single Pipe-separated CSV file with header.
	=> Use DataFrame Transformations API (not SQL)	

	=> Try it yourself


    JDBC format - Integrating with MySQL
    ------------------------------------
import org.apache.spark.sql.SparkSession
import java.util.Properties
import com.mysql.jdbc.Driver
import org.apache.spark.sql.SaveMode

object DataSourceJDBCMySQL {
  def main(args: Array[String]) {
    //System.setProperty("hadoop.home.dir", "C:\\hadoop\\");
    
    val spark = SparkSession
      .builder.master("local[2]")
      .appName("DataSourceJDBCMySQL")
      .getOrCreate()
      
    import spark.implicits._
    
    
    // Snippet 1: Reading from MySQL using JDBC
    val jdbcDF = spark.read
                    .format("jdbc")
                    .option("url", "jdbc:mysql://localhost:3306/sparkdb")
                    .option("driver", "com.mysql.jdbc.Driver")
                    .option("dbtable", "emp")
                    .option("user", "root")
                    .option("password", "cloudera")
                    .load()
                    
     jdbcDF.show()
      
     jdbcDF.createOrReplaceTempView("empTempView")
     spark.sql("SELECT * FROM empTempView").show()     
     
     // Snippet 2:  Writing to MySQL using JDBC
     spark.sql("SELECT * FROM empTempView")
        .write
        .format("jdbc")
        .option("url", "jdbc:mysql://localhost:3306/sparkdb")
        .option("driver", "com.mysql.jdbc.Driver")
        .option("dbtable", "emp2")
        .option("user", "root")
        .option("password", "cloudera")
        .mode(SaveMode.Overwrite)
        .save()
            
     // Snippet 3: Writing to MySQL using JDBC
     val ratingsCsvPath = "data/movielens/ratings.csv"
     val ratingsDf = spark.read
                            .format("csv")
                            .option("header", "true")
                            .option("inferSchema", "true")
                            .load(ratingsCsvPath)
     
     ratingsDf.printSchema()
     ratingsDf.show(10)
     
     ratingsDf.write
      .format("jdbc")
      .option("url", "jdbc:mysql://localhost:3306/sparkdb")
      .option("driver", "com.mysql.jdbc.Driver")
      .option("dbtable", "movielens_ratings2")
      .option("user", "root")
      .option("password", "cloudera")
      .mode(SaveMode.Overwrite)
      .save()      
      
      spark.close()     
  }
}


    Hive Format - Integrating with Hive
    -----------------------------------

package hive

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import java.io.File

object HiveExample1 {
  
  def main(args: Array[String]) {
    
    //System.setProperty("hadoop.home.dir", "C:\\hadoop\\");
    
    val warehouseLocation = new File("warehouse").getAbsolutePath
    
    println(warehouseLocation)

    val spark = SparkSession
            .builder()
            .appName("DataSourceHive")
            .config("spark.sql.warehouse.dir", warehouseLocation)
            .config("spark.master", "local[1]")
            .enableHiveSupport()
            .getOrCreate()      
      
    import spark.implicits._
  
    spark.sparkContext.setLogLevel("ERROR")
    
    spark.sql("drop database if exists sparkdemo cascade")
    spark.sql("create database if not exists sparkdemo")
    spark.sql("show databases").show()   
    
    spark.sql("use sparkdemo")
    
    println("Current database: " + spark.catalog.currentDatabase)
    
    spark.catalog.listTables().show()     
    
    val createMovies = 
      """CREATE TABLE IF NOT EXISTS 
         movies (movieId INT, title STRING, genres STRING) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
    val loadMovies = 
      """LOAD DATA LOCAL INPATH 'data/movielens/moviesNoHeader.csv' 
         OVERWRITE INTO TABLE movies"""
    
    val createRatings = 
      """CREATE TABLE IF NOT EXISTS 
         ratings (userId INT, movieId INT, rating DOUBLE, timestamp LONG) 
         ROW FORMAT DELIMITED 
         FIELDS TERMINATED BY ','"""
    
    val loadRatings = 
      """LOAD DATA LOCAL INPATH 'data/movielens/ratingsNoHeader.csv' 
         OVERWRITE INTO TABLE ratings"""
        
    spark.sql(createMovies)
    spark.sql(loadMovies)
    spark.sql(createRatings)
    spark.sql(loadRatings)
    
    spark.catalog.listTables().show()
     
    // Queries are expressed in HiveQL
    val moviesDF = spark.table("movies")   //spark.sql("SELECT * FROM movies")
    val ratingsDF = spark.table("ratings")  //spark.sql("SELECT * FROM ratings")
     
    //moviesDF.show(5)
    //ratingsDF.show(5)    
    
    val summaryDf = ratingsDF
                      .groupBy("movieId")
                      .agg(count("rating") as "ratingCount", 
                           avg("rating") as "ratingAvg")
                      .filter("ratingCount > 25")
                      .orderBy(desc("ratingAvg"))
                      .limit(10)
              
    summaryDf.show()
    
    val joinStr = summaryDf.col("movieId") === moviesDF.col("movieId")
    
    val summaryDf2 = summaryDf
                     .join(moviesDF, joinStr)
                     .drop(summaryDf.col("movieId"))
                     .select("movieId", "title", "ratingCount", "ratingAvg")
                     .orderBy(desc("ratingAvg"))
    
    summaryDf2.show()
    
    summaryDf2.write.format("hive").saveAsTable("topRatedMovies")
    spark.catalog.listTables().show()
        
    val topRatedMovies = spark.table("topRatedMovies")  //spark.sql("SELECT * FROM topRatedMovies")
    topRatedMovies.show() 
    
    spark.stop()    

  }
}

  ======================================
     Spark Streaming
  ======================================
   
 Spark's real-time data processing API

     Two APIs are available:

	1. Spark Streaming a.k.a DStreams API    (out-dated)
	2. Structured Streaming	(current and preferred API)

  Spark Streaming (DStreams API)
  ------------------------------

     => microbatch based processing
     => Provides "seconds" scale latency.  (near-real-time processing)
     => does not support event-time processing

       StreamingContext :
	  -> Is the starting point of execution
	  -> Defines a micro-batch window
	  -> Each micro-batch is an RDD

       DStream (discretized stream)
	  -> Is a continuous flow of RDDs.
	  -> Each micro-batch is represented as an RDD.

	  
  	# Create a local StreamingContext with two threads and batch interval of 1 sec.
	sc = SparkContext("local[2]", "NetworkWordCount")
	ssc = StreamingContext(sc, 1)

	lines = ssc.socketTextStream("localhost", 9999)
	words = lines.flatMap(lambda line: line.split(" "))
	pairs = words.map(lambda word: (word, 1))
	wordCounts = pairs.reduceByKey(lambda x, y: x + y)
	wordCounts.print()

	ssc.start()             
	ssc.awaitTermination() 


   Spark Structured Streaming
   -------------------------- 

	-> Consider the input data stream as the 'Input Table'. 
	  Every data item that is arriving on the stream is like a new row being appended to the Input Table.

	=> DataFrame -> Unbounded Table

	-> A query on the input will generate the 'Result Table'. 

	-> Every trigger interval (say, every 1 second), new rows get appended to the Input Table, 
	   which eventually updates the Result Table. 

	-> Whenever the result table gets updated, we would want to write the changed result 
	   rows to an external sink.

	Output Modes
	------------
	The 'Output' is defined as what gets written out to the external storage. 
	The output can be defined in a different mode:

	* Complete Mode - The entire updated Result Table will be written to the external storage.

	* Append Mode - Only the new rows appended in the Result Table since the last trigger will 
		      be written to the external storage. 

		      This is applicable only on the queries where existing rows in the 
		      Result Table are not expected to change.

	* Update Mode - Only the rows that were updated in the Result Table since the last trigger 
		      will be written to the external storage. 

		      Note that this is different from the Complete Mode in that this mode 
		      only outputs the rows that have changed since the last trigger. 

		      If the query doesn't contain aggregations, it will be equivalent to Append mode.


	Sample Socket Stream Example
	----------------------------

	from pyspark.sql import SparkSession
	from pyspark.sql.functions import explode
	from pyspark.sql.functions import split

	spark = SparkSession \
    		.builder \
    		.appName("StructuredNetworkWordCount") \
    		.getOrCreate()

	lines = spark \
    		.readStream \
    		.format("socket") \
    		.option("host", "localhost") \
    		.option("port", 9999) \
    		.load()

	# Split the lines into words
	words = lines.select( explode(split(lines.value, " ")).alias("word"))

	# Generate running word count
	wordCounts = words.groupBy("word").count()

	query = wordCounts \
    		.writeStream \
    		.outputMode("complete") \
    		.format("console") \
    		.start()

	query.awaitTermination() 


	=> Sources: File, Socket, Rate, Kafka

	=> Sinks: File, Console, Memory, Kafka, ForEach, ForEachBatch

   








		
